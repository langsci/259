\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.5599824}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[4]
}{}

% you may switch off externalization of changed files here:
%\memoizeset{readonly}


\author{Anthony R. Davis\affiliation{Southern Oregon University}\lastand Jean-Pierre Koenig\affiliation{University at Buffalo}}



\title{The nature and role of the lexicon in HPSG}


\abstract{This chapter discusses the critical role the lexicon plays in HPSG and the approach to lexical knowledge that is specific to HPSG. We describe the tenets of lexicalism in general, and discuss the nature and content of lexical entries in HPSG. As a lexicalist theory, HPSG treats lexical entries as informationally rich, representing the combinatorial properties of words as well as their part of speech, phonology, and semantics. Thus many phenomena receive a lexically-based account, including some that go beyond what is typically regarded as lexical. We turn next to the global structure of the HPSG lexicon, the hierarchical lexicon and inheritance. We show how the extensive type hierarchy employed in HPSG accounts for lexical generalizations at various levels and discuss some of the advantages of default (nonmonotonic) inheritance over simple monotonic inheritance. We then describe lexical rules and their various proposed uses in HPSG, comparing them to alternative approaches to relate lexemes and words based on the same root or stem.}


\begin{document}
\maketitle
\label{chap-lexicon}

\section{Introduction}

\is{lexicon|(}
The nature, structure, and role of the lexicon in the grammar of natural languages has been a
subject of debate for at least the last 50 years. For some, the lexicon is a prison that ``contains
only the lawless'', to borrow a memorable phrase from \citew[3]{DiSciulloandWilliams1987}, and not
much of interest resides there. In some recent views, the lexicon records merely phonological
information and some world knowledge about each lexical entry (see \citealt{Marantz1997}). All of
the action is in the syntax, save the expression of complex syntactic objects as inflected words.
In contrast, lexicalist theories of grammar, and HPSG in particular, posit a rich and complex
lexicon embodying much of grammatical knowledge.

This chapter has two principal goals.  One is to review the arguments for and against a lexicalist
view of grammar within the generative tradition.  The other is to survey the HPSG implementation of
lexicalism. In regard to the first goal, we begin with the reaction to \isi{Generative Semantics},
and note developments that led to lexicalist theories of grammar such as Lexical\indexlfg Functional
Grammar (LFG) and then HPSG.  Central to these developments was the argument that lexical processes,
rather than transformational ones, provided more perspicuous accounts of derivational morphological
processes.  The same kinds of arguments then naturally extended to phenomena like
passivization\is{passive}, which had previously been treated as syntactic.  Once on this path,
lexical treatments of other prototypically syntactic phenomena — long distance extraction,
\word{wh}-movement, word order, and anaphoric binding — were advanced as well, with HPSG playing a
leading role.

\largerpage
But this does not mean that opposition to lexicalism melted away.  Both Minimalism, and in
particular Distributed Morphology \citep{Bruening2018, Marantz1997} and Construction Grammar
\citep{Goldberg1995,Tomasello2003,vanTrijp2011} claim that lexicalist accounts fail in various ways.
We discuss some of these current issues, including the apparent occurrence of syntactically complex
structures in the lexicon, word-internal ellipsis, and endoclitics, each of which poses challenges
for those who advocate a strict separation between lexical and syntactic processes.  While we
maintain that the anti-lexicalist arguments are not especially strong, and the phenomena they are
based on somewhat marginal, we acknowledge that these questions are not yet settled. We then turn to
the specifics of the lexicon as modeled within HPSG.  Lexicalism demands, of course, that lexical
entries be informationally rich, encoding not merely idiosyncratic properties of a single lexical
item like its phonology and semantics, but also more general characteristics like its combinatorial
possibilities.  We outline what HPSG lexical entries must contain, and how that information is
represented.  This leads naturally to the next topic: with so much information in a lexical entry,
and so much of that repeated in similar ones, how is massive redundancy avoided?  The hierarchical
lexicon, in which individual lexical entries are the leaves of a multiple inheritance hierarchy, is
a core component of HPSG.  Types throughout the hierarchy capture information common to classes of
lexical entries, thereby allowing researchers to express generalizations at various levels.  Just as
all verbs share certain properties, all transitive verbs, all verbs of caused motion, and all
transitive verbs of caused motion share additional properties, represented as constraints on types
within the hierarchy.  We draw on examples from linking, gerunds, and passive constructions as
illustrations, but many others could be added.

Constraints specified on types in the hierarchy are deemed to be inherited by their subtypes, but
monotonic inheritance of this kind runs into vexing issues.  Most obviously, there are irregular
morphological forms; any attempt to represent, say, the phonology of \ili{English} plurals as a
constraint on a plural noun class in the hierarchical lexicon must then explain why the plural of
\word{child} is \word{children} and not *\word{childs}.  Beyond this simple example, there are
ubiquitous cases of lexical generalizations that are true by default, but not always.  Various
mechanisms for modeling default inheritance have therefore been one focus within HPSG, and we
furnish an example of their use in modeling the properties of gerunds in \ili{English} and other
languages.

\largerpage
Finally, we discuss lexical rules and their alternatives.  Along with the ``vertical'' relationships
between classes of lexical entries modeled by types and their subtypes in the hierarchical lexicon,
there is a need for ``horizontal'' relationships between lexical entries that are based on a single
root or stem, such as forms of inflectional paradigms.  Yet formalizing lexical rules adequately
within HPSG has proven surprisingly difficult; specifying just what information is preserved and
what is changed by a lexical rule is one prominent issue.  We conclude this chapter by describing
alternatives to lexical rules.  One is to appropriately underspecify properties of lexical entries
so that they cover all relevant variants of a single lexeme or word.\footnote{%
It is common since the the late 1990s to distinguish between lexemes and words in HPSG, with, for
example, some lexical rules mapping lexemes to lexemes (typically, derivational morphology) and some
lexical rules mapping lexemes to words (typically, inflection); see
\citet[176--178]{BonamiandCrysmann2018} for general discussion and \citet{RunnerandAranovich2003}
for arguments that argument structure and valence features are specific to words, not lexemes. We do
not further discuss the distinction between lexemes and words in this chapter for space reasons.}
The second augments the type
hierarchy via online type construction, extending the predefined lexical types specified in the
hierarchy to include ``virtual types'' that combine the information from multiple predefined types.



\section{Lexicalism}
\is{lexicalism|(}
\label{sec:lex}
\subsection{Lexicalism and the origins of HPSG}

Lexicalism began as a reaction to \isi{Generative Semantics}, which treated any regularity in the
structure of words (derivational patterns, broadly speaking) as only epiphenomenally a matter of
word structure and underlyingly as a matter of syntactic structure (see \citealt{Lakoff1970}, among
others). In the Generative Semantics view, all grammatical regularities are a matter of syntax (much
of it, in fact, logical syntax). \citet{Chomsky1970} presented many arguments that lexical knowledge
differs qualitatively from syntactic knowledge and should be modeled
differently. \citet{Jackendoff1975} provides an explicit model of lexical knowledge that follows Chomsky's
insights, although it focuses exclusively on derivational morphological processes. The main insight
that Jackendoff formalizes is that relations between stems  or words (say, between
\word{destruct} and \word{destruction}) are to be modeled not via a generative device but through a
redundancy mechanism that measures the relative complexity of a lexicon where these relations are
present or not present (the idea is that a lexicon where \word{destruct} and \word{destruction}
are related is simpler than one where they are not). \citet{Bochner1993} is the most formalized and
detailed version of this approach to lexical relations. Lexicalist approaches, including
LFG\indexlfg and HPSG, took their lead from Jackendoff's work.  LFG
has relied heavily on treating relations between stems and between words as lexical rules, rather
than the kind of generative devices that one finds in syntax. But, as accounts of linguistic
phenomena in LFG focused increasingly on the lexicon, the question of whether lexical rules retain
the character of redundancy rules or turn into yet another kind of generative device arose.
Consequently, the necessity of lexical rules has been questioned as well (see
\citealt{KoenigandJurafsky1994} and \citealt[29--49]{Koenig1999c} for potential issues that arise
once lexical rules are assumed to be involved in the creation of new lexical entries).

% TD 14 Nov: GL stuff starts here
% TD 14 Nov: don't forget an index entry for Generative Lexicon
% references:
% Pustejovsky, James. 1991. The Generative Lexicon, Computational Linguistics 17:4, pp. 409-441.
% Pustejovsky, James. 1995. The Generative Lexicon, MIT Press, Cambridge, MA.
% Pustejovsky, James, and Elisabetta Jezek. 2016. Integrating Generative Lexiconand Lexical Semantic Resources, Proceedings of LREC 2016, Portorož, Slovenia (http://lrec2016.lrec-conf.org/media/filer_public/2016/05/10/tutorialmaterial_pustejovsky.pdf).
Another stream of research relying on a richly structured lexicon is Generative Lexicon theory (GL).
\citet{Pustejovsky1991,Pustejovsky1995} and \citet{PustejovskyandJezek1996} present the elements of
this approach to lexical representation, which focuses on semantic phenomena such as coercion and
systematic polysemy. 
Within GL, lexical entries include, in addition to argument structure, an ``event structure'' and a
``qualia structure'', both of which play essential roles in GL accounts of semantic composition. 
For example, the natural interpretation of \word{enjoy the sandwich} as enjoying eating the sandwich
arises from information in the event structures of \word{enjoy} and \word{sandwich} and the qualia
structure of \word{sandwich}, which unify to yield this interpretation. 
% TD 14 Nov: end of GL stuff (a bit more comes later)

Lexicalism, at least within HPSG, embodies two distinct ideas. First is the idea that parts of words
are invisible to syntactic operations\is{Lexical Integrity} (\emph{Lexical Integrity}, see
\citealt{BresnanandMchombo1995}), so that relations between stems and between word forms cannot be
the result of or follow syntactic operations, as in \isi{Distributed Morphology}
\citep{HalleandMarantz1993}, or other linguistic models that assign no special status to the notion
of word. Relations between words are therefore not modeled via syntactic operations (hence the
appeal to Jackendoff's lexical rules\is{lexical rule} early on and to unary branching rules more recently). 
Second is the idea that the occurrence of a lexical head
in distinct syntactic contexts arises from distinct variants of words. For instance, the fact that
the verb \word{expect} can occur both with a finite clause and an NP+VP sequence (see
(\ref{expect-b}) vs.\ (\ref{expect-a})) means that there are two variants of the verb \word{expect},
one that subcategorizes for a finite clause and one where it subcategorizes for an NP+VP
sequence.\footnote{As this chapter is an overview of the approach to lexical knowledge HPSG embodies
  rather than a description of particular HPSG analyses of phenomena, we will sample liberally from
  various illustrative examples and simplify the analyses whenever possible so that readers can see
  the forest and not get lost in the trees.} 
\eal
\ex \label{expect-b} I expected that he would leave yesterday.
\ex \label{expect-a} I expected him to leave yesterday.
\zl

\noindent
Not all lexicalist theories, though, cash out these two
distinct ideas the same way. The net effect of lexicalism within HPSG is that words and phrases are
put together via distinct sets of constructions and that words are syntactic atoms. These two
assumptions justify positing two kinds of signs, \type{phrasal-sign} and \type{lexical"=sign}, and go
hand in hand with the surface-oriented character of HPSG and what one might call a principle of
surface combinatorics: if expression A consists of the concatenation of B and C (B $\oplus$ C), then
all grammatical constraints that make reference to B and C are limited to A.  


An evident concern regarding this view of the lexicon is the potential proliferation of lexical
entries, replete with redundant information. Will it be necessary to specify all the information in
these two variants for \word{expect} without regard for the large amount of duplication between
them? Will the same duplication be needed for the verb \word{hope}, which patterns similarly (but
not quite identically)? How will somewhat similar verbs, such as \word{foresee} and
\word{anticipate} which allow finite complements but not infinitive ones, be represented? We will
describe HPSG's solutions to these questions below, in our discussion of the hierarchical
lexicon. First, however, we turn to recent arguments against lexicalism, and then discuss in more
detail just what kinds of information should be in HPSG lexical entries.

\subsection{Recent challenges to lexicalism}

As there have been several challenges to lexicalism (see \citealt{Bruening2018} and
\citealt{Haspelmath2011} among others for some recent challenges), we now explore lexicalism and
Lexical Integrity\is{Lexical Integrity} in HPSG in more detail. We first note that lexicalism does
not imply that word and phrase formation are necessarily different ``components'' as is often
claimed (see \citealt{Marantz1997}, \citealt{Bruening2018}). Some lexicalist approaches \emph{do}
assume that word formation and phrase building belong to two different components of a language's
grammar (this is certainly true of \citealt{Jackendoff1975}), but they need not. Within HPSG, there
are approaches that treat every sign-formation (be it word-internal or word-external) as resulting
from typed mother-daughter configurations; this is the hypothesis pursued in \citealt{Koenig1999c},
and is also the approach frequently taken in implementations of large-scale grammars where lexical
rules are modeled as unary-branching trees; see the \ili{English} Resource Grammar
\citep{Copestake2002} and the grammars developed in the \isi{CoreGram} project \citep{Mueller2015}
(see \citealt[58]{MuellerLexicalism} for a similar point in his response to Bruening's paper).

%JPK 3-3 Changed a little the following
Furthermore, recent approaches to inflectional morphology within HPSG model realizational rules through the very same tools the rest of a language's grammar uses (see \citealt{CrysmannandBonami2016} and \crossrefchapteralt{morphology}).  There are also analyses of the structure of phrases where the same analytical tools -- multidimensional hierarchy of types and inheritance -- developed to model lexical knowledge (see Section~\ref{sec:hier-lex}) are employed to model phrase-structural constructions (see \citeauthor{Sag1997}'s 1997 analysis of relative clauses, for example). So, both in terms of the formal devices and in terms of analytical tools used to model datasets, words and phrases can be treated the same way in HPSG (although they need not be). Somewhat ironically, and despite claims to the contrary, word formation in the syntactocentric approach Marantz or Bruening advocates \emph{does} make use of distinct formal machinery to model word formation, namely realizational rules and various readjustment rules, as well as fusion and fission rules, to model inflectional morphology (see \citealt{HalleandMarantz1993,Embick2015}).\is{Distributed Morphology}

With this red herring out of the way, we concentrate on the two most important challenges \citet{Bruening2018} and \citet{Haspelmath2011} present to lexicalist views. The first challenge are cases of phrasal syntax feeding the lexicon, purportedly exemplified by sentences such as (\ref{bruen1}).

\ea
\label{bruen1}
I gave her a don't-you-dare! look.\footnote{
\citew[\page 3]{Bruening2018}
}
\z

\is{morphology|(}%\is{word!internal structure of|(}
We can provisionally accept for the sake of argument Bruening's contention that \word{don't-you-dare!} is a word in (\ref{bruen1}), despite its reliance on the (unjustified) assumption that the secondary object in (\ref{bruen1}) involves N-N compounding rather than an AP N structure (we refer readers to \citealt{BresnanandMchombo1995} or \citealt{MuellerLexicalism} for counter-arguments to Bruening's claim). Crucially, though, examples such as (\ref{bruen1}) have no bearing on HPSG's model of lexical knowledge, as HPSG-style lexicalism does not preclude constructions that form words from phrases. Nothing, as far as we know, rules out constructions of the form  \type{stem/word} $\rightarrow$ \type{phrase} in HPSG. The two assumptions underlying the HPSG brand of lexicalism we mentioned above do not preclude a \type{lexical-sign} having a \type{phrasal-sign} as sole daughter (although we do not know of any HPSG work that exploits this possibility) and examples such as (\ref{bruen1}) are simply irrelevant to whether HPSG's lexicalist stance is empirically correct.

The second challenge to lexicalism presented in \citet{Bruening2018} bears more directly on HPSG's assumption that words are syntactic atoms. Word-internal conjunction/ellipsis examples, illustrated in (\ref{ch08}) (adapted from Bruening's (31a), p.\,14), seem to violate the assumption that syntactic constraints cannot ``see'' the \isi{internal structure of words}\is{morphology}\is{Lexical Integrity}, as ellipsis in these kinds of examples seems to have access to the internal part of the word \word{over-application}. In fact, though, such examples do not violate Lexical Integrity\is{Lexical Integrity} if one enriches the representation of composite words (to borrow a term from \citealt[Chapter 11]{Anderson1992}) to include a representation of their internal phonological parts as proposed in \citet{Chaves2008}  and \citet{Chaves2014}.\is{affixation}

\ea
\label{ch08}
Over- and under-application of stress rules plagues Jim's analysis.
\z
\is{ellipsis!word internal}%
Chaves' analysis assumes that the phonology of compound words and words that contain affixoids (to borrow a term from \citealt[114--117]{Booij2005}) is structured. The MorphoPhonology or \attrib{mp} attribute of words (and phrases) is a list of phonological forms and morphs information. The \attrib{mp} of compound words and words that contain affixoids includes a separate member for each member of the compound, or for the affixoid and stem.  Thus in (\ref{ch08}), the \attrib{mp}s of \word{overapplication} and \word{underapplication} each contain two elements: one for \word{over}/\word{under}, and one for \word{application}. Given this enriched representation of the morphophonology of words like \word{under/overapplication}, a single ellipsis rule can apply both to phrases and to composite words, eliding the second member of the word \word{overapplication}'s \attrib{mp}. As Chaves (p.\,304) makes clear, such an analysis is fully compatible with Lexical Integrity, as there is no access to the internal structure\is{morphology!internal structure of words} of composite words, only to the (enriched) morphophonology of the entire word.

\citet{Haspelmath2011} similarly challenges the view that syntactic processes may not access the internal structure of words, although Haspelmath's point is merely that what is a word is cross-linguistically unclear. So-called \isi{suspended affixation} in \ili{Turkish} (see (\ref{turk}), \citealt[48]{Haspelmath2011}) also shows that word parts can be elided. We cannot discuss here whether Chaves' analysis can be extended to cases like (\ref{turk}) where suffixes are seemingly elided or whether lexical sharing (where a single word can be the daughter of two c-structure nodes \`a la \citealt{McCawley1982}), as proposed in \citet{Broadwell2008}, is needed. 

\ea
\label{turk}
\gll kedi ve köpek-ler-im-e \\
     cat and dog-\ig{pl-1sg-dat} \\\hfill(\ili{Turkish})
\glt `to my cat(s) and dogs'	
\z

What is important for current purposes is that these putative challenges to \isi{Lexical Integrity} such as (\ref{ch08}) or (\ref{turk}) do not necessarily render a substantive version of it implausible. The same is true of another potential challenge to Lexical Integrity which neither Bruening nor Haspelmath discuss, endoclitics, which we turn to next.

\is{clitic!endoclitic|(}
Endoclitics are clitics that at least appear to be situated within a word, rather than immediately
preceding or following it, as clitics often do.
% later \crossrefchapterp{clitics}. 
In many cases, endoclitics appear at morphological boundaries, as in the well-studied pronominal clitics\is{clitic} of European Portuguese \citep{Crysmann2000a}. An approach similar to what we have referenced above for composite words and elided morphology may extend to these as well. But some trickier cases have also come to light, in which the clitic appears within a morpheme, not at a boundary. Two of the best documented cases come from the Northeast Caucasian language \ili{Udi} \citep{Harris2000} (see (\ref{udi-endo})), and from \ili{Pashto} \citep{Tegey1977,Roberts2000,Dost2007} (see (\ref{pashto-endo}))).

\ea
\label{udi-endo}
\gll q'a\v{c}aɣ-ɣ-on bez t\"{a}nginax ba\v{s}=\textbfemph{q'un}-q'-e\footnotemark\\
     thief-\ig{pl-erg} my money.\ig{dat} steal$_{1}$-\ig{3pl}-steal$_{2}$-\ig{aorII} \\\hfill(Udi)
\glt `Thieves stole my money.' (root \word{ba\v{s}q'}, 'steal') 
\footnotetext{\citew[599]{Harris2000}}
\z

\eal
\label{pashto-endo}
\ex\label{pashto-endo-a}
\gll ʈəlwɑhə́=\textbfemph{me}\footnotemark \\
     push.\ig{impf.pst.3sg}=\ig{cl}.\ig{1sg} \\\hfill(Pashto)
\glt 'I was pushing it.'
\footnotetext{\citew[92]{Tegey1977}}
\ex\label{pashto-endo-b}
\gll ʈə́l=\textbfemph{me}-wɑhə\footnotemark \\
     push$_{1}$=\ig{cl}.\ig{1sg}-push$_{2}$.\ig{pf.pst.3sg} \\
\glt `I pushed it.'
\footnotetext{
\citew[92]{Tegey1977}
}
\zl

In these cases, as with clitics in general, there is a clash between the phonological criteria for wordhood, under which the clitics would be regarded as incorporated within words, and the syntactic constituency and semantic compositionality.
But what makes these particularly odd is that these clitics are situated word-internally, even morpheme-internally.\is{clitic}
\ili{Udi} subject agreement clitics such as \word{q'un} in (\ref{udi-endo}) typically attach to a focused constituent, which can be a noun, a questioned constituent, or a negation particle as well as a verb \citep{Harris2000}.
% Harris = whole paper
Under certain conditions, as in (\ref{udi-endo}), none of these options is available or permitted, and the clitic is inserted before the final consonant of the verb root, dividing it in two pieces, neither of which has any independent morphological status.
Its position in this instance is apparently phonologically determined; it cannot appear word-finally or word-initially, and as there is no morphological boundary within the word it must therefore appear within the monomorphemic root.
\ili{Pashto} clitics\is{clitic} seek ``second position'', whether at the phrasal, morphological, or phonological level; \word{me} in (\ref{pashto-endo}) appears to be situated after the first stressed syllable (or metrical foot), which, in the case of (\ref{pashto-endo-b}), also divides the verb into two parts that lack any independent morphological status.

If clitics are viewed as a syntactic phenomenon (``phrasal affixes'', as \citealt{Anderson2005} puts
it), these endoclitics must ``see'' into the internal structure of words (be it morphological,
prosodic, or something else), thereby seemingly violating Lexical
Integrity\is{Lexical Integrity}. Anderson's brief account invokes a reranking of Optimality Theoretic constraints from
their typical ordering, whereby the clitic's positional requirements outrank Lexical Integrity
requirements. \citet{Crysmann2000b} proposes an analysis, paralleling in many respects his account
of European \ili{Portuguese} clitics in \citet{Crysmann2000a}, using Reape's constituent
order\is{order domain} domains \citep{Reape1994} and, in particular, Kathol's topological
fields\is{topological field}
(\citealt{Kathol2000a}; see also \crossrefchapteralt[Section~\ref{order:sec-domains}]{order}). The ``morphosyntactic paradox'' in \ili{Udi}, to borrow
a phrase from \citet[\page 373]{Crysmann2003d}, is effectively ``resolved on the basis of discontinuous lexical items''; this account then ``parallels HPSG's representation of syntactic discontinuity'' \citep{Crysmann2000b}.

For \ili{Pashto}, researchers generally agree that the notion of second position is crucial, but that it
can be defined at various levels --- phrasal, lexical, and phonological. In this last case, clitics
can appear within a word following the first metrical unit, as illustrated above.
% \crossrefchapterp{phonology}. 
\citet{Dost2007} invokes the mechanisms of word order domains \citep{Reape1994} and topological fields \citep{Kathol2000a} at these various levels to account for this distribution of clitics. In this analysis, some words contain more than one order domain at the prosodic level. \isi{Lexical Integrity} is preserved to the extent that, while domains at the prosodic level are ``visible'' to clitics in \ili{Pashto}, syntactic processes do not reference the internal makeup of words.

Still, these accounts of endoclitics in \ili{Udi} and \ili{Pashto} appear to breach the wall of the strictest kind of Lexical Integrity, as they require access to some of the internal structure of lexical entries through a partial decomposition of their morphophonology into distinct order domains. Yet we would not wish to advocate models that permit unconstrained violations of Lexical Integrity, either. The troublesome cases we have noted here are relatively marginal or cross-linguistically rare; they seem to be limited in scope to prosodic or morphophonological information (e.g., ellipsis, insertion). As \citet{Broadwell2008} points out when comparing possible analyses of \ili{Turkish} suspended affixation, rejecting lexicalism altogether may lead to an unconstrained theory of the interaction between words/stems and phrases and thereby to incorrect predictions (e.g., that all affixes in \ili{Turkish} can be suspended). Likewise, we would not expect to find a language in which endoclitic positioning is utterly unconstrained or where syntactic operations are sensitive to the fact that \word{anticonstitutional} is based on the nominal root \word{constitution}, or where coordination of affixes is always possible. Rejecting lexicalism begs the question of why such languages do not seem to exist, why what is visible to syntactic operations of the internal structure of words (morphophonological structure) is so restricted or why even that kind of morphophonological visibility is so rare (particular affixoids and endoclitics, say).
\is{clitic!endoclitic|)}
\is{lexicalism|)}
\is{morphology|)}%\is{word!internal structure of|)}

\section{Lexical entries in HPSG}
\is{lexical entry|(}

\subsection{What are lexical entries?}

%JPK 3-3-21 the following paragraph has been edited

Because lexical entries (derived or not\footnote{%
Researchers working in the tradition of Höhle use the term \emph{lexical entry} for lexical items that are stored in the lexicon and \emph{lexical item} for all lexicon elements, that is, stored lexical items and those licensed by lexical rules. We will not make this distinction here.}) play a central role in accounting for the syntax of natural languages, lexical entries are informationally rich in HPSG. 
% TD 27 July: moved this sentence up here, here I think it fits better
An additional consequence of HPSG's lexicalist stance is that there will be many lexical entries where one might at first glance expect a single entry. We will see below how HPSG handles multiple entries and classes of entries while avoiding redundancy, but it is important at the outset to clarify what a lexical entry is in HPSG. 
One misunderstanding about lexical entries conflates descriptions and the entities they describe, or, in other words, fails to distinguish between constructions in the abstract and a particular word or phrase (i.e., a lexical entry vs.\ a fully instantiated lexeme or word). 
% One of the misunderstandings about lexical knowledge is that it confuses descriptions and entities being described, or the distinction between construc- tions and constructs (lexical entry vs. fully instantiated lexeme). 
% TD 27 July: reworded preceding sentence, because I wasn't sure what it meant,
% but I'm not sure I've expressed what's intended.
As \crossrefchaptert{formal-background} makes explicit, grammars in HPSG consist of \emph{descriptions} of structures, and the lexicon thus consists of descriptions of what a fully specified lexeme or word can be. 
%As the chapter on the formal foundations of HPSG discusses, grammars in HPSG consist of \emph{descriptions} of structures, and the lexicon thus consists of descriptions of what are fully specified lexemes. 
% TD 27 July: reworded a bit
To see the importance of the distinction between descriptions (stored or derived entries) and the fully instantiated objects that are being described, consider HPSG's model of subcategorization with reference to the relevant portion of the tree for sentence (\ref{expect-b}). 
HPSG's model of the dependency between heads and complements stipulates identity between the syntactic and semantic information of each complement (the value of the \attrib{synsem} attribute) and a member of the list of complements the head subcategorizes for. 
%HPSG's model of the dependency between heads and complements involves identity between the syntactic and semantic information of each complement (the value of the \attrib{synsem} attribute) and a member of the list of complements the head subcategorizes for. 
Since there are indefinitely many \attrib{synsem} values, on the assumption that there are indefinitely many clausal meanings (a point \citealt[8--9]{Jackendoff1990} emphasizes), there are, in principle, indefinitely many fully instantiated entries for the verb \word{expect} subcategorizing for a clausal complement (as in (\ref{expect-b})). 
But each of these fully instantiated entries for \word{expect} -- one for each clausal sentence that corresponds to the tree in Figure~\ref{expect-b-tree} -- corresponds to a single abstract description, and it is this description that the lexicon contains. 
%But each of these fully instantiated entries for \word{expect}, one for each clausal sentence that corresponds to the tree in (\ref{expect-b-tree}) corresponds to a single abstract description, and it is this description that the lexicon contains. 

\begin{figure}
	\begin{forest}
	[ [{\avm{[comps & < \1 >]}} ] 
	[{\avm{[synsem & \1 ]}} ] ]
\end{forest}	
\caption{\label{expect-b-tree} Sharing of valence information in a head-complement phrase}
\end{figure}


The formal status of lexical entries has engendered a fair amount of theoretical work and some debate.
We will touch on some aspects of this further below, in connection with online type construction.
For further discussion of these kinds of issues, see \crossrefchaptert{formal-background} and \crossrefchaptert{properties}.

\subsection{What information is in lexical entries?}
\label{lexicon:sec-what-information-is-in-lexical-entries}

% TD: 27 July 19: moved the first sentence of this paragraph up to the beginning of the section
Aside from the expected phonological and semantic information, specific to each lexeme or word,
lexical entries include morphological information and information about their combinatorial
potential. Morphosyntactic features can be part of the input to inflectional rules, but are also
used to select the appropriate types of phrases (via their projections through the Head Feature
Principle\is{principle!Head Feature}, see \crossrefchapteralt[\pageref{page-hfp}]{properties}), as shown in (\ref{select}). Some verbs, for instance, select for a PP headed by a particular preposition; others select for VPs whose verb is a gerund, or a bare infinitive, and so forth. Lexical entries thus include as much morphological information as both (inflectional) morphology and syntactic selection require.

\eal
\label{select}
\ex\label{select-a}John conceived \emph{of/*about} the world's tastiest potato chip.
\ex\label{select-b} John regretted \emph{going/*(to) go} to the party.
\zl


We illustrate the second leading idea behind HPSG or \lfg's\indexlfg \isi{lexicalism} -- that there
are different variants of lexical heads for different contexts in which heads occur -- with the
\ili{French} examples in (\ref{va}). The verb \word{aller} `go' in (\ref{va-a}) combines with a PP
headed by \word{à} that expresses its goal argument and a subject that expresses its theme
argument. The same verb in (\ref{va-b}) combines with the so-called non-subject clitic\is{clitic}
\word{y} that expresses its goal argument. We follow \citet{MillerandSag1997} and assume here that
\ili{French} non-subject clitics are prefixes. Since the context of occurrence of the head of the
sentence, \word{aller}, differs across these two sentences (\textsc{NP\_\_\_\_PP}[\word{loc}] and
\attrib{NP} \word{y}\_\_\_\_ , respectively and informally), there will be two distinct entries for
\word{aller} for both sentences, shown in (\ref{va-lx}) and (\ref{y-va-lx}) (we simplify the
entries' \isi{feature geometry} for expository purposes). Information in the entry in
(\ref{y-va-lx}) that differs from the information in the entry in (\ref{va-lx}) appears in red;
\word{p-aff} indicates that a member of \textsc{arg-st} is realized as a pronominal affix. 
% TD 30 July: good strategy for viewing online; will there be color in the printed version (if there is to be a printed version)?
% JPK 5 August: don't know but I assume most people will be viewing it on the screen (even if they download the pdf)

\eal
\label{va}
\ex\label{va-a} 
\gll Muriel va à Lourdes. \\
     Muriel go.\ig{prs.3sg} at Lourdes \\\hfill(French)
     \glt `Muriel is going to Lourdes.'
\ex\label{va-b}
\gll Muriel y va. \\
     Muriel there go.\ig{prs.3sg} \\
 \glt `Muriel is going there.'
\zl

\ea
% todo avm issue #17
\label{va-lx}
	\avm{
		[ morph & [form & \5 \\ 
                           i-form & \5 va \\
		           stem & v- ] \\
		cat & [head & [\type*{verb} 
		               vform & [mood & indic \\
		                        tns & pres \\
		                        agr & 3rdsing ] ] \\
		       subj  & <\1> \\
		       comps & <\2> \\
		       arg-st & <\1 NP$_{\3,3rd sg}$, \2 !PP[\type{loc}]$_{\4}$ ! > ] \\
		cont & [\type*{go-rel}  
		        theme & \3 \\ 
		        goal  & \4 ] ]
	}
\z

\ea
\label{y-va-lx}
	\avm{
		[ morph & [form & {\color{red}y-va} \\ i-form & va \\
		stem & v- ] \\
		cat & [head & [\type*{verb}
		vform & [mood & indic \\
		tns & pres \\
		agr & 3rdsing ] ] \\
		subj & <\1> \\
		comps & {\color{red}<>} \smallskip\\
		arg-st & <\1 NP$_{\3,3rdsg}$, {\color{red}PP[\rm{\type{p-aff, loc}}]$_{\4}$}> ] \\
		cont & [\type*{go-rel}  
		theme & \3 \\
		goal & \4 ]
		]
	}
\z


\attrib{cat}egory information in both entries contains part of speech information (including
morphologically relevant features of verb forms), \attrib{arg}ument-\attrib{st}ructure information
and valence information under \subj and \comps. \attrib{morph} information includes both stem form
information, inflected form information (\feat{i-form}) and, in case so-called clitics are present,
the combination of the clitic and inflected form information. Both entries illustrate how
informationally rich lexical entries are in HPSG. But, postulating informationally rich entries does
not mean stipulating all of the information within every entry. In fact, only the stem form and the
relation denoted by the semantic content of the verb \word{aller} need to be stipulated within
either entry. All the other information can be inferred once it is known which classes of verbs
these entries belong to. In other words, most of the information included in the entries in
(\ref{va-lx}) and (\ref{y-va-lx}) is not specific to these individual entries, an issue we take up
in Section~\ref{sec:hier-lex}.  As mentioned above, the informational difference between the two
entries for \word{va} and \word{y va} is indicated in red in (\ref{y-va-lx}). The first difference
between the two variants of \word{va} `goes' is in the list of complements: the entry for \word{y
  va} does not subcategorize for a locative PP, since the affix \word{y} satisfies the relevant
argument structure requirement. This difference in the realization of syntactic arguments (via
phrases and pronominal affixes) is recorded in the types of the PP members of \feat{arg-st},
\type{p-aff} in (\ref{y-va-lx}), but a PP headed by \word{\`a} in (\ref{va-lx}). Finally, the two
entries differ in the \attrib{form} of the verb, which is the same as the inflected form of the verb
in (\ref{va-lx}) (as indicated by the identically numbered \avmbox{5}), but not in (\ref{y-va-lx}),
whose \attrib{form} includes the prefix \word{y}.

\largerpage
One other question arises with regard to the information in lexical entries.
Are there attributes or values that occur solely within lexical signs\is{lexical sign}, and not in phrasal ones? 
If so, they would provide a diagnostic for distinguishing lexical signs from others.
The \feat{arg-st}\isfeat{arg-st} list, which we included in the categorial information of signs in (\ref{va-lx}) and (\ref{y-va-lx}), might be regarded as a feature confined to lexical signs (see, among others, \citealt[361]{GinzburgandSag2001}), on the premise that lexical items alone specify combinatorial requirements (but see \citealt{Przepiorkowski2001} for a contrary view, and see \crossrefchapteralt[Section~\ref{sec-free-without-domains}]{order} for other views questioning this assumption).
%\itdopt{You referenced the CxG chapter, but I think you meant to refer to the discussion of Bender's projection of \argst, didn't you? If so, I can change this.\\
%JPK No we meant the CxG chapter since some versions of CxG think phrasal constructions can introduce combinatorial requirements}
%Some information is claimed to be specific to lexical signs, such as phonological information (cross reference to Tseng here) and the \feat{arg-st} list, on the premise that lexical items alone specify combinatorial requirements (but see \citet{Przepiorkowski2001} for a contrary view, and see the chapter on Construction Grammar for other views questioning this assumption).
% TD 27 July: revised this sentence, replacing it with the two above.
But HPSG researchers have generally not explored this question in depth, and we will leave this issue here.
\is{lexical entry|)}

\subsection{The role of the lexicon in HPSG}

As we hope is evident by now, the lexicon plays a critical role in HPSG's explanatory mechanisms, as lexical entries encode not merely their idiosyncratic phonological and semantic characteristics, but their distributional and combinatorial potential as well.
Much of the information contained in lexical entries is geared towards modeling how words interact with one another, as we have already seen. 
As detailed in \crossrefchaptert{arg-st}, their combinatorial potential is recorded using two kinds of information, 
a list of syntactic arguments or syntactic requirements to be satisfied, and distinct lists that indicate how these requirements are to be satisfied (as local dependents, as non-local dependents, or as clitics/affixes).
Not only are syntactic arguments recorded; so is their relative obliqueness\is{obliqueness} (in terms of grammatical function), as per the partial hierarchy in (\ref{obl-hier}) from \citet[\page 266]{PollardandSag1992}. 

\begin{exe}
	\ex\label{obl-hier} \attrib{subject} $<$ \feat{primary obj} $<$ \feat{second obj} $<$ \feat{other complements}
\end{exe}

\is{Binding Theory|(}
We illustrate this explanatory role by noting the role of the lexicon in HPSG's approach to binding, as described in \citew{PollardandSag1992} (see \crossrefchapteralt{binding} for details). As lexical entries of heads record both syntactic and semantic properties of their dependents, constraints between properties of heads and properties of dependents, e.g., subject-verb agreement, or between dependents, e.g., binding constraints, illustrated in (\ref{bind}), can be stated, at least partially, as constraints on classes of lexical entries. The principle in (\ref{princ-a}) is such a constraint.

\eal
\label{bind}
\ex[]{\label{bind-a} Mathilda$_{i}$ saw herself$_{i}$ in the mirror.} 
\ex[*]{\label{bind-b} Mathilda$_{i}$ saw her$_{i}$ in the mirror.} 
\zl
\ea
\label{princ-a}An anaphor must be coindexed with a less oblique co-argument, if there is
one.\is{principle!Binding!Principle A}
\z

Principle (\ref{princ-a}) is, formally, a constraint on lexical entries that makes use of the required information in an entry's argument structure regarding the syntactic and semantic properties of its dependents. 
%Principle (\ref{princ-a}) is, formally, a constraint on lexical entries that makes use of the fact that an entry's argument structure records the syntactic and semantic properties of a word's dependents. 
The three argument structures in (\ref{a-st-bind}) illustrate permissible and ungrammatical entries. (\ref{a-st-bind-a}) illustrates exempt anaphors, as there is no less oblique syntactic argument than the anaphoric NP \citep[Section~\ref{binding:sec-excempt-anaphors}]{chapters/binding}; (\ref{a-st-bind-b}) illustrates a non-exempt anaphor properly bound by a less oblique, co-indexed non-anaphor; (\ref{a-st-bind-c}) illustrates an ungrammatical lexical entry that selects for an anaphoric syntactic argument that is not co-indexed by a less oblique syntactic argument, despite not being an exempt anaphor (i.e., not being the least oblique syntactic argument).

\eal
\label{a-st-bind}
\ex[]{\label{a-st-bind-a} 
	\avm{[arg-st & <NP$_{i, ana}$, \ldots >]
}}
\ex[]{\label{a-st-bind-b}
	\avm{[arg-st & <NP$_{i}$ NP$_{i, ana}$ >]
	}
}
\ex[*]{\label{a-st-bind-c}
	\avm{[arg-st & <XP$_{j}$, NP$_{i, ana}$ >]
	}	
}
\zl

\largerpage
Our purpose here is not to argue in favor of the specific approach to binding just outlined. Rather,
we wish to illustrate that in a theory like HPSG where much of syntactic distribution is accounted
for by properties of lexical entries, co-occurrence restrictions treated traditionally as
constraints on trees (via some notion of command) are modeled as constraints on the argument
structure of lexical entries. It is tempting to think of such a lexicalization of binding principles
as a notational variant of tree-centric approaches. Interestingly, this is not the case, as argued
in \citew{Wechsler1999}. Wechsler argues that the difference between argument structure and valence
is critical to a proper model of binding in \ili{Balinese}. Summarizing briefly, voice alternations
in \ili{Balinese}\is{diathesis alternation!voice alternation} (e.g., objective or agentive voices)
do not alter a verb's argument structure but do alter its valence -- the subject and object it
subcategorizes for. As binding is sensitive to relative \isi{obliqueness} within \feat{arg-st}, binding possibilities are not affected by voice alternations within the same clause, which are represented with different valence values. In the case of raising, on the other hand, the argument structure of the raising verb and the valence of the complement verb interact, as the subject of the complement verb is part of the argument structure of the raising verb. An HPSG approach to binding therefore predicts that voice alternations within the embedded clause will not affect binding of co-arguments of the embedded verb, but will affect binding of the raised NP and an argument of the embedded verb. This prediction seems to be borne out, as the \ili{Balinese} examples in (\ref{bal}) show. 

\eal
\label{bal}
% \ex\label{bal-a}
% \gll Ia$_{i}$ nawang {awakne$_{i}$/\changed{i}a$_{*i}$ } lakar tangkep polisi. \\
%      3rd \ig{av}.know self/3rd \ig{fut} \ig{ov}.arrest police  \\\hfill\added{(Balinese)}
% \glt `He$_{i}$ knew that the police would arrest himself$_{i}$/him$_{*i}$.'
% \ex\label{bal-b}
% \gll Cang ngaden ia$_{i}$ suba ningalin awakne$_{i}$/ia$_{*i}$ \\
%      1sg \ig{av}.think 3rd already \ig{av}.see self/3rd \\
% \glt `I believe him$_{i}$ to have seen himself$_{i}$/ him$_{*i}$.'
% \ex\label{bal-c}
% \gll Cang ngaden awakne$_{i}$ suba tingalin=a$_{i}$.\\
%      1sg \ig{av}.think self$_{i}$ already \ig{ov}.see=3 \\
% \glt `I believe him to have seen himself.'
%
\ex\label{bal-a}
\gll Ia$_{i}$ nawang       awakne$_{i}$ / ia$_{*i}$  lakar    tangkep        polisi. \\
     3        \ig{av}.know self        {} 3                   \ig{fut} \ig{ov}.arrest police  \\\hfill(Balinese)
\glt `He$_{i}$ knew that the police would arrest himself$_{i}$/him$_{*i}$.'
\ex\label{bal-b}
\gll Cang      ngaden        ia$_{i}$ suba     ningalin   awakne$_{i}$ / ia$_{*i}$ \\
     1\ig{sg} \ig{av}.think 3        already \ig{av}.see self        {} 3 \\
\glt `I believe him$_{i}$ to have seen himself$_{i}$/him$_{*i}$.'
\ex\label{bal-c}
\gll Cang ngaden        awakne$_{i}$ suba    tingalin=a$_{i}$.\\
     1\ig{sg}    \ig{av}.think self$_{i}$   already \ig{ov}.see=3 \\
\glt `I believe him to have seen himself.'
\zl

Sentence (\ref{bal-a}) shows that the proto-agent (the first element of \feat{arg-st}) of the subject-to-object raising verb \word{nawang} `know' can bind the raised subject (which in this case corresponds to the proto-patient of the complement verb \word{tangkep} `arrest' since that verb is in the objective voice). Sentence (\ref{bal-b}) shows that the raised (proto-agent) subject of the complement verb can bind its proto-patient argument. Critically, sentence (\ref{bal-c}) shows that the raised proto-patient (second) argument of the complement verb can be bound by the complement verb's proto-agent. The contrast between sentences (\ref{bal-b}) and (\ref{bal-c}) illustrates that while binding is insensitive to valence alternations (the same proto-agent binds the same proto-patient argument in both sentences), raising is not (the proto-agent argument is raised in (\ref{bal-b}) and the proto-patient argument in (\ref{bal-c})). As Wechsler argues, this dissociation between valence subjects and less oblique arguments on the \feat{arg-st} list is hard to model in a configurational approach to binding that equates the two notions in terms of c-command or the like. What is important for our purposes is that a ``lexicalization'' of argument structure, valence, and binding has explanatory power beyond tree configurations, illustrating some of the analytical possibilities informationally rich lexical entries create. 

See also \crossrefchapterw[Section~\ref{binding:toba-batak}]{binding} for a more detailed discussion of parallel data from \ili{Toba Batak}.
\is{Binding Theory|)}

\subsection{Lexical vs.\ constructional explanations}


As we have noted above, HPSG posits that much of the combinatorics of natural language syntax is lexically determined; lexical entries contain information about their combinatorial potential and, as a consequence, if a word occurs in two distinct syntactic contexts, it must have two distinct combinatorial potentials. Under this view, phrase-structure rules are boring and few in number. They are just the various ways for words to realize their combinatorial potential. In the version of HPSG presented in \citew{PollardandSag1994}, for example, there are only a handful of general phrase-structural schemata, one for a head and its complements, one for a head and its specifier, one for a head and a filler in an unbounded dependency and so forth, and the structure of clauses is relatively flat in that relations between contexts of occurrence of words is done ``at the lexical level'' rather through operations on trees that increase the depth of syntactic trees. 
%JPK 3-3-21 I added a little in the preceding sentence

In a transformational approach, on the other hand, relations between contexts of occurrence of words are seen as relations between \emph{syntactic} trees, and the information included in words can thus be rather meager. In fact, in some recent approaches, lexical entries contain nothing more than some semantic and phonological information, so that even part of speech information is something provided by the syntactic context (see \citealt{Borer2003,Marantz1997}). In some constructional approaches\indexcxg (\citealt{Goldberg1995}, for example), part of the distinct contexts of occurrence of words comes from phrase structural templates that words fit into. So again, there can be a single entry for several contexts of occurrence.

HPSG's approach to lexical knowledge is quite similar to that of Categorial Grammar\indexcg (to some degree this is due to HPSG's borrowing from Categorial Grammar important aspects of its treatment of subcategorization).\footnote{
See also the chapters by \crossrefcitet[Section~\ref{evolution:sec-early-hpsg}]{evolution} on the history of HPSG and by \crossrefcitet{cg} for a comparison of HPSG and Categorial Grammar.} As in HPSG, the combinatorial potential of words is recorded in lexical entries so that two distinct contexts of occurrence correspond to two distinct entries. The difference from HPSG lies in how lexical entries relate to each other. In many forms of Categorial Grammar (be it Combinatorial or Lambek-calculus style), relations between entries are the result of a few general rules (e.g., type raising, function composition, hypothetical reasoning, etc.) (see \citealt{Dowty1978} for an approach that countenances lexical rules, though).
% TD 30 July: what is ``hypothetical reasoning'' here?
%JPK 5 August: you hypothesize that you have seen something, say a direct object you are missing, and then when the time comes you say: hey here is a direct object, I guess my hypothesis was not so bad. Useful for displaced constituents, right node raising, gapping...
%JPK 5 Aug: I also added a clause here to include ref. to Dowty 1978, as per Stefan's comments.
The assumption is that those rules are universally available; however, those rules may be organized in a type hierarchy and an individual language might avail itself of only a portion of this hierarchy, as in \citet{Baldridge2002}. Relations between entries in HPSG can be much more idiosyncratic and language-specific. We note, however, that nothing prevents lexical rules constituting a part of a Categorial Grammar (see \citealt{Carpenter1992b}),
% whole chapter
so that this difference is not necessarily qualitative, but concerns how much of researchers' efforts are typically spent on extracting lexical regularities; HPSG has focused much more, it seems, on such efforts.

\section{The hierarchical lexicon}
\label{sec:hier-lex}


We\is{hierarchical lexicon|(}\is{type hierarchy!lexical|(} have now seen that \isi{lexicalism}
demands that lexical entries be information rich, in order to encode what might otherwise be
represented as syntactic rules.  To avoid massive and redundant stipulation throughout the lexicon,
we need mechanisms to represent the regularities within it. Two main mechanisms have been used in
HPSG to achieve this. The first mechanism is the organization of information shared by lexical
entries or parts of entries into a hierarchy of types, in a way quite similar to semantic networks
within knowledge representation systems (see, among others, \citealt{BrachmanandSchmolze1985}). This
hierarchy of types (present in HPSG since the beginning: \citealt{PollardandSag1987} and the seminal
work of \citealt{Flickingeretal1985}, \citealt{Flickinger1987}) ensures that individual lexical
entries only specify information that is unique to them. The second mechanism is lexical rules,
which relate variants of entries, and more generally, members of a lexeme's morphological family
(which consists of a root or stem as well as all stems derived from that root or stem) or members of
a word's inflectional paradigm.

% TD, 14 Nov. 19: a bit more Generativ Lexicon stuff added, starting here
HPSG is, of course, not the only linguistic framework to exploit inheritance, although HPSG
researchers, perhaps more than others, have emphasized its central role in expressing lexical
generalizations. Appeals to similar mechanisms feature prominently in Generative Lexicon (GL)
accounts of lexical semantics, for example.  Both the lexical typing structure and qualia structures
within GL, in particular the formal quale, have values situated in type hierarchies
\citep{PustejovskyandJezek1996} and GL accounts of coercion and metonymy rely crucially on multiple
inheritance within qualia values.
% TD, 14 Nov. 19: end of additional material
%TD, 15 Nove. 19: Did some editing

In this section, we discuss the hierarchical organization of the lexicon into cross-cutting classes
of lexical entries at various levels of generality.  We examine two distinct techniques for
\isi{inheritance}, which are not mutually exclusive.  One is to create subtypes directly, with
pertinent additional constraints stated for each subtype.  Different classes of words are thus
reified as subtypes of \type{word} (or \type{lexical-sign}) in the hierarchy, and all lexical items
that belong to that subtype inherit its constraints.  Another technique, more prevalent in current
HPSG work, uses implicational statements.  If certain properties hold of a lexical item (for
example, if its \attrib{aux} value is +), then others must hold as well (e.g., it subcategorizes for
a VP complement, whose subject is token identical to the auxiliary verb's).  These statements need
not involve all of the information that's present in the entire \type{word}, so they need to refer
only to substructures within \type{word} objects, like their \attrib{synsem} values.


\subsection{Inheritance}

\largerpage
\is{inheritance|(} 
All grammatical frameworks classify lexical entries to some extent, of course.
Basic part of speech information is one obvious case.  This high-level classification is present in
HPSG, too, as part of the hierarchy of types of heads. That information is recorded in the value of
the \attrib{head} feature. A simple hierarchy of types of heads is depicted in
Figure~\ref{pos-hier}.

\begin{figure}
\begin{forest}
type hierarchy
[head 
  [noun] 
  [verb,calign with current] % should be a straight line down from parent
  [~~\ldots~~] ]
\end{forest}
\caption{\label{pos-hier}A hierarchy of subtypes of \type{head}}
\end{figure}

Each of these types is (typically) a partial specification of a lexical entry's head properties.
Typing of \attrib{head} information allows the ascription of appropriate properties to different classes of lexical entries. For example, case information is only relevant to nouns in English, and whether a verb is an auxiliary or not is only relevant to verbs. 
The subtypes of \type{head} in Figure~\ref{pos-hier} allow us to define additional specifications of the properties appropriate for different parts of speech.
For example, English lexical entries with a \attrib{head} value of \type{noun} contain an attribute for \attrib{case}, while those for \type{verb} contain the attributes \attrib{aux}, \attrib{tense}, and \attrib{aspect}, as shown in (\ref{pos-def}) (we use implicational statements in (\ref{pos-def}) to indicate feature appropriateness conditions for the types \type{noun} and \type{verb} for perspicuity only; such conditions would be part of the grammar's signature\is{signature}, see \crossrefchaptert[\pageref{formal:page-signature}]{formal-background}).\footnote{%
Strictly speaking the logic works in both ways: the presence of features like \textsc{case} makes it possible to infer the presence of the type \type{noun} and the type \type{noun} requires the feature \textsc{case} to be there. We focus on the first implication here.
}
In other words, the grammar's signature will specify that only for nouns (those lexical entries whose \attrib{head} value is of type \type{noun}) is the attribute \attrib{case} appropriate.
Similarly, only for verbs are the attributes \attrib{aux}, \attrib{tense}, and \attrib{aspect} appropriate.
% TD 30 July: I reworded these a bit, but what they say doesn't accord well with what's in (19).  The implication is the other way, and (19) should say (I think) that if a lexical entry has a CASE attribute, it's a noun.
% TD 30 July: Given that the statements in (19) aren't meant to be the correct formal stements anyway, I think it migh be best to avoid that appearance, and just state them partly in words.  As you say, the real formal statement resides in the grammar's signature.
%  TD 30 July: So I might recasr (19) something like this:
% If the attrbute \attrib{case} is an attribute within a lexical entry's \attrib{head} value, then the value of \attrib{head} is of type \type{noun}.
% If the attrbute \attrib{aux}, \attrib{tense}, or \attrib{aspect} is an attribute within a lexical entry's \attrib{head} value, then the value of \attrib{head} is of type \type{verb}.
%JPK 5 August: Agreed, changed as you suggested

\ealnoraggedright
\label{pos-def}
\ex\label{pos-def-a} 
If the attribute \attrib{case} is an attribute within a lexical entry's
\attrib{head} value, then the value of \attrib{head} is of type \type{noun}.  
\ex\label{pos-def-b} 
If the attributes \attrib{aux}, \attrib{tense}, or \attrib{aspect} are attributes within a lexical entry's
\attrib{head} value, then the value of \attrib{head} is of type \type{verb}.  
\zl
% moved to where it is referenced in the text. St. Mü. 20.02.2021

Typing of parts of speech thus lets us specify what it means for a part of speech to be a noun or a verb in a particular language (of course, there will be strong similarities in these properties across languages) and omit for individual noun and verb entries properties they share with all nouns and verbs.
%Each type in (\ref{pos-hier}) includes in its definition a specification of which features are appropriate for it, as shown in (\ref{pos-def}). (\ref{pos-def}) specifies what it means to be a noun or a verb in a particular language (of course, there will be strong similarities in these properties across languages). 
% TD, 27 July: revised in accordance with what I think {pos-def} should look like


% TD, 27 July: Indeed; here are the two paragraphs from this part of the two version of this section, but I don't think either one is what we wish to say, so below them is my attempt at a new version

% The structures in (\ref{pos-def}) are essentially abbreviatory conventions.
% By themselves, they perform no function of representing lexical generalizations.
% However, to the extent that expressing such generalizations makes reference to these features, it may be warranted to reify them as subtypes of \type{word} in the type hierarchy.
% We will examine some ways of expressing contentful generalizations in the following section.

% More technically, each type of head imposes some constraint on lexical entries of that type. Thus, (\ref{pos-def-a}) requires all noun lexemes to be eligible for case information. Here, the constraint on each type specifies the value of \feat{synsem|loc|cat|head}, as the atomic value \type{noun}, \type{verb}, etc. Since these atomic values are disjoint, and since the \attrib{head} value is unique for each lexical entry, the types in (\ref{pos-hier}) are also disjoint. If there's a type corresponding to each possible \attrib{head} value, then they constitute a partition of lexical entries as well. Lexical entries for particular lexemes make use of the  definitions of types like (\ref{pos-def}) to abstract information that is shared across classes of entries. Thus, the pronoun \word{him} need only include the fact that its \attrib{head} is of type \word{noun}; the fact that it might bear case can be inferred. Similarly, the entry for the verb \word{can} need only include information that its head information include the specification [\attrib{aux} \, +] for us to be able to infer that it is a \type{verb}. 

The statements in (\ref{pos-def}) are in some sense merely definitional, as noted.
% Their usefulness arises from two sources.
% First, since the types \type{noun} and \type{verb} are disjoint, and since the \attrib{head} value is unique for each lexical entry, the types in (\ref{pos-hier}) ensure that only nouns can have case values and only verbs can have aux, tense, and aspect values.
% TD, 30 July: I may be incorrect about this, so I've removed it and revise a bit below.
% TD, 27 July: for this to be true, though, wouldn't we need the implication to be bidirectional?  And how does this interact with feature appropriateness conditions?  Where are attributes like CASE introduced?
%JPK July29: Well it's part of the signature technically, I revised the wording above to reflect that
But they allow us to state just once the general information that applies to whole classes of lexical entries.
Thus, the pronoun \word{him} need only include the fact that it bears accusative case; the fact that it is a noun can be inferred. 
Similarly, the entry for the verb \word{can} need only include the head information \avm{[aux & $+$]} for us to be able to infer that it is a \type{verb} (assuming \attrib{aux} is not an appropriate attribute for another type).

\subsection{Representing lexical generalizations}

So far, we have merely shown an HPSG implementation of a part of speech taxonomy, but once we consider subtypes with additional constraints, the utility of the hierarchical lexicon within a lexicalist framework becomes apparent.
There are interesting generalizations to be made about more specific classes, such as transitive verbs, past participles, or predicators denoting caused motion (regardless of their part of speech).
In the hierarchical lexicon, we can represent these ``interesting'' classes as types.
Which classes are worth positing in the grammar of a given language depends on the properties of its grammar; thus we expect lexical classes to specify a mix of cross-linguistically common (possibly, in some cases, universal) and language-particular constraints.

A seemingly straightforward way to ``capture generalizations about the elements of the lexicon'' is to posit a hierarchy of subtypes of \type{word}.
Thus types such as \type{verb-word} and  \type{noun-word} specify properties of verbs and nouns, and types such	as \type{subj-control-pred} and \type{obj-control-pred} specify properties of predicates that exhibit subject and direct object control.
Individual lexical items belong to multiple types in the hierarchy; the verbs \word{try} and \word{attempt} inherit the information from \type{verb-word} and \type{subj-control-pred}, while the nouns \word{attempt} and \word{effort} inherit the information from \type{noun-word} and \type{subj-control-pred}.

\citet{AckermanandWebelhuth1998}
% TD, 27 July: this citation comes out wrong in the PDF; not sure why, and don't know about citet
 use this kind of hierarchy of subtypes of \type{word} in their accounts of \ili{German} passives and other phenomena, which we will discuss briefly in the following section.
In this case, the information involved in their account is both morphological and syntactic, and they propose a hierarchy of verb types at the \type{word} level.

However, a hierarchy of subtypes of \type{word} is, while formally feasible, potentially rather inelegant.
Note first that types like \type{verb} and \type{noun} are already defined as subtypes of the type \type{head}.
There is an obvious danger of redundancy if we additionally posit parallel subtypes of \type{word} such as \type{verb-word} and  \type{noun-word}, serving no other function than as types with the corresponding \attrib{head} values.
Furthermore, signs in HPSG are structured objects, with their various kinds of information deliberately arranged in a way that associates pieces of information that ``travel together.''
The information within \attrib{head}, for example, is grouped there because it is all subject to the
Head Feature Principle\is{principle!Head Feature}.
Both part-of-speech and control information are found within \attrib{synsem}, as phonological information has no bearing on these things.
So rather than creating subtypes of \type{word} to capture regularities in the lexicon, we would prefer to express those regularities as constraints on subtypes that encompass only the information that's pertinent.
These are the smallest, ``narrowest'' portions of \type{word} objects that include all that information; the remaining portions of a \type{word} can be ignored in this context.
In other words, we take advantage of HPSG's \isi{feature geometry} and of the hierarchies of types appropriate for a particular substructure within signs to express generalizations as ``locally'' as possible (see \crossrefchapteralt{formal-background}).

Implicational statements\is{constraint!implicational} can serve well for expressing generalizations as ``locally'' as possible; they constrain the range of possible values of attributes and can stipulate structure sharing among them.
As a simple example, consider the possible complements of prepositions.
Unlike verbs, which, at least in some languages, can have multiple elements on their \attrib{comps} lists, prepositions are limited to at most one.
There are no ditransitive prepositions (as far as we are aware).
The following statement expresses this generalization in \ili{English} as well as more formally.

\ealnoraggedright
\ex If a \type{category} object has a \attrib{head} value of type \type{prep}, then the value of its \attrib{comps} feature is a list that contains at most one element.
\ex \avm{
[%\type*{cat}
head & prep ] } \impl
\avm{
[comps & \eliste{} $\vee$ < \type{synsem} >]
}
\zl

%JPK 3-3-21 Added examples before linking constraints
A more extensive example concerns linking of semantic roles to syntactic arguments, and is drawn from the work of \citet{DavisandKoenig2000b}, \citet{Davis2001}, and \citet{KoenigandDavis2003}. Consider the examples in (\mex{1}).

\eal
\ex \label{link-ex1} Rover killed the squirrel.
\ex \label{link-ex2} Rover dragged the toy to the den.
\ex \label{link-ex3} Rover jumped over the fence.
\zl

\noindent
The mapping from semantic roles to subjects and objects in these sentences can be described by the following informally stated constraints:\is{linking}

\begin{exe}
\ex \begin{xlist}
\ex Causal verbs link the causer to the subject.
\ex Caused motion verbs link the causer to the subject and the moving entity, if distinct from the causer, to the direct object.
\ex However, caused motion verbs in which the causer and moving entity are the same thing can link both to the subject (and needn't have a direct object).
\end{xlist}
\end{exe}

\noindent
The second and third statements are subcases of the first, so ideally we prefer to state the
substance of the first statement just once, rather than repeat it.  We could posit subtypes of
\type{word}, along the lines of the approach mentioned above, such as \type{transitive-verb} and
\type{caused-motion-verb}.  But implicational statements provide an arguably simpler way to model
the facts of linking.  Since the constraints we wish to express concern both \feat{arg-st} and
\attrib{cont}, our implications are stated on \type{local} objects, which are the minimal type of
object containing these attributes.  We presuppose here a hierarchy of semantic relation types as
values of \attrib{cont}, including \type{cause-rel}, \type{motion-rel} and their subtype
\type{caused-motion-rel}, each of which licenses attributes for the required participant roles.

First, we require that the causer, denoted in (\ref{cm-trans-verb-lc}) by the value of \attrib{act},
be linked to the subject (the first element of \feat{arg-st}):

\ealnoraggedright
\ex If a \type{synsem} object's \feat{cat|head} value is of type \type{verb}, and its \attrib{cont} value is of type \type{cause-rel}, then its value of \feat{cont|act} is token identical to the index of the first element of its \feat{arg-st} list.
\ex\label{cm-trans-verb-lc}
\avm{
[\punk{cat|head}{verb} \\
 cont & cause-rel ]} \impl
\avm{
[arg-st & <NP$_{\1}$, \ldots> \\
 cont & [act & \1] 
 ]
	}
\zl
% TD, 30 July: I think the left-hand side of this needs to be fixed;
% It should have two lines, the first with CAT|HEAD verb, and the second with CONT cause-rel; this will then exclude nominalizations
%JPK 5 August: Done

Then, we link the moving entity in a caused motion verb, denoted in (\ref{move-link}) by the value
of \attrib{und}, to any NP on \feat{arg-st}:

\ealnoraggedright
\ex If a \type{synsem} object's \feat{cat|head} value is of type \type{verb}, and its \attrib{cont} value is of type \type{move-rel}, then its value of \feat{cont|und} is token identical to the index of some NP element of its \feat{arg-st} list.
\ex \label{move-link}
\avm{
[\punk{cat|head}{verb} \\
 cont & move-rel ]}
\impl
\avm{
[arg-st & <\ldots, NP$_{\1}$, \ldots> \\
cont   & [und & \1] \\
 ]}
\zl
% TD, 30 July: Here, to be picky (and consistent with the wording in (26), the left-hand side should have two lines, the first with CAT|HEAD verb, and the second is what we have here already, CONT move-rel
%JPK 5 August: done

Both of these implicational statements\is{constraint!implicational} apply to a verb with a \attrib{cont} value of type \type{caused-motion-rel}.
Note that if the causer and the moving entity are distinct, they will be realized as separate NPs on the \feat{arg-st} list.
This is the linking pattern we find in numerous verbs, such as \word{throw}, \word{lift}, \word{expel}, and so on.
In some cases, however, the causer and the moving entity may be one and the same.
If the \attrib{act} and \attrib{und} values are identical in \attrib{cont}, then the second implication allows the moving entity to be realized as the subject, or as a reflexive direct object, as in:

\ea
\label{roll}
The kids deliberately rolled (themselves) down the hill.
\z
%\itdopt{But without ``themselves'' the kids are not the causer.\\
%JPK Apparently not in English :). What we say is correct for natives}

What is ruled out by this pair of statements, though, is a hypothetical verb \word{quoll}, with a linking pattern like this:

\begin{exe}
\ex[*]{\label{quoll}
The rock quolled the kids down the hill. \\
Intended: `The kids rolled the rock down the hill.'
}
\end{exe}

Additional restrictions may apply to some verbs of motion.
For instance, many verbs of locomotion entail that the causer and moving entity are identical, and allow only an intransitive variant:

\begin{exe}
\ex\label{stroll}The kids strolled (*themselves) down the hill.
\end{exe}

We could represent this identity using another constraint, solely within \attrib{cont}, as follows, where the type \type{self-move-rel} is a subtype of \type{move-rel}:

\ealnoraggedright
\ex If a \type{synsem} object's \feat{cat|head} value is of type \type{verb}, and its \attrib{cont} value is of type \type{self-move-rel}, then its values of \feat{cont|act} and \feat{cont|und} are token identical.
\ex
\avm{
[\punk{cat|head}{verb} \\
 cont & self-move-rel ]}
\impl 
\avm{
[cont & [act & \1\\
         und & \1 ]] }
\zl
% TD, 30 July: Here too, the left-hand side should have two lines, the first with CAT|HEAD verb, and the second is what we have here already, CONT self-move-rel
%JPK 5 August done

When we consider the most specific types of the lexical hierarchy, where individual lexical entries reside, the same kinds of constraints, pertaining solely to a given lexical entry's phonological form, inflectional class, specific semantics, register, and so forth, can be employed.
This lexeme or word-specific information needs to be spelled out somewhere in any grammatical framework.
We can now view this as just the narrowest, most particular case of specifying information about a class of linguistic entities.
At the same time, information shared across a broader set of lexical entries need not be stated separately for each one.\is{diathesis alternation!locative alternation}
Thus, the phonology of the word \word{spray} and the precise manner of motion of the particles or liquid caused to move in a spraying event are unique to this lexical entry.
However, much of its syntactic and semantic behavior -- it is a regular verb,  participating in a locative alternation, involving caused ballistic motion of a liquid or other dispersable material -- is shared with other \ili{English} verbs such as \word{splash}, \word{splatter}, \word{inject}, \word{squirt}, and \word{smear}.
To the extent that these ``narrow conflation classes'', as \citet{Pinker1989} terms them, are founded on clear semantic criteria, we can readily state syntactic and semantic constraints on the appropriate types in the relevant type hierarchy.
Thus much of the semantics of a verb like \word{spray} need not be specified at the level of that individual lexical entry.
Apart from the broad semantics of caused motion, shared by numerous verbs, the verbs in the narrow conflation class containing \word{spray} share the selectional restriction, noted above, that their objects are set in motion by an initial impulse and that they are liquid or particulate material.
We might therefore posit a subtype of the type \type{caused-motion-rel} to represent this shared semantics triggering the locative alternation, with further subtypes for the semantics of the individual verbs.
Note that not all these constraints apply to precisely the same class (there are other verbs with somewhat different semantics, like \word{load} and \word{wrap}, exhibiting the locative alternation, for example), so several types might be required.

To sum up the import of these brief examples, the substance of the hierarchical lexicon need not be directly expressed in terms of subtypes of \type{word}, but rather in implicational statements that express constraints among types in the structures inside lexical entries.
Interactions among these statements provide a way for classes of lexical items to inherit and share properties, so that they need not specify the same information over and over again.

% TD, 30 July: deleted the duplicate paragraph that was here (it's at the beginning of the section).

%\subsection{Inheritance}
% TD, 27 July: I've deleted most of what was here in the first part of this section, since it's just duplicate text from the Inheritance section above
% TD, 27 July: I've moved one paragraph up to that section, and left the rest as is for the moment, but it will need a bit of work.

% TD, 27 July: The following is duplicated material, appearing above.  Whether we want it here, there, or nowhere is to be decided
%So far, this is merely an HPSG implementation of a part of speech taxonomy, but once we consider subtypes with additional constraints the utility of the hierarchical lexicon within a lexicalist framework becomes apparent.
%There are interesting generalizations to be made about more specific classes, such as transitive verbs, or past participles, or predicators denoting caused motion (regardless of their part of speech).
%In the hierarchical lexicon, we can represent these ``interesting'' classes as types.
%Which classes are worth positing in the grammar of a given language depends on the properties of its grammar; thus we expect lexical classes to specify a mix of cross-linguistically common (maybe, in some cases, universal) and language-particular constraints.

% TD, 30 July: Take a look at the remaining several paragraphs from here on to the end of the subsection, and let me know if you think there's anything of value in them that we haven't already covered above.  My sense is that we could just junk all of this.
%JPK 5 August: Just did, let's see if we feel something is missing!
%Consider some of the subtypes of verbs shown in (\ref{verb-hier}) adapted from \citew{Boumaetal2000b}. We adopt Bouma et al.'s hierarchy of words for expository purposes, although, as mentioned above, some of the generalizations it encodes should be modeled more locally, e.g., the information that a verb is or is not an auxiliary should be restricted to its \attrib{head} value.
% TD, 27 July: Are they really treating these as subclasses of verbs (i.e. of \type{word}, with a \attrib{ahead} value of \type{verb}?  If so, we should be clear about this. 
%JPK July 29: yes, I pointed this fact out now.
%\begin{exe}
%	\ex\label{verb-hier}
%	\begin{tikzpicture}[baseline]
%	\Tree
%	[.{\type{verb-lxm}} [.{\fbox{\feat{aux/main}}}  
%	{\type{main}} {\type{aux}}
%	]
%	[.{\fbox{\attrib{vform}}}
%	{\type{fin}} {\type{base}} {\ldots}
%	]
%	[.{\fbox{\feat{arg-st}}} 
%	{\type{intrans}}  {\type{trans}} {\ldots}
%	]  ]
%	\end{tikzpicture}
%\end{exe}

%Again, each subtype specifies additional information constraining the lexical entries belonging to it.
%The boxed labels have no independent formal status (although they play a role in the framework of online type construction, discussed below), indicating simply that the parent type, here \type{verb}, is partitioned by the subtypes under each box.

%JPK 5 Aug: I assumed the following was supposed to be greyed out too as it makes little sense in the new context.
%Typically, this means that each subtype specifies an atomic value for a particular attribute, out of a set of mutually disjoint values, as in the part of speech types above.
%Thus \type{main-verb} and \type{aux-verb} are disjoint subtypes of \type{verb}, with the values + and -- for the attribute \feat{synsem|loc|cat|head|aux}.


%In general, types are posited in the hierarchy when there is some additional constraint to state about them.
%We now briefly examine some of the lower levels of the lexical hierarchy; that is, some more specific lexical types that illustrate how types are one way to reduce the amount of information that needs to be stipulated in individual lexical entries and are one of the tools HPSG employs to represent lexical generalizations. We begin with  the \type{transitive-verb} type (\type{trans-vb} for short).
%Apart from requiring its \feat{arg-st} list to contain two NPs, \type{trans-vb} is further constrained, at least in \ili{English}, to be a main verb rather than an auxiliary verb (see the value of \attrib{head} \type{main} in (\ref{trans-verb}) and the hierarchy of verbal head information in (\ref{verb-hier})): there are no transitive auxiliaries in \ili{English}. So, \type{trans-vb} includes information constraining the feature values of transitive verbs that goes beyond simply specifying the nature of the \feat{arg-st} list.

%The partial representation of the type \type{transitive-verb} in (\ref{trans-verb}).

%\begin{exe}
	%\ex\label{trans-verb}
	%\avm{
	%	[\type*{trans-vb}
	%	head & main \\
	%	arg-st & < NP, NP, \ldots > ]
	%}
	
%\end{exe}

%A more specific subtype of \type{trans-vb} is \type{caused-motion-transitive-verb}, which states information about the semantics of verbs in the class as well as their subcategorization, as in (\ref{cm-trans-verb}). 

%\begin{exe}
%	\ex\label{cm-trans-verb}
%	\avm{
%		[\type*{caused-mot-trans-vb} 
%		content & [\type*{caused-motion-rel} 
%		causer & \1 \\
%		moved & \2 ]\\
%		arg-st & <NP$_{\1}$, NP$_{\2}$> 
%		]
%	}
%\end{exe}



%The information in each type constitutes constraints on objects of that type.
%With the types situated in a hierarchy, each type inherits all the constraints of its supertypes.
%Thus constraints will be inherited from supertypes.
%But additional constraints can be added; this is the principal fashion in which generalizations about classes of lexical entries can be stated.
%For example, following \citet{DavisandKoenig2000b} and \citet{KoenigandDavis2003}
%we might state a contraint on argument realization on the type \type{caused-motion-transitive-vb}, to ensure that the causer is linked to the subject and the entity that is caused to move to the direct object, as in (\ref{cm-trans-verb-lc}). (cross-reference to chapter on argument structure and linking here)
%Here, we make use of Richter's logic \citep{Richter1999} to encode constraints on information that is included in lexical entries. The constraint in (\ref{cm-trans-verb-lc}) says that if a verb's semantic content is a cause relation, the causer arguments corresponds to the index of the first NP on the \feat{arg-st} list and that if a verb's semantic content is a motion relation, the moved entity is realized as an NP. Implicational constraints such as (\ref{cm-trans-verb-lc}) relieve some of the burden of encoding generalizations over lexical entries  exclusively through the lexical type hierarchy, and can lead to a simpler model of lexical generalizations in some cases, as \citet{KoenigandDavis2003} point out. When it is preferable to use a hierarchy of lexical types or conditional constraints on the information included in lexical types remains an open issue. 

%Introducing the type \type{self-move-rel} is motivated because a sizable number of \ili{English} verbs share this property (if this weren't the case, and only, e.g., \word{stroll} behaved this way, we would simply incorporate this into the semantics of \type{stroll-rel}).
%We can see how the linking constraints and this semantic constraint interact to allow the observed linking patterns of \ili{English}.


%We can see from these brief illustrations how subtypes a can combine the constraints of the types depicted in (\ref{verb-hier}), through multiple inheritance.
%Infinitive forms of transitive verbs, for example, inherit the constraints of both %\type{infinitive-verb} and \type{transitive-verb}.
%Provided that the constraints do not conflict (i.e., the descriptions of the two types unify), such a type can exist.
%Whether it is useful to reify such a type is another matter; not all possible combinations of constraints yield linguistically interesting classes of lexical entries (this is another issue we address in the discussion of online type construction).
%While an economical type-based description of regular morphological paradigms may prove descriptively adequate, it is implausible in assuming that each form of every fully regular verb is reified as a lexical entry.
%We will discuss mechanisms (lexical rules and online type construction) that offer better accounts of morphologically regular and productive word formation below.




%More specific classes of transitive caused-motion verbs, such as the \word{spray} verbs in \ili{English} that exhibit locative alternations, inherit the additional constraints in (\ref{cm-trans-verb}) and further specify additional semantic constraints that characterize these alternating verbs.
%The hierarchical organization of lexical types allows us to state these additional restrictions, which are often language-particular, in the appropriate place without additional formal mechanisms.
%The range of ditransitive constructions, to take one such case, varies across languages, with some lacking them entirely and others freely allowing them in, e.g., morphologically productive causatives of any transitive verb.
%For those languages, like \ili{English}, in between these extremes, semantic (and possibly other) constraints can be placed on the type \type{ditransitive-verb}, limiting such verbs to those involving, e.g., transfer of possession.

%We now illustrate how the organization of the lexicon in a hierarchy of lexical types minimizes the information that needs to be specified within individual entries, such as those for the forms of the \ili{French} verb \word{va} we provided earlier (see (\ref{va-lx})). We start with semantics and how it links to the argument structure. We can infer that the use of \word{va} illustrated in (\ref{va-a}) includes two arguments, a theme and a goal, from the hierarchy of semantic relations, which ensures that all types of directed motion events, of which \type{go-rel} is a subtype, includes these two arguments (see \citet{Davis2001} for such an approach to semantic relations). The linking of these arguments to an NP and PP follows either from linking types, as in \citew{DavisandKoenig2000b} or \citew{Davis2001}, or from constraints similar to those we show above in (\ref{cm-trans-verb-lc}) for \ili{English} caused-motion verbs. The relation between the argument structure of \word{va} and its subcategorization requirements for a subject and PP complement follows from general constraints on words and a general type for intransitive verbs, analogous to (\ref{trans-verb}) for transitive verbs. The inflectional features of this form are instantiations of possible combinations of values of mood, tense, and agreement information within \ili{French} verbs. Finally, the expression of these inflectional features is the result of either general lexical rules (see \citet{MillerandSag1997} for some examples) or, as in more recent work in HPSG, a network of associations between morphosyntactic features and forms at various positions in the word (see \citealt{CrysmannandBonami2016}). In the end, nothing but the meaning of this use of \word{va} and the fact that the stem form is \word{v-} need be stipulated in the entry.
% TD, 30 July: end of proposed deletion region

\is{inheritance|)}

\subsection{Cross-cutting types in the lexicon}
% TD, 30 July: We might not need a separate subsection for this, but I've retitled this subsection to match its contents better.
\is{inheritance!multiple inheritance|(}
Having now illustrated the use of implicational statements to specify constraints on classes of lexical entries at various levels of generality, we present in this section an example of cross-cutting types, each expressing some generalization about a class of words.
Drawn from \citew{AckermanandWebelhuth1998}, this sample analysis concerns \ili{German} passives,\is{passive} which come in several varieties, each with its own constraints.
Each passive construction uses a different auxiliary (\word{werden}, \word{sein}, or \word{bekommen}) and two of these constructions require a participial form of the verb, while the \word{sein} passive requires \word{zu} followed by an infinitive VP.
Additionally, passives appear attributively, as NP modifiers, as well as predicatively.
Here are two examples of the \word{zu} + infinitive passive, the first attributive, the second predicative:

\begin{exe}
	\ex\label{zu-pass}
	\begin{xlist}
		\ex\label{zu-pass-a}
		\gll die dem Mann von Johann zu schenkenden Blumen \\
		the the man by Johann to give flowers  \\
		\glt `the flowers that must be given to the man by Johann'
		\ex\label{zu-pass-b}
		\gll weil die Blumen dem Mann von Johann zu schenken sind \\
		because the flowers the man by Johann to give are \\
		\glt `because the flowers must be given to the man by Johann'
	\end{xlist}
\end{exe}

Ackerman \& Webelhuth's account of \ili{German} passives posits a multiple inheritance hierarchy of lexical types (note that these are all subtypes of a type \type{word}, not subtypes of values within it). A portion of their hierarchy of \ili{German} passive types is shown in Figure~\ref{fig:pass-hier}.
The suffix \type{-lci} on the names of types in this figure stands for ``lexical combinatorial item'', which is basically equivalent to lexical entry.


\begin{figure}
	% \begin{forest}
	% 	[{\type{univ-pas-bas-lci}}
	% 	[{\type{univ-do-pas-lci}}, name=D
	% 	[{\type{german-zuinf-pas-lci}}, name=Z
	% 	[{\type{german-pers-zuinf-pas-lci}} 
	% 	[{\type{german-long-pers-neutral-zuinf-pas-lci}}, name=PZ
	% 	[{\type{german-long-pers-zuinf-pas-lci}} ]
	% 	[{\type{german-long-attr-zuinf-pas-lci}} ]
	% 	]		
	% 	]
	% 	]
	% 	]
	% 	[{\type{german-pas-lci}}, name=G
	% 	[ , no edge ]
	% 	[{\type{german-long-pas-lci}}, name=L
	% 	[ , no edge ]  ]
	% 	[{\type{german-io-pas-lci}} ]	
	% 	]
	% 	[ {\type{univ-imp-pas-lci}} ]
	% 	[ {\ldots} ]
	% 	]
	% 	\draw(G.south) -- (Z.north); 
	% 	\draw(L.south) -- (PZ.north);
	% \end{forest}
\begin{forest}
type hierarchy
    [univ-pas-bas-lci
      [univ-do-pas-lci
        [german-zuinf-pas-lci, edge to=!un  % draw an edge to the node up (!u) and the next siebling
                                            % of this node (!un) = german-pas-lci
          [german-pers-zuinf-pas-lci]
        ]
      ]
      [german-pas-lci, calign with current, 
                       calign=first         % the center of the children is exactly below german-pas-lci
                                            % this is so since we have two daughters in the tree,
                                            % but three in the hiarchy. There will be a further edge
                                            % from german-zuinf-pas-lci.
        [german-long-pas-lci
          [german-long-pers-neutral-zuinf-pas-lci, l*=2, edge to=!r111, % draw an edge to the node (!r111),
                                % which is found by going to the first child of the root node (!r1)
                                % = univ-do-pas-lci and the first child of this node (!r11) =
                                % german-zuinf-pas-lci and the first child of this node (!r111) = german-pers-zuinf-pas-lci
            [german-long-pers-zuinf-pas-lci ]
            [german-long-attr-zuinf-pas-lci ]
          ]		
        ]
        [german-io-pas-lci ]	
      ]
      [ univ-imp-pas-lci ]
      [ \ldots, fit=band % maybe?
      ]
    ]
\end{forest}

		\caption{\label{fig:pass-hier}A portion of the hierarchy of passive lexical types according to \citet[244]{AckermanandWebelhuth1998}}	
\end{figure}

While all passives share the constraint that a logical subject is demoted, as stipulated on a general \type{univ-pas-bas-lci} passive type, the other requirements for each kind of passive are stated on various subtypes.
The \word{zu}+infinitive passive, for instance, requires not only that \word{sein} is the auxiliary and that the main verb is infinitive, but that the semantics involves some additional modal meaning.
This differs from the other passives, which simply maintain the semantics of their active counterparts.
However, the types of the passive verb \word{schenken(den)} in (\ref{zu-pass-a}) and (\ref{zu-pass-b}) both inherit from several passive verb supertypes.
As mentioned, at a general level, there is information common to all \ili{German} passives, or indeed to passives universally, namely that the ``logical subject'' (first element of the basic verb's \feat{arg-st} list) is realized as an oblique complement of the passive verb, or not at all.
A very common subtype, which Ackerman \& Webelhuth also regard as universal, rather than specific to \ili{German}, specifies that the base verb's direct object is realized as the subject of its passive counterpart; this defines personal passives.
Once in the \ili{German}-specific realm, an additional subtype specifies that the logical subject, if realized, is the object of a \word{von}-PP; this holds true of all three types of \ili{German} personal passives.
Among its subtypes is one that requires \word{zu} and the infinitive form of the verb; moreover, although Ackerman \& Webelhuth do not spell this out in detail, this subtype specifies the modal force associated with this passive construction but not the others.
Finally, both the predicative and attributive forms are subtypes of all the preceding, but these inherit also from distinct supertypes for predicative and attributive passives of all kinds.
The supertype for predicative passives constrains them to occur with an auxiliary; its subtype for \word{zu} + infinitive passives further specifies that the auxiliary is \word{sein}.
The attributive passive type, on the other hand, inherits from modifier types generally, which do not allow auxiliaries, but do require agreement in person, number, and case with the modified noun.
In summary, the hierarchical lexicon is deployed here to factor out the differing properties of the
various \ili{German} passive constructions, each of which includes its particular combination of
properties via multiple inheritance.%
\is{hierarchical lexicon|)}%
\is{inheritance!multiple inheritance|)}

% TD, 30 July: Some of the following paragrpah now precedes the summary of the preceding subsection, while the remainder could probably just be deleted..  Let me know what you think.
%The most specific types of the lexical hierarchy, where individual lexical entries reside, is where constraints pertaining solely to a given word or root -- its phonological form, inflectional class, specific semantics, register, and so forth -- are stated.
%Specific information about a word needs to be spelled out somewhere in any grammatical framework.
%In a hierarchically organized lexicon we can view this as just the narrowest, most particular case of specifying information about a class of linguistic entities.
%But where information is shared across a broader set of lexical entries, it need not be stated separately for each one.
%Thus, the phonology of the word \word{spray} and the precise manner of motion of the particles or liquid caused to move in a spraying event are unique to this lexical entry.
%However, much of its syntactic and semantic behavior -- it is a regular verb,  participating in a locative alternation, involving caused ballistic motion of a liquid or other dispersable material -- is shared with other \ili{English} verbs such as \word{splash}, \word{splatter}, \word{inject}, \word{squirt}, and \word{drizzle}.
%To the extent that these ``narrow conflation classes,'' as Pinker (1989) terms them, are founded on clear semantic criteria, we can readily state syntactic and semantic constraints at the appropriate level in the hierarchical lexicon (some, however, such as \citet{BriscoeandCopestake1999}, cast doubt on the feasibility of formulating such constraints for dative and other alternations in \ili{English}, suggesting that lexical rules might be a better alternative).
%Given this semantic similarity, it may be that much of the semantics of a verb like \word{spray} need not be specified at the level of that individual lexical entry.
%Apart from the broad semantics of caused motion, shared by numerous verbs, the verbs in the narrow conflation class containing \word{spray} share the selectional restriction, noted above, that their objects are set in motion by an initial impulse and that they are liquid or particulate material.
%We might therefore posit a subtype of the type \type{caused-motion-rel} to represent this shared semantics triggering the locative alternation, with further subtypes of that for the semantics of the individual verbs.
%Note that not all these constraints apply to precisely the same class (there are other verbs with somewhat different semantics, like \word{load} and \word{wrap}, exhibiting the locative alternation, for example), so a multitude of types in the hierarchy is crucial.

\subsection{Default inheritance in the lexicon}

\is{inheritance!default inheritance|(}%
So far, we have assumed rigid, monotonic inheritance of all information in supertypes to their subtypes; none of the inherited information can be overridden.
This runs into difficulties when dealing with lexical entries that appear to be exceptional in some way, the obvious examples being morphological irregularities.
How can productive regular forms such as *\word{childs} be blocked, and only \word{children} allowed as a lexical entry?
Under default unification, although the plural of \word{child} might inherit the information from the pertinent lexical entry and from the \type{plural-noun} type, which would entail the phonology for *\word{childs}, this regular plural form would be overridden.
\is{morphology!irregular morphology}

Several approaches to exceptions and irregularities have been proposed; we will focus first on
\emph{default unification}, and examine an alternative involving type underspecification, in the
following section. Various complex issues arise in attempting to formulate a workable system of
default unification and inheritance.  See, e.g., \citet{BriscoeandCopestake1999} for a brief
overview of various ways that default unification might be defined.
\citet{LascaridesandCopestake1999} list several desirable criteria, including these:

\begin{itemize}
	\item Non-default information is always preserved; this implies some means of  distinguishing non-default from default (overridable) information.
	\item Default unification behaves like monotonic unification whenever possible; that is, if monotonic unification is possible, the default unification mechanism should yield the same result.
	\item Default unification is order-independent; this means that it is commutative and associative, like monotonic unification.
\end{itemize}

They explore the properties of their system, called \isi{YADU}, in considerable detail.
The intent is to preserve the behavior of non-default unification in cases where no default information is present, and for defeasible information at a more specific level in the type hierarchy to override defeasible information at a more general level.\footnote{%
As \citet[\page 126]{Malouf2000a} states: ``Default inheritance as appealed to by, e.g., \citet{Sag97a}, is an abbreviatory device that helps simplify the construction of lexical type hierarchies.  When used in this way, defaults add nothing essential to the analysis. They simply provide a mechanism for minimizing the number of types required.  Any type hierarchy that uses defaults can be converted into an empirically equivalent one that does not use defaults, but is perhaps undesirable for methodological reasons.''
}

We now sketch how YADU functions, using the example of \ili{English} verb forms in
\citet{LascaridesandCopestake1999}.  The pertinent linguistic facts here are as follows:
\ili{English} past and passive participles are always identical in form, (simple) past tense
suffixes are usually the same as the corresponding participles', and the past tense suffix of most
verbs is \word{-ed}.  The last two statements are defeasible, while the first is not.  In YADU, each
type is represented with a nondefeasible typed feature structures, plus a set of defeasible feature
structures, each with an associated type.  The type hierarchy in Figure~\ref{def-verb-hier} provides
an example (here, the nondefeasible information comes first, and the set of defeasible structures
follows the slash\is{$\slash$}).\is{morphology!irregular morphology}

% Lascarides \& Copestake figure 3 (p. 61) goes here
\begin{figure}
\begin{forest}
[{\avm{[\type*{verb}
past & $\top$ \\
pastp & \2\\
passp & \2
]/\{<[past & \1 \\ pastp & \1], \type{verb}>,
    <[past & \1 \\ passp & \1], \type{verb}>\}
}} 
	[{\avm{[\type*{regverb}
past & $\top$]/\{<[past & +ed], \type{regverb}>\}
}} 
		[ {\avm{[\type*{pst-t-verb}
past & $\top$]/\{<[past & +t], \type{pst-t-verb}>\}
}} 
		]
	]
]
\end{forest}
\caption{\label{def-verb-hier} A type hierarchy of ``rules'' for past forms of English verbs, incorporating nondefault information (to the left of /) and default information (to the right of /),
 from \citet[61]{LascaridesandCopestake1999}}
\end{figure}
%JPK 3-3-21 edited the following after Tony's wording as well as caption of figure

In Figure~\ref{def-verb-hier}, the most general type \type{verb} stipulates the identity of the past participle and passive participle forms as nondefault information. The value of \attrib{past}, the simple past tense form, is unspecified, because some English verbs have irregular past tense forms (the symbol $\top$ denotes the most general type, and here indicates merely that nothing more specific can be stated about the \attrib{past} form of every English verb). On the right-hand side of the /\is{$\slash$} is default information; this states that normally, the value of \attrib{past} is shared with both the values of both participle forms, whether the verb is regular (e.g., \word{walked}) or irregular (e.g., \word{understood}), although there are also verbs for which this is not true, such as \word{give} (\word{gave}, \word{given}), which override this default.  For regular verbs (type \type{regverb}), the value of \attrib{past} will be, by default, the result of a function that suffixes \word{-ed} to the verb stem (Lascarides \& Copestake gloss over the details of morphology and phonology here), but this is defeasible. In the more specific type \type{pst-t-verb}, for instance, the default \word{-ed} is overridden by (again default) information that the suffix is \word{-t}.

Thus a \type{pst-t-verb} like \word{burn}/\word{burnt} inherits the nondefault information from \type{regverb} and \type{verb}, but overrides the regular past forms.
The default information in \type{pst-t-verb} is associated with a more specific type than that in \type{regverb}, so it takes precedence in YADU's unification procedure.
And as \citeauthor{LC99a} note (p. 62): ``This is the reason for separating \type{verb} and \type{regverb} in the example above, since we want the \word{+t} value to override the \word{+ed} information on \type{regverb} while leaving intact the default reentrancy which was specified on \type{verb}.  If we had not done this, there would have been a conflict between the defaults that was not resolved by priority.'' For morphological irregularities such as \word{children}, the same devices can be used, with a type for the lexical entry of \word{child} that overrides the regular plural form.

As an example of the use of default, nonmonotonic inheritance outside of morphology, consider the account of the syntax of gerunds\is{gerunds} in various languages developed by \citet{Malouf2000a}.
% TD, 27 July: this citation comes out wrong in the PDF; not sure why, and don't know about citet
Gerunds exhibit both verbal and nominal characteristics, and furnish a well-known example of seemingly graded category membership, which does not accord well with the categorical assumptions of mainstream syntactic frameworks.
Roughly speaking, \ili{English} gerunds, and their counterparts in other languages, act much like verbs in their ``internal'' syntax, allowing direct objects and adverbial modifiers, but function distributionally (``externally'') as NPs.
To take but a couple of pieces of evidence (see \citealt[27--33]{Malouf2000a} for more details), 
% TD 18 Dec: the reference needs to be redone in conformance with standards; this may mean hunting down Malouf's book
gerunds can be the complement of prepositions, whereas finite clauses cannot (as in (\ref{ger-n})); however, adverbs, not adjectives, can modify gerunds, while adjectives must be used to modify deverbal nouns (as in (\ref{ger-v})).


\eal
\label{ger-n}
\ex[]{\label{ger-n-a}
Pat is concerned about Sandy('s) getting arrested.}
\ex[*]{\label{ger-n-b}Pat is concerned about (that) Sandy got arrested.}
\zl
\eal
\label{ger-v}
\ex[]{\label{ger-v-a}
Pat disapproved of (me/my) *quiet/quietly departing before anyone noticed.
}
\ex[]{
\label{ger-v-b}
Pat disapproved of my quiet/*quietly departure.
}
\zl


One approach to modeling these distinctions is directly, via syntactic rules that allow an NP to be expanded as a constituent internally headed by a verb.
As \citeauthor{Malouf2000a} notes, this offers no account of the observed behavior of gerund-like forms across languages.
Some possible combinations of noun-like and verb-like attributes are frequently attested cross-linguistically in gerunds and their equivalents, while others are rare or unattested.
Cross-linguistically, gerunds vary in their subcategorization possibilities: some allow subjects and complements, while some allow only complements and no subjects.
But there appear to be no cases of gerund-like lexical items that can take a subject but cannot take complements.

Instead of such unmotivated syntactic rules, \citeauthor{Malouf2000a} posits a lexical rule, which converts the lexical category of a verb to \type{noun}, but otherwise preserves its verbal properties, such as subcategorization.
With strictly monotonic inheritance, this poses problems, as it would force us to abandon useful generalizations about nouns other than gerunds (e.g., they do not take direct object complements, as many verbs and their gerunds do).
Default inheritance provides one way to model the observed phenomena, without weakening the constraints on parts of speech to the point where no meaningful constraints distinguish them. 

In Malouf's account, there are both ``hard'' constraints -- a verb lexical entry, for example, must have a \attrib{head} value of type \type{relational} (encompassing verbs, adjectives, and adpositions) -- and ``soft,'' overridable constraints -- a verb lexical entry by default has a \attrib{head} value of type \type{verb}.
In addition, following \citet{Boumaetal2001}, he posits the types \type{ext-subj} and \type{ext-spr}.
The former constrains the \attrib{head} value to \type{relational} and the first element of the \feat{arg-st} list to be the \attrib{subj} (only adjectives, adpositions, verbs, and predicative NPs have subjects), while the latter constrains the \attrib{head} value to \type{noun} and the first element of the \feat{arg-st} list to be the \attrib{spr} (only nouns have specifiers), as shown in (\ref{ext-ss}).


\begin{exe}
\ex\label{ext-ss}
\begin{tabular}{@{}l@{~}l@{\hspace{2em}}l@{~}l}
a. &\avm{
[\type*{ext-subj}
 head & relational \\
 subj & <\1> \\
 arg-st & <\1, \ldots> ] 
}
& b. &
\avm{
[\type*{ext-spr}
 head   & noun \\
 spr & <\1> \\
 arg-st & <\1, \ldots>] 
}
\end{tabular}
\end{exe}

\citeauthor{Malouf2000a} then specifies default \attrib{head} values for the lexical classes \type{n} and \type{v} (see (\ref{v-def}) for the latter's definition). As gerunds have both properties of nominal and relational heads, they are subtypes of both, as shown in the multiple inheritance hierarchy in Figure~\ref{ger-hier}. The \type{v} type, which concerns us here, has a default \attrib{head} value \type{verb}, as shown in (\ref{v-def}) in addition to the non-default, more general type \type{relational} it also includes (default information follows $/$).


\begin{figure}
% \begin{forest}
% [{\type{head}}
% [{\type{func}}  [{\type{det}} ] [{\type{marker}} ] ]
% [{\type{subst}} [{\type{noun}} 
% 					[{\type{c-noun}} ] [{\type{gerund}}, name=G ]
% 				  ]
% 				 [{\type{relational}}, name=R
% 				 [, no edge ] 
% 				 [{\type{verb}} ] [{\type{adj}} ] [{\type{prep}} ]
% 				 ] ]
% ]
% \draw(R.south) -- (G.north);
% \end{forest}
\begin{forest}
type hierarchy
  [head
    [func
      [det]
      [marker]
    ]
    [subst
      [noun
        [c-noun]
        [gerund, edge to=!un]
      ]
      [relational, calign children={1}{2}
        [verb]
        [adj]
        [prep]
      ]
    ]
  ]
\end{forest}
\caption{\label{ger-hier}A cross-cutting hierarchy of types of \type{head} according to \citet[65]{Malouf2000a}}
\end{figure}


\begin{exe}
\ex\label{v-def}
\avm{
[\type*{v}
head & relational /verb 
%\\
% JPK 3-3-21 removed the offending cont cont & psoa
 ]
}
\end{exe}

However, in the subtype of \type{v} called \type{vger}, the default value \type{verb} is overridden.  In \type{vger}, the \attrib{head} value is of the type \type{gerund}, which is a subtype of both \type{noun} and \type{relational}, but not of \type{verb}.
The type \type{vger} is shown in (\ref{vger}); where \type{f-ing} is a function that produces the \word{-ing} form of an \ili{English} verb from its root.


\begin{exe}
\ex\label{vger}
\avm{
[\type*{vger}
morph & [
          root & \1 \\
          i-form & f-ing(\1) ]\\
head & gerund ]
}
\end{exe}

%JPK 3-3-21 edited the following after Tony's wording.
The type \type{vger} is thus compatible with ``verb-like'' characteristics.
But, as its \attrib{head} is also a subtype of \type{noun}, its \attrib{subj} list is empty and the first element on its \feat{arg-st} list is its \attrib{spr} value. In addition, gerunds allow direct complements (unlike ordinary nouns), but not subjects (unlike ordinary verbs).
Malouf's hierarchy of types makes this prediction, in effect, because the \type{ext-spr} type requires that the ``external argument'' (the first on the \feat{arg-st} list) is realized as the value of \attrib{spr}.

\largerpage
While it would be possible to construct type hierarchies of lexical types, \attrib{head} types, and so on that would allow for ``anti-gerunds'' -- those that would act externally as nouns, allow subjects, but not permit direct complements -- this would require reorganizing these type hierarchies to a considerable extent.
Given that many nouns besides gerunds -- nominalizations, for example -- are relational it could be difficult to model a hypothetical language that permits only the anti-gerunds rather than the normal ones.

\citeauthor{Malouf2000a} further notes a key difference between gerunds and exceptions like *\word{childs}/\word{children}: \ili{English} gerunds are productive (and completely regular morphologically).
If the same mechanisms of default unification are involved in both, what accounts for this difference?
His answer is that productive and predictable processes involve on-line type construction (see Section~\ref{sec:alt} for details).
The irregular form \word{children} must of course be learned and stored, not generated online.
The default mechanisms described above, however, are employed at higher levels of the lexical hierarchy, and the individual gerunds forms \emph{are} productively generated online.
Note that, in contrast to the morphological and syntactic consistency among gerunds, \ili{English} nominalizations display some idiosyncrasies that suggest at least some of them must be stored as distinct lexical items.
Thus, as Malouf emphasizes, modeling prototypicality in the lexicon within HPSG can draw on both default inheritance and on-line type construction; together, they make ``the connection between prototypicality, and productivity'' (p.\ 127).
\is{inheritance!default inheritance|)}\is{type hierarchy!lexical|)}

\section{Lexical rules}
\label{lexicon-sec-lexical-rules}


In\is{lexical rule|(} this section we describe the role lexical rules play in HPSG as well as their formal nature, i.e., how they model ``horizontal'' relations among elements of the lexicon. These are relations between variants of a single entry (be they subcategorizational or inflectional variants) or between members of a morphological family, as opposed to the ``vertical'' relations modeled through inheritance. Thus they provide a means to represent the intuitive notion of ``derivation'' of one lexeme from another. 

\subsection{What is the nature of lexical rules in HPSG?}
\label{lexicon-sec-nature-of-lexical-rules}

\largerpage
While lexical rules or similar devices have been invoked within HPSG since its inception,
formalizing their nature and behavior still continues.  The intent, however, has always been, as
\citet{Lahm2016} stresses, to treat lexical rules (typically written $A \mapsto B$\is{$\mapsto$}) to
mean that for every lexeme or word described by $A$ there is one described by $B$ that has as much
in common with A as possible.

\citet{CopestakeandBriscoe1991}, \citet{BriscoeandCopestake1999}, \citet{Meurers2001}, and many
others formalize the notion of lexical rule within HPSG by introducing a type, say \type{lex-rule},
with the attributes \attrib{in} and \attrib{out}, whose values are respectively the rule's input and
output lexical descriptions. As \citet{BriscoeandCopestake1999} note, lexical rules of this form
also bear a close relationship to default unification.  The information in the input is intended to
carry over to the output by default, except where the rule specifies otherwise and overrides this
information. But, as \citet{Lahm2016} points out, a sound basis for the formal details of how
lexical rules work is not easily formulated. Meurers' careful analysis of how to apply lexical rules
to map a description $A$ into the description $B$ does not always work as intended, in that what we
would expect to be licit inputs are not always actually such, and no output description results as a
consequence. Fortunately, it is not clear that this is a severe problem in practice, and Lahm notes
that he has not found an example of practical import where Meurers' lexical rule formulation would
encounter the problems he raises.

\largerpage
In a slight variant of the representation of lexical rules proposed by Copestake \& Briscoe and
\citeauthor{Meurers2001}, the \attrib{out} attribute can be dispensed with; the information in the
lexical rule type that is not within the \attrib{in} value then constitutes the output of the
rule. \citet[87]{AckermanandWebelhuth1998} employ this style of representation; their type
\type{derived-lci} adds a \attrib{lexdtr} attribute (equivalent to \attrib{in}) that contains the
input lexical entry's information. The difference between the two representations with only the
attributes \attrib{synsem} and \attrib{phon} included for expository purposes is shown in
(\ref{two-lex}).

\begin{exe}
\ex \label{two-lex}
\begin{xlist}
\ex \label{two-lex-a}
\avm{
[in & [phon   & a\\
       synsem & b ] \\
out & [phon   & c\\
       synsem & d ] 		
]
}
\ex \label{two-lex-b}
\avm{
[ phon   & c \\
  synsem & d \\

\multicolumn{2}{l}{\textsc{in} [phon   & a\\
                                synsem & b ]} ] 
}
\end{xlist}
\end{exe}



In the variant in (\ref{two-lex-b}), lexical rules are treated as subtypes of a
\type{derived-lexical-sign}\is{derived word} type, which can combine with other types in the lexical
hierarchy, merely adding the derivational source via the \attrib{in} value.  Formulated in either
fashion, lexical rules are essentially equivalent to unary syntactic rules, with the \attrib{in}
attribute corresponding to the daughter and the \attrib{out} attribute to the mother (or the rest of
the information in the rule, if the \attrib{out} attribute is done away with). This is the way
lexical rules are implemented in the \ili{English} Resource Grammar (see
\url{http://www.delph-in.net/erg/} for demos and details about this large-scale implemented grammar
of \ili{English}) as well in the CoreGram Project and the Grammix grammar development environment
(see \citealt{MuellerGrammix} and \url{https://hpsg.hu-berlin.de/Software/Grammix/} for details on
the Grammix software). See also \crossrefchapterw[Section~\ref{cl:resources}]{cl} for remarks on
implementations.

One clear advantage of this kind of representation, i.e., a representation in which the attribute
\attrib{out} is dispensed with and lexical ``rules'' are simply subtypes of \type{derived-word} or
\type{derived-lexeme}, is that they are then positioned in the lexical hierarchy and subject to the
same implicational constraints as other classes of words.  They can also be organized in complex
networks of more or less general rules.  As \citet{Riehemann1998} and \citet{Koenig1999c} show, if
one includes in the lexical hierarchy unary-branching rules to model derivational morphology, a
unified account of derivational processes that apply both productively to an open-ended set of
lexemes as well as unproductively to another closed set of lexemes becomes possible.  Consider the
approach to derivational morphology taken by \citet{Riehemann1998}.\is{affixation} Example
(\ref{bar}) (Riehemann's (1)) illustrates \word{-bar} suffixation\is{morphology!derivational} in
\ili{German}, a process by which an adjective that includes a modal component can be derived from
verb stems (similar to \ili{English} \word{-able} suffixation).  A lexical rule approach could posit
a verb stem input and derive an adjective output.  As Riehemann stresses, though, there are many
different subtypes of \word{-bar} suffixation, some productive, some unproductive, all sharing some
information.  This combination of productive and unproductive variants of a lexical process is
exactly what the type hierarchy is meant to capture and what Riehemann's \word{Type-Based
  Derivational Morphology} capitalizes on.  The structure in (\ref{bar-der}) presents the relevant
information about Riehemann's type for regular \word{-bar} `-able' adjectives (see
\citealt[68]{Riehemann1998} for more details).  Critically, \word{-bar} adjectives include a
singleton-list base (the value of \feat{morph-b}) that records the information of the adjective's
verbal base (corresponding to the would-be lexical rule's input).  Because of this extra layer, the
local information in the base (the \type{local} object under \textsc{morph-b} \ldots{}
\textsc{local}) and the \word{-bar} adjective (the \type{local} object under \textsc{synsem|local})
can differ without being in conflict.

\largerpage
\ea
\label{bar}
\gll Sie  bemerken die Veränderung. Die Veränderung ist bemerkbar. \\
     they notice   the change       the change      is  noticeable \\
\glt `They notice the change. The change is noticeable.'
\z

\ea
\label{bar-der}
% todo avm phonliste
\avm{
[\type*{reg-bar-adj}
 phon    & \1 \+ \phonliste{ bar } \\
 morph-b & <[\type*{trans-verb}
            phon  & \1 \\
            local & local ]> \\
            \punk{synsem|local}{local} ]					
}
\z

See also the chapters by \citet[Section~\ref{morphology:sec-Riehemann}]{chapters/morphology} and \citet[Section~\ref{cxg:sec-cxg-morphology}]{chapters/cxg} for further discussion of Riehemann's proposal.


\subsection{Phenomena accounted for by lexical rules}

Lexical rules have been put to many uses: derivational and inflectional morphology (\citealt{CopestakeandBriscoe1995}; see \citealt{EmersonandCopestake2015} for an alternative approach to inflection in HPSG that is morpheme-based),
% \itdopt{Also cite \citew{Eynde94}?\\
% JPK Unfortunately, we have not read this work and it's kinda of a No-No to cite work you haven't read or even glanced at :)}
%JPK 15 Nov.2019: edited as per Guy's point
conversion in interaction with complex predicate formation \citep{Mueller2010}, negation \citep{KimandSag2002,Mueller2010}, and diathesis alternations \citep{BC99a,Mueller2003e,MuellerLFGphrasal,Davis2001}. Moreover, proposals for lexical rules in HPSG have extended beyond what are traditionally or evidently viewed as lexical phenomena, to include treatments of affixal realization of arguments, extraction, unbounded dependencies, and adjuncts \parencites{Monachesi1993}[\page 378]{ps2}{NvB94a}{Keller95b}{MS97a-u}. In this section, we describe the use of lexical rules to model the realization of arguments as extracted dependents or affixes, rather than complements. We concentrate on two of these cases (affixal realization of arguments and complement extraction), which we will contrast with alternative analyses not involving lexical rules presented by the same authors (see the next section). They thus provide a good illustration of some of the analytical choices available to model relations between variant lexical entries based on a single stem. 


\largerpage
We begin with the Complement Extraction Lexical Rule (hereafter, CELR)\is{extraction}\is{lexical
  rule!Complement Extraction}\is{unbounded dependency} proposed in \citew[\page
378]{PollardandSag1994}, shown in (\ref{celr}). The input to the rule is any lexeme that selects for
a syntactic argument (\avmbox{3}) that the lexeme requires to be expressed as a complement (as
indicated, this syntactic argument is also a member of the \attrib{comps} list). The output
stipulates that this same syntactic argument is no longer a member of the \attrib{comps} list;
however, the \attrib{slash} set now includes a new element, which is the local information of this
syntactic argument \iboxb{1}. Informally stated, the input entry specifies that a syntactic argument
must be realized as a complement, whereas the output entry specifies that the same syntactic
argument must be realized by a non-local dependent (see \citealt[Chapter~4]{PollardandSag1994} for
the distinction between \attrib{local} and \attrib{nonlocal} information).
% TD, 1 Aug.: We should have a page number from P&S 1994 for this citation; however, I just looked in there and couldn't find any mention of this issue with respect to the CELR, though maybe I missed it.
% JPK 5 Aug: Can't find it either so I reworded. 

\ea
Complement Extraction Lexical Rule (adapted from \citealt[\page 378]{PollardandSag1994}):\\
\label{celr}
	\avm{
		[arg-st & < \ldots, \3, \ldots >\\
		comps & <\ldots, \3[loc & \1], \ldots >\\
		slash & \2
		]
	}
	$\mapsto$
	\avm{
		[arg-st & < \ldots, \3[loc \1\\ slash & \{ \1 \} ], \ldots >\\
		comps & <\ldots >\\
		slash & \{ \1 \} \cupAVM \2
		]
	}
\z


A similar use of lexical rules to model alternative realizations of arguments can be found in
\citew{Monachesi1993},\is{clitic!in \ili{Romance} languages} who analyzes alternations between
complements and pronominal object affixes (traditionally called object clitics) in \ili{Italian} in
a way that parallels the \ili{French} examples in (\ref{va}). \shuffle\is{$\bigcirc$} in the rule,
shown in (\ref{cl-lr}), a.k.a.\ the ``shuffle''\is{shuffle} operation, stands for the unordered
concatenation of two lists, since any member of the input's \attrib{comps} list can be realized as a
clitic and therefore not be included in the output's \attrib{comps} list (see
\crossrefchaptert[\pageref{rel-shuffle}]{order} for a more formal explanation of \shuffle). In the
output of the lexical rule in (\ref{cl-lr}), a subset of the list of complements in the input
\iboxb{2} corresponds to a list of clitic \type{synsem}s, realized as prefixes through inflectional
rules not shown here.
%JPK 10 Aug: Define the shuffle operation

\ea
\label{cl-lr}
Clitic Lexical Rule adapted from \citew[\page 439]{Monachesi1993}:\\
\avm{
[\type*{word}
 head  & verb \\
 comps & \1 \shuffle \2 \\
 clts  & \eliste ] $\mapsto$
[\type*{word}
 comps & \1  \\
 clts & \2 \listOf{cl-ss}] }
\itddone{Monachesi does not have the typing of the list in \ibox{2}. And in fact you do not need this since once the stuff is in the list, you know that these things are clitics. It may be helpful to have this restriction if there are verbs that do not allow some of their arguments to be realized as clitics. Then this could be stated in the lexical items and would block the application of the lexical rule. Actually what you do is not Monachesi style. She had a recursive rule adding one clitic at the time, but you mix this with a mapping approach. Not sure this is ok. In any case I would have to adapt the heading.
\\
JPK You are right and the addition of the heading and making clear it's adapted is OK with us.\\
Stefan: I still think it is too far away from what Monachesi does. But if you think it is OK to discuss it like this, remove the bubble and we are done. =:-)
\\
JP Yeah, it's always the same: we can keep the original but then we have to explain. I guess in this chapter we tried (or blame it on me!) to adapt the original to avoid parochial decisions of the time, except for L\&C as this is so different that we had to reproduce the original.
}
\z

Here as well, a lexical rule is employed in an analysis of what might well be considered a syntactic phenomenon.
The possibility of treating phenomena like extraction and pronominal object affix placement at a lexical level, however, makes sense when they are considered fundamentally as matters of the combinatorial requirements of predicators, rather than effects of movement.
% \itdopt{Wasn't there also an argument with respect to idiosyncrasy by Miller \& Sag?\\
% JPK Indeed, but this would lead us to far astray, rehashing the arguments for the affixal nature of French so-called object clitics}

Before turning to the alternatives, we note in passing that lexical rules are inherently ``directional'', with an input and an output.
This seems intuitively correct in the cases we have discussed, but might not always be so.
Is there inherent directionality, for example, between the causative and inchoative alternants of verbs such as \word{melt} or \word{slide} or between the ditransitive and prepositional object frames of verbs such as \word{give}, as \citet[731]{Goldberg1991} or \citet[18--23]{Goldberg1995} ask?
The alternatives to lexical rules described in the following section lack this notion of directionality.

\subsection{Alternatives to lexical rules}
\label{sec:alt}

In this section we briefly examine two alternatives to lexical rules, each involving underspecification. The types of members of the \feat{arg-st} list might be underspecified so that a single lexical description can correspond to more than one subcategorization frame. Or the type of the entry itself may be underspecified, so that it subsumes multiple inflectional or derivational forms. In both cases, the intent is that sufficiently underspecified information covers multiple entries that would otherwise have to be specified and related by lexical rules. We begin with alternatives to the complement extraction and clitic lexical rules in (\ref{celr}) and (\ref{cl-lr}), proposed in \citew{Boumaetal2001} and \citew{MillerandSag1997}.%
\isfeat{arg-st}\is{valence}

In both cases, the idea is to distinguish between ``canonical'' and ``non"=canonical''\is{canonical synsem@canonical \type{synsem}} realizations of syntactic arguments, as shown in the hierarchy of \type{synsem} types in Figure~\ref{synsem}. ``Canonical'' means local realization as a complement or subject/""specifier, and ``non-canonical'' means realization as an affix or filler of an unbounded dependency. Linking constraints between semantic roles (values of argument positions)\is{linking} and syntactic arguments (members of \feat{arg-st}) do not specify whether the realization is canonical or not; thus they retain their original form. Only canonical members of \feat{arg-st} must be structure-shared with members of valence lists. The two constraints that determine the non-canonical realization of fillers are shown in (\ref{fillers}) and (\ref{wd-bouma}). (\ref{fillers}) specifies what it means to be a \type{gap-ss}, namely that the argument is extracted (its local information is ``slashed'') whereas (\ref{wd-bouma}) prohibits any \type{gap-ss} member from being a member of the \attrib{comps} list (see \citealt[23]{Boumaetal2001}). As these two constraints are compatible with either a canonical or extracted object, there is no need for the lexical rule in (\ref{celr}). (\attrib{deps} in (\ref{wd-bouma}) is an attribute \citeauthor{Boumaetal2001} introduce that includes not only syntactic arguments, the value of \feat{arg-st}, but also some syntactic adjuncts; $\ominus$ stands for list subtraction)
% TD, 1 Aug: Is this from Bouma, et al.?  If so we should indicate that more clearly in this paragraph (and the following examples).
%JPK 5 Aug I now make that clear and give the page number. Added stuff about DEPs

\begin{figure}
\begin{forest}
type hierarchy
[synsem
  [canon-ss] 
  [non-canon-ss
    [gap-ss]
    [aff-ss]]]	
\end{forest}
\caption{\label{synsem} Subtypes of \type{synsem}}
\end{figure}

\ea
\label{fillers}
%\label{gap}
% todo avm list(gap-ss) should not produce list(GAP-SS)
% #23 too much space after \+ and \-
		\type{gap-ss} \impl
		\avm{
		[loc & \1 \\
			slash & \{ \1 \} ]
		}
		\ex\label{wd-bouma}
		\type{word} \impl
		\avm{
			[subj  & \1 \\
			 comps & \2 \- \listOf{gap-ss} \\
			 deps  & < \1 > \+ \2
			]
		}
\z

\citet{MillerandSag1997}\is{clitic!in \ili{Romance} languages} make a similar use of non-canonical relations between the \feat{arg-st} list and the valence lists, eschewing lexical rules to model \ili{French} pronominal object affixes (traditionally called clitics) and proposing instead the constraint on the type \type{cl-wd} (the type for verbs that include object affixes in (\ref{cl-wd}),\itddone{What is this a constraint on?} where a subset of \feat{arg-st} members, those that are realized as affixes (of type \type{aff}), are not also selected as complements. 


\ea
\label{cl-wd}
Constraints on words with clitics adapted from \citew[587]{MS97a-u}:\\
\avm{
  [morph & [form   & F\sub{PRAF}(\1, \ldots) \\
            i-form & \1 ] \\
   synsem & [loc|cat & [head & verb \\
                        subj  & \2 \\
                        comps & \3 \listOf{non-aff}  \\
         		arg-st & (\2 \+ \3) \shuffle \nelistOf{aff}]]]
	}
% \itdopt{@JP: I would prefer f over F for the whole volume.\\
% JPK I am OK with it, BUT M\&S use F!\\
% Stefan: This AVM differs in so many ways from the original \ldots \\
% JPK Yes, but otherwise you have to have several AVMs and explain everything and we are trying to be ``helpful'' to the readers. But this is an art form!}
\z 

% TD, 1 Aug: I think it's worth reminding the reader a bit more how these do the same work the lexical rules do, so I've added the following; let me know what you think:
%JPK 5 Aug. Sounds good, I added one sentence that alluded to what we say later.
In both of these analyses, related sets of lexical entries that could be thought of as ``generated by lexical rules'' are instead regarded as the various possible ways of obeying constraints like those in (\ref{fillers}) or (\ref{cl-wd}).
This comes at a cost of additional types and constraints for extraction, and a loosening of requirements for the correspondence between the \feat{arg-st} list and the valence lists.
However, these approaches, in dispensing with lexical rules, sidestep both the conceptual and representational issues that we noted earlier and attempts to restrict lexical rules to cases where they cannot be avoided, e.g., derivational morphology.

\is{type underspecification|(} The second alternative to lexical rules based on underspecification
was presented in \citew{KoenigandJurafsky1994} and \citew{Koenig1999c}. Typically in HPSG, all
possible combinations of types are reified in the type hierarchy (in fact, they must be present, per
the requirement that the hierarchy be sort-resolved: \citealt{Carpenter1992},
\citealt{PollardandSag1994}), or, equivalently, that each linguistic entity be assigned exactly one
maximally specific type -- a.k.a.\ \word{species} (\citealt[78]{Richter2000};
\crossrefchapteralt[Section~\ref{sec-essentials}]{formal-background}).  Thus, if one partitions verb
lexemes into transitive and intransitive and, orthogonally, into, say, finite verbs and gerunds
(limiting ourselves to two dimensions here for simplicity), the type hierarchy must also contain the
combinations transitive+finite, transitive+gerund, intransitive+finite, and
intransitive+gerund. Naturally, this kind of fully enumerated type system is unsatisfying. For one
thing, there is no additional information that the combination subtype \type{transitive+finite}
carries that is not present in its two supertypes \type{transitive} and \type{finite}, and similarly
for the other combinations. In contrast to the ``ordinary'' types, posited to represent information
shared by classes of lexemes, these combinations seem to have no other purpose than to satisfy a
formal requirement on the mathematical structure of a type hierarchy (namely, that it forms a
\isi{lattice} under meet and join).  Second, and related to the first point, this completely
elaborated type hierarchy is redundant. Once you know that all verbs fall into two valence classes,
transitive and intransitive, and simultaneously into two inflectional classes, finite and gerund,
and that valence and inflection are two orthogonal dimensions of classification of verbs, you know
all you need to know; the type of any verb can be completely predicted from these two orthogonal
dimensions of classification and standard propositional calculus inferences.\footnote{One possible
  way of making formally explicit the idea behind on-line type construction within the
  model-theoretic approach to HPSG that is now standard
  \citep{King1989,Richter2000,chapters/formal-background} is to allow maximally specific sorts, or
  species, to be either sets of species or non-atomic sums of species, just in cases where
  orthogonal dimensions of classification have been used since \citet{Flickinger1987}. For reasons
  of space, we do not pursue this line of inquiry in this chapter.}

%JPK 6 Aug: I have added some stuff at the end of the previous paragraph to address some of Stefan's comments.

Figure~\ref{fig-verb-hier2} is a simplified  hierarchy of verb lexemes we use for strictly expository purposes, where the boxed labels in small caps \attrib{vform} and \feat{arg-st} are mnemonic names of orthogonal dimensions of classification  of subcategories of verbs (and are not themselves labels of subcategories). Inheritance links to the predictable subtypes are dashed and their names grayed out; this indicates that these types can be inferred, and need not be declared explicitly as part of the grammar. A grammar of \ili{English} would include statements to the effect that head information about verbs includes a classification of verbs into finite or base forms (of course, there would be more types of verb forms in a realistic grammar of \ili{English}) as well as a classification into intransitive and transitive verbs (again, a realistic grammar would include many more types).


\begin{figure}
% 	\begin{forest}
%        [{\type{verb}} 
%       					[{\fbox{\attrib{vform}}}
%       						[{\type{fin}}, name=A1  
%       						 [{\tc{gray}{\type{fin+intrans}}}, name=A,edge={gray,dashed}   ]    		
%       							[, no edge ] ]
%       						[{\type{base}}, name=B1       							[{\tc{gray}{\type{base+trans}}}, name=B,
%       								edge={gray,dashed}  ]   		 
%       							[, no edge ] ]
%       						] 
%       					[{\fbox{\feat{arg-st}}} 
%       					    [{\type{intrans}}, name=C1 
%       					 		[, no edge ]
%       					 		[{\tc{gray}{\type{base+intrans}}}, name=C,edge={gray,dashed}  ] ]
%       						[{\type{trans}}, name=D1 
%       						     [, no edge ]
%       						     [{\tc{gray}{\type{fin+trans}}}, name=D,edge={gray,dashed} ]   ]
%       					]  
%       	]
%       				\draw[style=dashed,gray] (C1.south) -- (A.north);
%       				\draw[style=dashed,gray] (D1.south)-- (B.north);
%       				\draw[style=dashed,gray] (B1.south)-- (C.north);
%       				\draw[style=dashed,gray] (A1.south)-- (D.north);
% \end{forest}
\begin{forest}
  type hierarchy,
  for level=3{
    instance, % Apply "instance" style to nodes on (absolute) level 3.
    l*=1.5, % increases the distance to the second level by a factor of 1.5
  },
  [verb 
    [vform,partition,
      [fin,
        [fin+intrans, edge to=!r21,
          before drawing tree={
            % Move all "instance" nodes 3em to the left.  All instance nodes are
            % given by nodewalk "cNNN" (current, next leaf, next leaf, next leaf).
            for nodewalk={cNNN}{x-=3em},
            % Non-official style. It pushes the nodes of the given nodewalk
            % apart: the first node is not moved at all, the last one is moved
            % by #1 (here, 6em). The spreading is uniform (in the sense that
            % the gaps between all nodes are increased by the same amount), and it
            % occurs in dimension given by #2 (here, "x").
            distribute space={6em}{x}{cNNN},
            % Comment missing here:
            spread=x{cNNN}{!r22}{3em},
          },
        ]
      ]
      [base,
        [base+trans,
          % Draws an extra edge from the current node to the given node.  The
          % other node is given by its "relative node name" (section 3.15).
          % "!" says we will provide a nodewalk (starting at the current node).
          % Nodewalk "r22" is a shorthand for "root, n=2, n=2", in \ili{English} "go
          % to the root, go to the second child, go to the second child".
          edge to=!r22
        ]
      ]
    ]
    [arg-st,partition,
      [intrans, 
        [base+intrans, edge to=!r12]
      ]
      [trans,
        [fin+trans, edge to=!r11]
      ]
    ]
  ]
\end{forest}

\caption{\label{fig-verb-hier2}An example of on-line type construction}
\end{figure}

\citet{CrysmannandBonami2016}\is{morphology!inflectional} have shown how this \word{online type construction}, where predictable combinations of types of orthogonal dimensions of classification are not reified in the grammar, is useful when modeling productive inflectional morphology. Consider, for example, exponents of morphosyntactic features whose shape remains constant, but whose position within a word's template (to speak informally here) varies. One case like this is the subject and object markers of \ili{Swahili}, which can occur in multiple slots in the \ili{Swahili} verb template \citep{Stump1993b,BonamiandCrysmann2016}. 
% TD, 1 Aug: something missing here
%JPk 5 Aug done

For reasons of space we illustrate the usefulness of this dynamic approach to type creation, the \isi{Type Underspecified Hierarchical Lexicon (TUHL)}, with an example from \citet{Koenig1999c}:\ the cross-cutting classification of syntactic/semantic information and stem form in the entry for the \ili{French} verb \word{aller} (see \citealt{BonamiandBoye2001} for a much more thorough discussion of \ili{French} stem allomorphy along similar lines; Crysmann \& Bonami's much more developed approach to stem allomorphy would model the same phenomena differently and we use Koenig's simplified presentation for expository purposes only). The forms of \word{aller} are based on four different suppletive stems:\is{morphology!suppletion} \word{all-} (1\textsuperscript{st} and 2\textsuperscript{nd} person plural of the indicative and imperative present, infinitive, past participle, and imperfective past), \word{i-} (future and conditional), \word{v-} (1\textsuperscript{st}, 2\textsuperscript{nd}, or 3\textsuperscript{rd} person singular and 3\textsuperscript{rd} person plural of the indicative present), and \word{aill-} (subjunctive present). These four suppletive stems are shared by all entries (i.e., senses) of the lexeme \word{aller}: the one which means `to fit' as well as the one which means `to leave', as shown in (\ref{aller-lex}) (see \citealt[40--41]{Koenig1999c}). The cross-cutting generalizations over lexemes and stems are represented in Figure~\ref{all-hier}. Any \word{aller} stem combines one entry and one stem form. In a traditional HPSG type hierarchy, each combination of types (grayed out in Figure~\ref{all-hier}), would have to be stipulated. In a TUHL, these combinations can be dynamically created when an instance of \word{aller} needs to be produced or comprehended.

\largerpage
\eal
\label{aller-lex}
\ex\label{aller-lex-a}
\gll Marc est             allé        à  Paris. \\
     Marc be.\ig{prs.3sg} go.\ig{ptcp} to Paris \\\hfill(French)
\glt `Marc went to Paris.'
\ex\label{aller-lex-b}
\gll Marc s'en              ira. \\
     Marc \ig{3.refl}.of.it go.\ig{fut.3sg} \\
\glt `Marc will leave.'
\ex\label{aller-lex-c}
\gll Ce   costume te  va             bien. \\
     this suit    you go.\ig{prs.3sg} well \\
\glt `This suit becomes you.' (lit. goes well to you)
\ex\label{aller-lex-d}
\gll Il faut que  j'y        aille. \\
     it must that I.to.there go.\ig{subj.prs.1sg} \\
\glt `I must go there.'
\zl

\begin{figure}
%\begin{sideways}
%\oneline{%
  \begin{forest}
    type hierarchy,
    instances,
    for tree={
      s sep=0,
    },
    [aller
      [aller-entries, partition
	[``fit'', name=F, l*=4, for children={l*=2},
          [``fit''+all-, edge to=A]
          [``fit''+v-, edge to=V]
        ]
	[``go''
          [``go.to'', name=G, l*=2, for children={l*=4},
            [``go.to''+all-, edge to=A, ignore edge]
            [,coordinate,edge'={}]
            [``go.to''+v-, edge to=V]
          ]
          [``leave'', name=L, for children={l*=6},
            [``leave''+all-, edge to=A, ignore edge]
            [,coordinate,edge'={}]
            [``leave''+v-, edge to=V,ignore edge]
          ]
        ]
      ]
      [aller-stems, partition, calign children=34, where n children=0{l*=4.5}{},
  	[all-, name=A, before computing xy={s-=3em} [,phantom,ignore,ignore edge]]
  	[v-, name=V, before computing xy={s-=1.5em} [,phantom,ignore,ignore edge]]
  	[i-, name=I, l*=2,
          [``fit''+i-, edge to=F]
          [``go.to''+i-, edge to=G, ignore edge]
          [``leave''+i-,  edge to=L, ignore edge]
  	]
  	[aill-, name=AI,
          [``fit''+aill-, edge to=F]
          [``go.to''+aill-, edge to=G]
          [``leave''+aill-, edge to=L]  	
	]
      ]
    ]
  \end{forest}
%}
%\end{sideways}
\caption{\label{all-hier} A hierarchy of lexical entries and stem-forms for the French verb
  \word{aller}, adapted from \citew[\page 137]{Koenig1999c}}
\end{figure}



Both the distinction between canonical and non-canonical \type{synsem} and type underspecification avoid conflict between the information specified in the variants of words based on a single lexeme (e.g., conflicts on how syntactic arguments are realized); they abstract over the relevant pieces of conflicting information. 
% TD, 1 Aug: I don't understand ``Both \type{synsem} and type underspecification''
%JPK 5 Aug rephrased
Underspecifying information included in lexical entries or lexical types allows a single entry or type to stand for the two distinct entries or types that would be related as input and output by lexical rules. 

Lexical rules have played a crucial role in the rise of lexicalist approaches to syntax. But the two alternative analytical tools we discussed in this section (which, of course, can be combined in an analysis) have chipped away at their use in HPSG. Inflectional morphology is now dealt with through lexical types associating morphosyntactic features with forms/positions and constraints on words (ensuring that all morphosyntactic features are realized, see \crossrefchaptert[\pageref{morphology:morphology-well-formedness}]{morphology}). 
Non-canonical realization of syntactic arguments as affixes or fillers in unbounded dependencies is  modeled by many (but see \citealt{LH2006a}, among others, for an opposing view)\itddone{Not by everyone. \citet{LH2006a} argue against this and I use traces as well. I guess the whole German crowd does not assume this approach. I suggest adding a footnote.}  by distinguishing kinds of members of the \feat{arg-st} list and constraints on words that relate valence, argument structure, and dependents lists.\footnote{But see \citet{LH2006a}, \citet{MuellerCoreGram}, \citet[Section~4.9]{MuellerCurrentApproaches} and \citew{MuellerGS} for trace-based approaches.}

So, what remains of the case for lexical rules? Well, first, as we showed above, lexical rules are now simply unary-branching rules within the lexical part of the type hierarchy. As such they are not formally distinct from the rest of the lexical hierarchy or the hierarchy of signs, as they used to be. Second, they are not meant to model just unproductive processes, as they were originally intended to in \citet{Jackendoff1975} and \citet{Bochner1993}.
% \itdobl{This is a misrepresentation of Jackendoff, who writes on p.\,668: ``The nature of the revision is clear. Lexical redundancy rules are learned from generalizations observed in already known lexical items. Once learned, they make it easier to learn new lexical items: we have designed them specifically to represent what new independent information must be learned. However, after a redundancy rule is learned, it can be used generatively, producing a class of partially specified possible lexical entries''\\
% JPK Actually, it's Jackendoff --- and Bochner--- who are misrepresenting. The point of having redundancy rules is to avoid the problems with lexically governed processes that required Lakoff to introduce exception features as in his thesis. But then it can't be productive. The nod to ``generativity'' is BS; they know it's a problem but there is simply no way you can have redundancy rules and have it be generative EXCEPT in the sense of analogical reasonning a la Bybee. Which is OK, although this has never been formalized, but then it's a different process than the lexical redundancy rules. So basically I am right :). I would love to add an entire paragraph explaining why this is BS, but is it really the place? Note that Bresnan and Kaplan are very clear in her 1982 chapters that these are redundancy rules. I call this the great lexical rules hoax. By the way this is why lexical rules were never used for inflection, because there redundancy rules are clearly not the way to go.}
They can be used to model unproductive processes, but they can also model productive derivational processes (in fact both when a single derivational process is both; see \citealt{Riehemann98a} and the discussion of her approach in the chapters by \crossrefcitet[Section~\ref{morphology:sec-Riehemann}]{morphology} and \crossrefcitet[Section~\ref{cxg:sec-cxg-morphology}]{cxg}).

Still, the existence of two distinct ways of dealing with potential conflict of information -- underspecification or unary branching rules -- raises the issue of which one should be used when. Unfortunately, there is no general guideline; it depends on the nature of the data that needs to be modeled. \citet{Mueller2006,Mueller2010} argues that diathesis phenomena, broadly speaking, favor a lexical rules approach over a phrase-structural constructional approach à la \citet{Goldberg1995}\indexcxg or an online type construction approach suggested in \citew{Kay2002}. The arguments are convincing, but it should be noted that some of the data involves derivational morphology (e.g., causatives) or passive morphemes, which involves a Type-Based Derivational Morphology of the kind \citet{Riehemann98a} argues for (such an approach was suggested in \citealt[Chapter~4]{Koenig1999c}). What remains unclear to us is whether there are instances where lexical rules as unary-branching rules are a better model of ``horizontal'' generalizations that do not involve morphological processes, i.e., whether the kind of lexical rules \citet{PollardandSag1994} proposes (e.g., the Complement Extraction Lexical Rule) are ever motivated over the  underspecification treatment of such phenomena proposed in \citet{Boumaetal2001}.
\is{lexical rule|)}\is{type underspecification|)}

\section{Conclusion}

Our principal goals in this chapter have been to present the HPSG viewpoint on the structure and content of individual lexical entries, and the organization of the lexicon as a whole.
Unsurprisingly, both of these are pervaded by HPSG's lexicalist stance.
With regard to lexical entries, this entails informationally rich and sometimes complex representations. 
A lexical entry models not only a word's idiosyncratic properties, but also its general morphological, distributional, combinatorial, and semantic characteristics.
Consequently, HPSG researchers have devoted a great deal of attention to representing all of these in a parsimonious way, so as to avoid massive redundancy in the lexicon.
We have surveyed several techniques addressing how to parcel out information shared among entries into descriptions that are true of sets of entries.
First, feature geometry plays a key role in organizing portions of this information within a lexical entry in ``packages'' that tend to recur throughout the lexicon.
This in turn allows these recurring portions to be associated with types in a hierarchy.
Through inheritance, these common elements can be stated in just one location for the class of words that share them, and multiple inheritance makes it possible to represent numerous cross-cutting classifications of words.
We have shown two ways in which HPSG scholars have exploited these mechanisms.
One is by creating a hierarchy of subtypes of \type{word} and/""or \type{lexeme}, each with associated constraints.
The other, probably more commonly employed in current work, is to posit type hierarchies of various objects within lexical entries, along with implicational statements that constrain the content of a lexical entry containing those types of objects.

This hierarchical character of the HPSG lexicon serves to model the ``vertical'' relationships among classes of words, based on properties like part of speech, subcategorization, linking, morphological and paradigmatic classes, and so forth.
There is also a ``horizontal'' aspect of lexical relations, however, for which lexical rules explicitly relating one class of lexemes or words to another have been proposed.
While their original use was primarily to model systematic sets of, say, forms in an inflectional paradigm, HPSG's lexicalist approach to syntax has also seen them employed in accounts of phenomena such as extraction, traditionally regarded as outside the lexicon.
We also presented two alternatives to lexical rules that appear to handle these phenomena equally well.
One involves underspecification within lexical entries in a way that permits them to describe the right range of related forms, while the other allows underspecification within type hierarchies, and requires fully specified types to be constructed ``online''.
Both of these alternatives, like lexical rules, avoid massively repetitive specification of properties of families of systematically related words. Lexical rules as well as the two alternatives we outlined are independently needed and, although one can make suggestive remarks as to when to use lexical rules or either alternative, the issue cannot be settled \word{a priori } and must be argued on a case by case basis. But, the rich and intricate hierarchical lexicon cum lexical rules is a defining, enduring, and pervasive feature of HPSG, more prominent here than in almost any other grammatical framework.%
\is{lexicon|)}




 
%\section*{Abbreviations}
\section*{\acknowledgmentsUS}

We thank Anne Abeillé for very helpful comments and Stefan Müller for so carefully reading and commenting on several versions of the manuscript  and helping us improve this chapter considerably. We thank Elizabeth Pankratz for editorial comments and proofreading.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}

\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
