\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.5599866}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[25]
}{}



\title{Processing}
\author{Thomas Wasow\affiliation{Stanford University}}


\abstract{Although not much psycholinguistic research has been carried out in the framework of HPSG, the architecture of the theory fits well with what is known about human language processing.  This chapter enumerates aspects of this fit.  It then discusses two phenomena, island constraints and relative clauses, in which the fit between experimental evidence on processing and HPSG analyses seems particularly good.}


\begin{document}
\maketitle
\label{chap-processing}

\section{Introduction}

Little psycholinguistic research has been guided by ideas from HPSG (but see \citealt{Konieczny96a-u} for a notable exception).  This is not so much a reflection on HPSG as on the state of current knowledge of the relationship between language structure and the unconscious processes that underlie language production and comprehension.  Other theories of grammar have likewise not figured prominently in theories of language processing, at least in recent decades.\footnote{Half a century ago, the \isi{Derivational Theory of Complexity} (DTC) was an attempt to use psycholinguistic experiments to test aspects of the grammatical theory that was dominant at the time.  The DTC was discredited in the 1970s, and the theory it purported to support has long-since been superseded.  See \citet{FBG74a-u} for discussion.}  The focus of this chapter, then, will be on how well the architecture of HPSG comports with available evidence about language production and comprehension.  

%\largerpage[-1]
My argument is much the same as that put forward by \citet[Chapter~9]{SWB2003a}, and \citet{SW2011a,SW2015a}, but with some additional observations about the relationship between \isi{competence} and \isi{performance}. I presuppose the ``\isi{competence hypothesis}'' (see \citealt[Chapter~1]{Chomsky65a}), that is, that a theory of language use (performance) should incorporate a grammar representing the knowledge of language (competence) that is drawn on in everyday comprehension and production, as well as in other linguistic activities, such as language games and the (often artificial) tasks employed in psycholinguistic experiments.  

The primary reason for adopting the competence hypothesis is parsimony: a theory of language use is
simpler if it does not have to repeat much the same information about the language in both its
production and comprehension components.  This information would include things like the vocabulary,
the preferred word orders, and most of the rest of what linguists encode in their grammars.  A
performance theory that incorporates a grammar only needs to include such information
once.\footnote{There are of course some discrepancies between production and comprehension that need
  to be accounted for in a full theory of language use.  For example, most people can understand
  some expressions that they never use, including such things as dialect-specific words or accents.
  But these discrepancies are on the margins of speakers' knowledge of their languages.  The vast
  majority of the words and structures that speakers know are used in both production and
  comprehension.  Further, it seems to be generally true that what speakers can produce is a proper
  subset of what they can comprehend.  Hence, the discrepancies can plausibly be attributed to
  performance factors such as memory or motor habits.  See \citet{GollanEtal2011} for evidence of
  differences between lexical access in production and comprehension.  See \citet{MommaPhillips2018}
  for arguments that the structure-building mechanisms in production and comprehension are the same.
  For a thoughtful discussion of the relationship between production and comprehension, see
  \citet{MacDonald2013} and the commentaries published with it.}  Moreover, to the extent that the
theoretical constructs of the grammar play a role in modeling both production and comprehension, the
overall theory is simpler.

\largerpage[-1] 
There is also, however, an empirical reason for preferring a model with a good fit
between competence and performance.  As noted by \citet{BresnanEtAl2001}, preferences that are only
statistical tendencies in some languages can show up in others as categorical requirements.  The
example they discuss in detail is the avoidance of clauses with third-person subjects but first- or
second-person objects or obliques. In \ili{English}, this is a powerful statistical tendency, which
they document by showing that the passivization rate in the Switchboard corpus is very significantly
lower when the agent is first- or second-person than when it is third-person.  In \ili{Lummi} (a
\ili{Salish} language of British Columbia), this preference is categorical: clauses with
third-person subjects but first- or second-person objects or obliques are simply unacceptable.
\citet{Hawkins2004a-u,Hawkins2014} argues that such examples are by no means exceptional, and
formulates the following ``\isi{Performance--Grammar Correspondence Hypothesis}'' (PGCH):
\newpage
\begin{quote}
Grammars  have  conventionalized  syntactic  structures  in  proportion   to their degree of
preference in performance, as evidenced by frequency of use and ease of processing.\footnote{In the
  \citeauthor{BresnanEtAl2001} example, I know of no experimental evidence that clauses with
  third-person subjects and first- or second-person objects are difficult to process.  But a
  plausible case can be made that the high salience of speaker and addressee makes the pronouns
  referring to them more accessible in both production and comprehension than expressions referring
  to other entities.  In any event, clauses with first- or second-person subjects and third-person
  objects are far more frequent than clauses with the reverse pattern in languages where this has
  been checked.  Thus, the \citeauthor{BresnanEtAl2001} example falls under the PGCH, at least with
  respect to ``frequency of use''.}
\end{quote}
\largerpage[-1]
There are two ways in which a processing model incorporating a grammar might capture this
generalization.  One is to give up the widespread assumption that grammars provide categorical
descriptions, and that any quantitative generalizations must be extra-grammatical; see
\citet{Francis2021} for arguments supporting this option, and thoughtful discussion of literature
on how to differentiate processing effects from grammar.  For example, some HPSG feature
structure descriptions might allow multiple values for the same feature, but with
probabilities (adding up to 1) attached to each value.\footnote{I discussed this idea many times
  with the late Ivan Sag\aimention{Ivan Sag}.  He made it clear that he believed grammatical
  generalizations should be categorical.  In part for that reason, this idea was not included in our
  joint publications on processing and HPSG.}  I hasten to add that fleshing out this idea into a
full-fledged probabilistic version of HPSG would be a large undertaking, well beyond the scope of
this chapter; see \citet{Linadarki2006} and \citet{MT2008a-u} for work along these lines.  But the
idea is fairly straightforward, and would allow, for example, \ili{English} to have \emph{in its
  grammar} a non-categorical constraint against clauses with third-person subjects and first- or
second-person objects or obliques.   

The second way for a theory adopting the \isi{competence hypothesis} to represent Hawkins's PGCH
would be to allow certain generalizations to be stated either as grammatical constraints (when they
are categorical) or as probabilistic performance constraints.  This requires a fit between the
grammar and the other components of the performance model that is close enough to permit what is
essentially the same generalization to be expressed in the grammar or elsewhere.  In the case
discussed by \citeauthor{BresnanEtAl2001}, for example, treating the constraint in question as part
of the grammar of \ili{Lummi} but a matter of performance in \ili{English} would require that both
the theory of grammar and models of production would include, minimally, the distinction between
third-person and other persons, and the distinction between subjects and non-subjects.  Since
virtually all theories of grammar make these distinctions, this observation is not very useful in
choosing among theories of grammar.  I will return later to phenomena that bear on the choice among
grammatical theories, at least if one accepts the \isi{competence hypothesis}.

Since its earliest days, HPSG research has been motivated in part by considerations of
\isi{computational tractability} (see \crossrefchapteralt{evolution}, for discussion).  Some of the
design features of the theory can be traced back to the need to build a system that could run on the
computers of the 1980s.  Despite the obvious differences between human and machine information
processing, some aspects of HPSG's architecture that were initially motivated on computational
grounds have turned out to fit well with what is known about human language processing.  A prime
example of that is the computational analogue to the competence hypothesis, namely the fact that the
same grammar is used for parsing and generation. In Section~\ref{sec-features-of-HPSG}, I will
discuss a number of other high-level design properties of HPSG, arguing that they fit well with what
is known about human language processing, which I summarize in Section~\ref{sec-key-facts}.  In
Section~\ref{sec-phenomena-processing}, I will briefly discuss two phenomena that have been the
locus of much discussion about the relationship between grammar and processing, namely island
constraints and differences between subject and object relative clauses.

\section{Key facts about human language processing}
\label{sec-key-facts}

In this section I review a number of well-known general properties of human language processing.  Most of them seem evident from subjective experience of language use, but there is supporting experimental evidence for all of them.  

\subsection{Incrementality}
\label{processing:incrementality}

\largerpage\enlargethispage{3pt}
Both\is{incremental processing|(} language production and comprehension proceed incrementally, from the beginning to the end of
an utterance.  In the case of production, this is evident from the fact that utterances unfold over
time.  Moreover, speakers very often begin their utterances without having fully planned them out,
as is evident from the prevalence of \isi{disfluencies}.  On the comprehension side, there is
considerable evidence that listeners (and readers) begin analyzing input right away, without waiting
for utterances to be complete.  A grammatical framework that assigns structure and meaning to
initial substrings of sentences will fit more naturally than one that doesn't into a processing
model that exhibits this incrementality we see in human language use.

I hasten to add that there is also good evidence that both \isi{production} and \isi{comprehension}
involve anticipation of later parts of sentences.  While speakers may not have their sentences fully
planned before they begin speaking, some planning of downstream words must take place.  This is
perhaps most evident from instances of nouns exhibiting quirky cases determined by verbs that occur
later in the clause.  For example, objects of \ili{German} \emph{helfen}, `help', take the dative
case, rather than the default accusative for direct objects.  But in a sentence like
(\ref{ex:kind-bald-helfen}), the speaker must know that the verb will be one taking a dative object
at the time the dative case article \emph{dem} is uttered.

\ea\label{ex:kind-bald-helfen}
\gll Wir werden dem        Kind bald helfen.\\
     we will    the.\DAT{} child soon help\\\hfill (German)
\glt `We will help the child soon.'
\z
Likewise, in comprehension there is ample evidence that listeners and readers anticipate what is to
come.  This has been demonstrated using a variety of experimental paradigms.  Eye-tracking\is{eye-tracking}
studies (see \citealt{TanenhausEtal1995}, \citealt{AltmannKamide99}, \citealt{ArnoldEtal2007}, among
many others) have shown that listeners use semantic information and \isi{world knowledge} to predict
what speakers will refer to next. 

Thus, a theory of grammar that fits comfortably into a model of language use should provide
representations of initial substrings of utterances that can be assigned (partial) meanings and be
used in predicting later parts of those utterances. 
\is{incremental processing|)}

\subsection{Non-modularity}

\largerpage
Psycholinguistic research over the past four decades has established that language processing
involves integrating a wide range of types of information on an as-needed basis.  That is, the
various components of the language faculty interact throughout their operation.  A model of language
use should therefore \emph{not} be modular, in the sense of Jerry Fodor's influential
\citeyear{Fodor83a-u} book, \emph{The Modularity of Mind}.\footnote{Much of the psycholinguistic
  research of the 1980s was devoted to exploring \isi{modularity} -- that is, the idea that the
  human linguistic faculty consists of a number of distinct ``informationally encapsulated''
  modules.  While Fodor's book was mostly devoted to arguing for modularity at a higher level, where
  the linguistic faculty was one module, many researchers at the time extended the idea to the
  internal organization of the linguistic faculty, positing largely autonomous mechanisms for
  phonology, morphology, syntax, semantics, and pragmatics, with the operations of each of these
  sub-modules unaffected by the operations of the others. The outcome of years of experimental
  studies on the linguistic modularity idea was that it was abandoned by most psycholinguists. For
  an early direct response to Fodor, see \citet{MarslenTyler87}.}

Some casual observations argue against modular language processing.  For example, the famously
ambiguous sentences (\ref{ex:beer-tastes-good}) and (\ref{ex:carry-dogs}) can be disambiguated in
speech by the stress patterns.
\eal
\ex I forgot how good beer tastes.\label{ex:beer-tastes-good}
\ex Dogs must be carried.\label{ex:carry-dogs}
\zl
The two meanings of (\ref{ex:beer-tastes-good}) correspond to two different parses (one with \emph{good} as part of the noun phrase \emph{good beer} and the other with \emph{how good} as a verb phrase modifier).  The two meanings of (\ref{ex:carry-dogs}) have the same syntactic structure, but differ in whether the requirement is that all dogs be carried, or that everyone carry a dog.  This interaction of \isi{prosody} with syntax (in the case of (\ref{ex:beer-tastes-good})) and with semantics (in the case of (\ref{ex:carry-dogs})) is produced and perceived before the end of the utterance, suggesting that phonological information is available in the course of syntactic and semantic processing.  

Moreover, non-linguistic knowledge influences the disambiguation in both of these cases.  If (\ref{ex:beer-tastes-good}) is preceded by ``I just finished three weeks without alcohol'', the natural interpretation of \emph{good} is as a modifier of \emph{tastes}; but following ``I just finished three weeks drinking only Bud Light'', \emph{good} is more naturally interpreted as a modifier of \emph{beer}.  In the case of (\ref{ex:carry-dogs}), only one interpretation (that anyone with a dog must carry it) is plausible, given our knowledge of the world.  Indeed, most non-linguists fail to see the \isi{ambiguity} of (\ref{ex:carry-dogs}) without a lengthy explanation.  

More rigorous evidence of the non-modular character of language processing has been provided by a variety of types of experiments.  The work of Michael Tanenhaus \aimention{Michael Tanenhaus} and his associates, using \isi{eye-tracking} to investigate the time-course of sentence \isi{comprehension}, played an important role in convincing most psycholinguists that human language understanding is non-modular.  See, for example, \citet{EberhardEtal95}, \citet{McMurrayEtal2008}, \citet{TSKES95a}, \citet{TSKES96a}, and \citet{TanenhausTrueswell95}. A recent survey of work arguing against modularity in language processing is provided by \citet{SpevackEtal2018}.  

\subsection{Importance of words}
\label{sec-importance-of-words}

%\largerpage
The individual properties of words play a central role in how people process phrases and sentences.
Consider, for example, what is probably the most famous sentence in psycholinguistics,
(\ref{ex:horse-raced-past}), due originally to \citet[\page 320]{Bever70}.

\ea\label{ex:horse-raced-past}
The horse raced past the barn fell.
\z
The extreme difficulty that people who have not previously been exposed to
(\ref{ex:horse-raced-past}) have comprehending it depends heavily on the choice of words.  A
sentence like (\ref{ex:applicant-interviewed}), with the same syntactic structure, is far easier to
parse.

\ea\label{ex:applicant-interviewed}
The applicant interviewed in the morning left.
\z
Numerous studies (e.g.\ \citealt{FordEtal82}; \citealt{TrueswellEtal93}; \citealt{MPS94a-u}; \citealt{BresnanEtal2007}; \citealt{WasowEtal2011}) have shown that such properties of individual words as subcategorization preferences, semantic categories (e.g.\ animacy), and frequency of use can influence the processing of utterances.  

\subsection{Influence of context}

%\largerpage
Much of the evidence against modularity of the language faculty is based on the influences of
non-linguistic \isi{context} and \isi{world knowledge} on language processing.  The well-known
\isi{McGurk effect} \citep{McGurkMacDonald76} and the \isi{Stroop effect} \citep{Stroop35}
demonstrate that, even at the word level, visual context can influence linguistic
\isi{comprehension} and \isi{production}.

Linguistic context also clearly influences processing, as the discussion of examples (\ref{ex:beer-tastes-good}) and (\ref{ex:carry-dogs}) above illustrates.  The same conclusion is supported by numerous controlled studies, including,
among many others, those described by \citet{CS85a}, \citet{AltmannSteedman88}, \citet{Branigan2007}, \citet{Tooley2007a}, \citet{MatsukiEtal2011}, and \citet{SpevackEtal2018}.  The last of these references concludes (p.\,11), ``when humans and their brains are processing language with each other, there is no format of linguistic information (e.g., lexical, syntactic, semantic, and pragmatic) that cannot be rapidly
influenced by \isi{context}.''

\subsection{Speed and accuracy of processing}

\largerpage
A good deal of psycholinguistic literature is devoted to exploring situations in which language
processing encounters difficulties, notably work on garden paths (in comprehension) and disfluencies
(in production).  Much more striking than the existence of these phenomena, however, is how little
they matter in everyday language use.  While ambiguities abound in normal sentences (see
\citealt{Wasow2015} and also \crossrefchapteralt[Section~\ref{cl:prac:rank}]{cl}), comprehenders
very rarely experience noticeable \isi{garden paths}. Similarly, \isi{disfluencies} in spontaneous
speech occur in nearly every sentence but rarely disrupt communication.

People are able to use speech to exchange information remarkably efficiently.  A successful account of human language processing must explain why it works as well as it does.  

\section{Features of HPSG that fit well with processing facts}
\label{sec-features-of-HPSG}

In this section, I review some basic design features of HPSG, pointing out ways in which they comport well with the properties of language processing listed in the previous section.

\subsection{Constraint-based}
\label{sec-processing-constraint-based}

%\largerpage[2]
Well-formedness of HPSG representations is defined by the simultaneous satisfaction of a set of
constraints that constitutes the grammar \crossrefchapterp[\page
\pageref{page-wellformedness-linguistic-objects}]{formal-background}.  This lack of
\isi{directionality} allows the same grammar to be used in modeling \isi{production} and \isi{comprehension}.

Consider, for instance, the example of quirky case assignment illustrated in
(\ref{ex:kind-bald-helfen}) above.  A speaker uttering (\ref{ex:kind-bald-helfen}) would need to
have planned to use the verb \emph{helfen} `help' before beginning to utter the object NP.  But a listener hearing (\ref{ex:kind-bald-helfen}) would encounter the dative case on the article \emph{dem} before hearing the verb and could infer only that a verb taking a dative object was likely to occur at the end of the clause.  Hence, the partial mental representations built up by the two interlocutors during the course of the utterance would be quite different.  But the grammatical mechanism licensing the combination of a dative object with this particular verb is the same for speaker and hearer. 

In contrast, theories of grammar that utilize sequential operations to derive sentences impose a \isi{directionality} on their grammars.  If such a grammar is then to be employed as a component in a model of language use (as the \isi{competence hypothesis} stipulates), its inherent directionality becomes part of the models of both production and comprehension.  But production involves mapping meaning onto sound, whereas comprehension involves the reverse mapping.  Hence, a directional grammar cannot fit the direction of processing for both production and comprehension.\footnote{This was an issue for early work in computational linguistics that built parsers based on the transformational grammars of the time, which generated sentences using derivations whose direction went from an underlying structure largely motivated by semantic considerations to the observable surface structure.  See, for example, \citet{HobbsGrishman75}.}  

\citet{BraniganPickering2017} argue at length that ``\isi{structural priming} provides an implicit method of investigating linguistic representations''.\footnote{Priming is the tendency for speakers to re-use linguistic elements that occurred earlier in the context; structural priming (which \citeauthor{BraniganPickering2017} sometimes call \emph{abstract priming}) is priming of linguistic structures, abstracted from the particular lexical items in those structures.}  They go on to conclude (p.\,14) that the evidence from priming supports ``frameworks that \ldots{} assume nondirectional and constraint-based generative capacities (i.e., specifying well-formed structures) that do not involve movement''.\footnote{\citeauthor{BraniganPickering2017}'s conclusions are controversial, as is evident from the commentaries accompanying their target article.}  HPSG is one of the frameworks they mention that fit this description.   

\subsection{Surface-oriented}

The features and values in HPSG representations are motivated by straightforwardly observable linguistic phenomena. HPSG does not posit derivations of observable properties from abstract
underlying structures.  In this sense it is \isi{surface-oriented}.

The evidence linguists use in formulating grammars consists of certain types of performance data, primarily judgments of \isi{acceptability} and meaning.  Accounts of the data necessarily involve some
combination of grammatical and processing mechanisms.  The closer the grammatical descriptions are to the observable phenomena, the less complex the processing component of the account needs to be.

For example, the grammatical theory of \citet{Kayne94a-u}, which posits a universal underlying order
of specifier-head-complement, requires elaborate (and directional) transformational derivations to
relate these underlying structures to the observable data in languages whose surface order is
different (a majority of the languages of the world).  In the absence of experimental evidence that
the production and comprehension of sentences with different constituent orders involve mental
operations corresponding to the grammatical derivations \citeauthor{Kayne94a-u} posits, his theory
of grammar seems to be incompatible with the competence hypothesis.

Experimental evidence supports this reasoning.  As \citet[\page 9]{BraniganPickering2017} conclude,
``[P]riming evidence supports the existence of abstract syntactic representations. It also suggests
that these are shallow and monostratal in a way that corresponds at least roughly to the assumptions
of [\ldots{}] \citet{ps2} [\ldots{}]. It does not support a second, underlying level of syntactic
structure or the syntactic representation of empty categories associated with the movement of
constituents in some transformational analyses.''

\subsection{Informationally rich representations}

The feature structure descriptions of HPSG include all types of linguistic information relevant to the well-formedness and interpretation of expressions. This includes phonological, morphological, syntactic, semantic, and contextual information.  They can also incorporate non-linguistic contextual information (e.g.\ social information), though this has not been extensively explored.

The cooccurrence of these different types of information within a single
representation facilitates modeling \isi{production} and \isi{comprehension} processes that make
reference to more than one of them.  The architecture of the grammar is thus well suited to the
non-\isi{modularity} and \isi{context}-sensitivity of language processing.  It is interesting in
this regard to consider the conclusions of two papers by psycholinguists who surveyed experimental
evidence and inferred what types of grammatical information were essential for processing.

The following series of quotes captures the essence of what \citet{MPS94a-u} wrote regarding lexical representation, based on a survey of a wide range of psycholinguistic studies:
\begin{itemize} 
\item ``[T]he \isi{lexical
representation} for a word includes a representation of the
word's phonological form, orthographic form, semantics,
grammatical features (including grammatical category), morphology
(at least inflectional), argument structure, and X-bar
structure.'' (p.\,684)
\item ``[T]he connection
structure of the lexicon encodes relationships among
different types of lexical information.''\footnote{A reviewer asked what feature of HPSG this maps into.  The answer is straightforward:  a word's phonological form, semantics, grammatical features, morphology, and argument structure are all represented together in one feature structure description, and the different pieces of the description may be linked through coindexing or tagging.} (p.\,684)
\item
``In addition to constraints that hold between various aspects
of lexical representations, sentence and discourse contexts also
constrain lexical representations during processing [\ldots].'' (p.\,686)
\end{itemize}
With the possible exception of ``X-bar structure'', this sounds very much like a description of the types of information included in HPSG feature structure descriptions.

\largerpage
Over twenty years later, \citet{BraniganPickering2017} came to the following conclusions about linguistic representations, based on priming studies:
\begin{itemize}
\item ``The syntactic representations capture local relationships
between a `moth\-er' and its constituent `daughter(s)' (e.g.,
a VP comprising a verb and two NPs), independent of the
larger context in which the phrase appears (e.g., that the VP
occurs within a subordinate clause), or the internal structure
of the subphrases that constitute it (e.g., that the
first NP comprises a determiner, adjective, and noun).'' (p.\,9)
\item ``[S]ome elements that are not phonologically represented may
be syntactically represented.'' (p.\,10)
\item ``Other priming evidence similarly indicates that some
semantically specified elements are not specified syntactically.'' (p.\,11)
\item ``[T]he semantic level of representation
contains at least specifications of quantificational information,
information structure, and thematic roles.'' (p.\,11)
\item ``Evidence
from priming supports a range of mappings between information encoded in the semantic representation and information encoded in the syntactic representation: between
thematic roles and grammatical functions, between thematic roles and word order, between animacy and syntactic
structure, and between event structures and syntactic
structures.'' (p.\,12)
\end{itemize}
The two lists are quite different.  This is in part because the focus of the earlier paper was on lexical representations, whereas the later paper was on linguistic representations more generally.  It may also be attributable to the fact that \citeauthor{MPS94a-u} framed their paper around the issue of \isi{ambiguity} resolution, while \citeauthor{BraniganPickering2017}'s paper concentrated on what could be learned from \isi{structural priming} studies.  Despite these differences, it is striking that the conclusions of both papers about the mental representations employed in language processing are very much like those arrived at by work in HPSG.

\subsection{Lexicalism}
\label{processing:sec-lexicalism}

\largerpage\enlargethispage{5pt}
A\is{lexicalism|(} great deal of the information used in licensing sentences in HPSG is stored in
the lexical entries for words (see \citealp{MWArgSt} and also
\crossrefchapteralp[Section~\ref{prop:sec-lexicon}]{properties}).  A hierarchy of lexical
types\is{type hierarchy!lexical} permits commonalities to be factored out to minimize what has to be
stipulated in individual entries, but the information in the types gets into the representations of
phrases and sentences through the words that instantiate those types. Hence, it is largely the
information coming from the words that determines the well-formedness of larger expressions.  Any
lexical decomposition would have to be strongly motivated by the morphology.

\citet[Section~2.3]{BraniganPickering2017} note that grammatical structures (what some might call
\emph{constructions}\is{construction}) such as V-NP-NP can prime the use of the same abstract
structure, even in the absence of lexical overlap.  But they also note that the priming is
consistently significantly stronger when the two instances share the same verb, a fact known as
\emph{the lexical boost}\is{lexical boost}.  They write, ``To explain abstract priming, lexicalist
theories must assume that the syntactic representations [\ldots] are shared across lexical
entries.'' (p.\,12) The types in HPSG's lexicon provide just such
representations. \citeauthor{BraniganPickering2017} go on to say that the \isi{lexical boost} argues
for ``a representation that encodes a binding between constituent structure and the lemma [\ldots]
of the lexical entry for the head.''  In HPSG, this ``binding'' is simply the fact that the word
providing the lexical boost (say, \emph{give}) is an instantiation of a type specifying the
structures it appears in (e.g.\ the ditransitive verb type, see also \citealp*{YKD2019a-u}).

Similarly, the fact, noted in Section~\ref{sec-importance-of-words} above, that a given structure
may be more or less difficult to process depending on word choice is unsurprising in HPSG, so long
as the processor has access to information about individual words and not just their types.    
\is{lexicalism|)}

\subsection{Underspecification}

HPSG allows a class of linguistic structures that share some feature values to be characterized by
means of feature structure descriptions that specify only the features whose values are shared.
Such \isi{underspecification} is very useful for a model of processing (particularly a model of the
comprehender) because it allows partial descriptions of the utterance to be built up, based on the
information that has been encountered.  This property of the grammar makes it easy to incorporate
into an incremental processing model.

\section{Two phenomena of interest}
\label{sec-phenomena-processing}

\subsection{Island constraints}

\largerpage
Ever since \citeauthor{Ross67}'s seminal dissertation \citeyearpar{Ross67} introduced the notion of
``island\is{island constraint} constraints'', linguists have sought explanations for their existence, often
suggesting that they were motivated by processing considerations (notably \citealt{Grosu72-u};
\citealt{Fodor83}; \citealt{Deane91}).  The basic idea is that island constraints restrict the
search space the parser needs to consider in looking for a gap to match a filler it has encountered,
thereby facilitating processing.  This then raises the question of whether island constraints need
to be represented in grammar (language particular or universal), or can be attributed entirely to
processing and/""or other factors, such as pragmatics.

In principle, this question is orthogonal to the choice among theories of grammar.  But in recent
years, a controversy has arisen between some proponents of HPSG and certain transformational
grammarians, with the former (e.g.\ \citealp{Chaves2012}; \citeyear{chapters/islands}, Chapter~\ref{chap-islands} of this volume; \citealt{HofmeisterSag2010};
\citealt*{HofmeisterEtal2013}; \citew{CP2020a-u}) arguing that certain island phenomena should be attributed entirely
to extra"=grammatical factors, and the latter (e.g.\ \citealt{Phillips2013} and
\citealt{SWP2012a-u}) arguing that island\is{island constraint} constraints are part of grammar. 

I will not try to settle this dispute here.  Rather, my point in this subsection is to note that a
theory in which there is a close fit between the grammar and processing mechanisms allows for the
possibility that some island phenomena should be attributed to grammatical constraints, whereas
others should be explained in terms of processing.  Indeed, if the basic idea that islands
facilitate processing is correct, it is possible that some languages, but not others, have
grammaticalized some islands, but not others.  That is, in a theory in which the grammar is a
tightly integrated component of a processing model, the question of whether a particular island
phenomenon is due to a grammatical constraint is an empirical one whose answer might differ from
language to language.  

Early work on islands (e.g.\ \citealt{Ross67} and \citealt{Chomsky73a}) assumed that, in the absence
of negative evidence, island constraints could not be learned and hence must be innate and therefore
universal.  But cross-linguistic variation in island constraints, even between closely related
languages, has been noted since the early days of research on the topic (see e.g.\
\citealt{Erteschik73a-u} and \citealt{EngdahlEjerhed82}).
% pages OK since they are about the topic entirely. 

This situation is what one might expect if languages differ with respect to the extent to which the
processing factors that motivate islandhood have been grammaticalized.  In short, a theory with a
tight fit between its grammatical machinery and its processing mechanisms allows for hybrid accounts
of islands that are not available to theories without such a fit. 

\largerpage
One example of such a hybrid is \citeauthor{Chaves2012}'s \citeyearpar{Chaves2012} account of
\citeauthor{Ross67}'s \isi{Coordinate Structure Constraint}.  Following much earlier work,
\citeauthor{Chaves2012} distinguishes between the ``\isi{conjunct constraint}'', which prohibits a
gap from serving as a conjunct in a coordinate structure (as in *\emph{What did you eat a sandwich
  and?}) and the ``\isi{element constraint}'', which prohibits a gap from serving as an element of a
larger conjunct (as in *\emph{What did you eat a sandwich and a slice of?}).  The conjunct
constraint, he argues, follows from the architecture of HPSG and is therefore built into the
grammar.  The element constraint, on the other hand, has exceptions and, he claims, should be
attributed to extra-grammatical factors.  See \crossrefchaptert{islands} for a more detailed
discussion of islands.  


\subsection{Subject vs.\ object relative clauses}

One of the most discussed phenomena in the literature on human sentence processing is the difference
in processing complexity between relative clauses\is{relative clause|(} (RCs) in which the gap is the subject and those in which the gap is the object -- or, as they are commonly called, ``subject RCs'' and ``object RCs''; see, among many others, \citet{WannerMaratsos78}, \citet{Gibson98a}, \citet{TraxlerEtal2002}, and \citet{GennariMacDonald2008}.  Relative clause processing complexity has been shown to be influenced by a number of factors other than the grammatical function of the gap, including the animacy and pronominality of the overt NP in the RC, as well as the frequency, animacy, and discourse properties of the head of the RC.\footnote{The stimuli in the experimental studies on this topic always have RCs with one overt NP, either in subject or object position and a gap corresponding to the other grammatical function. In most of the studies, that NP is non-pronominal and animate.  See \citet{RealiChristiansen07} and \citet{RolandEtal2012} for evidence of the role of these factors in processing complexity.} When these factors are controlled for, however, most psycholinguists accept that it has been established that subject RCs are generally easier to process than object RCs, at least in \ili{English}.\footnote{This processing difference corresponds to the top end of the ``\isi{accessibility hierarchy}'' that \citet{KC77a} proposed as a linguistic universal. Based on a diverse sample of 50 languages, they proposed the hierarchy below, and hypothesized that any language allowing RC gaps at any point in the hierarchy would allow RC gaps at all points higher (to the left) on the hierarchy.
\ea
Subject 
> 
Direct Object 
> 
Indirect Object 
> 
Oblique 
> 
Genitive
> 
Object of Comparison
\z
\citeauthor{KC77a} speculated that the generality of this hierarchy of relativizability lay in processing, specifically on the \isi{comprehension} side.  The extensive experimental evidence that has been adduced in support of this idea in the intervening decades has been concentrated on subject RCs vs. (direct) object RCs.  The remainder of the hierarchy remains largely untested by psycholinguists.}

\enlargethispage{3pt}
One approach to explaining this asymmetry has been based on the distance between the filler and the
gap (see, among others, \citealt{WannerMaratsos78}; \citealt{Gibson98a}; \citealt{Hawkins2004a-u}).
In languages like \ili{English}, with basic SVO clause order and RCs that follow the nouns they
modify, the distance between the filler (the relativizer or head noun) and the gap is greater for an
object gap than for a subject gap.  If holding a filler in memory until the gap is encountered puts
an extra burden on the processor, this could explain why object RCs are harder to process than
subject RCs.  This distance-based account makes an interesting prediction for languages with
different word orders.  In languages like \ili{Japanese} with SOV order and RCs that precede the
nouns they modify, the distance relationships are reversed -- that is, the gaps in object RCs are
closer to their fillers than those in subject RCs.  The same is true of \ili{Chinese}, with basic
SVO order and RCs that precede the nouns they modify.  So the prediction of distance-based accounts
of the subject/object RC processing asymmetry is that it should be reversed in these languages.

The experimental evidence on this prediction is somewhat equivocal.  While \citet{HsiaoGibson03}
found a processing preference for object RCs over subject RCs in \ili{Chinese}, their findings were
challenged by \citet{LinBever2006} and \citet{VasishthEtal2013}, who claimed that \ili{Chinese} has
a processing preference for subject RCs.  In \ili{Japanese}, \citet{MiyamotoNakamura2003} found that
subject RCs were processed more easily than object RCs.  The issue remains controversial, but, for
the most part, the evidence has not supported the idea that the processing preference between
subject RCs and object RCs varies across languages with different word orders.

The most comprehensive treatment of \ili{English} RCs in HPSG is \citet{Sag97a}. Based entirely on
distributional evidence, Sag's analysis treats (finite) subject RCs as fundamentally different from
RCs whose gap does not function as the subject of the RC.  The difference is that the
\slasch\isfeat{slash} feature, which encodes information about long-distance dependencies in HPSG,
plays no role in the analysis of subject RCs.  Non-subject RCs, on the other hand, involve a
non-empty \slasch value in the RC.\footnote{The idea that at least some subject gaps differ in this
  fundamental way from non-subject gaps goes back to \citet[\page 171--172]{Gazdar81a}.}

Sag deals with a wide variety of kinds of RCs. From the perspective of the processing literature,
the two crucial kinds are exemplified by (\ref{ex:reporter-attacted-senator}) and
(\ref{ex:senator-attacked-reporter}), from \citet[\page 2]{Gibson98a}. 
\eal\label{ex:reporter}
\ex The reporter who attacked the Senator admitted the error.\label{ex:reporter-attacted-senator}
\ex The reporter who the Senator attacked admitted the error.\label{ex:senator-attacked-reporter}
\zl
A well-controlled experiment on the processing complexity of subject and object RCs must have
stimuli that are matched in every respect except the role of the gap in the RC.  Thus, the
conclusion that object RCs are harder to process than subject RCs is based on a wide variety of
studies using stimuli like (\ref{ex:reporter}).  Sag's analysis of
(\ref{ex:reporter-attacted-senator}) posits an empty \slasch\isfeat{slash} value in the RC, whereas
his analysis of (\ref{ex:senator-attacked-reporter}) posits a non-empty \slasch value.

There is considerable experimental evidence supporting the idea that unbound\-ed dependencies --
that is, what HPSG encodes with the \slasch\isfeat{slash} feature -- add to processing complexity; see, for
example, \citet{WannerMaratsos78}, \citet{KingJust91}, \citet{KluenderKutas93}, and
\citet{Hawkins99a}.  Combined with Sag's HPSG analysis of \ili{English} RCs, this provides an
explanation of the processing preference of subject RCs over object RCs.  On such an account, the
question of which other languages will exhibit the same preference boils down to the question of
which other languages have the same difference in the grammar of subject and object RCs.  At least
for \ili{English}, this is a particularly clear case in which the architecture of HPSG fits well
with processing evidence.%
\is{relative clause|)}

\section{Conclusion}

This chapter opened with the observation that HPSG has not served as the theoretical framework for
much psycholinguistic research.  The observations in Sections~\ref{sec-key-facts}
through~\ref{sec-phenomena-processing} argue for rectifying that situation.  The fit between the
architecture of HPSG and what is known about human sentence processing suggests that HPSG could be
used to make processing predictions that could be tested in the lab.

To take one example, the explanation of the processing asymmetry between subject and object RCs
offered above is based on a grammatical difference in the HPSG analysis:  all else being equal,
expressions with non-empty \slasch\isfeat{slash} values are harder to process than those with empty \slasch
values.  Psycholinguists could test this idea by looking for other cases of phenomena that look
superficially very similar but whose HPSG analyses differ with respect to whether \slasch is empty.
One such case occurs with pairs like \citegen[\page 103]{Chomsky77a-u} famous minimal pair in
(\ref{ex:chris-please}). 
\eal\label{ex:chris-please}
\ex Chris is eager to please.\label{ex:chris-eager}
\ex Chris is easy to please.\label{ex:chris-easy}
\zl
Under the analysis of \citet[Section~4.3]{ps2}, \emph{to please} in (\ref{ex:chris-easy}) has a non-empty
\slasch\isfeat{slash} value but an empty \slasch value in (\ref{ex:chris-eager}).  Processing
(\ref{ex:chris-eager}) should therefore be easier.  This prediction could be tested experimentally,
and modern methods such as \isi{eye-tracking} could pinpoint the locus of any difference in
processing complexity to determine whether it corresponds to the region where the grammatical
analysis involves a difference in \slasch\isfeat{slash} values. 

The current disconnect between theoretical investigations of language structure and psycholinguistic
studies is an unfortunate feature of our discipline.  Because HPSG comports so well with what is
known about processing, it could serve as the basis for a reconnection between these two areas of
study. 

\section*{\acknowledgmentsUS}

A number of people made valuable suggestions on a preliminary outline and an earlier draft of this
chapter, leading to improvements in content and presentation, as well as the inclusion of previously
overlooked references.  In particular, I received helpful comments from (in alphabetical order):
Emily Bender, Bob Borsley, Rui Chaves, Danièle Godard, Jean-Pierre Koenig, and Stefan Müller.
Grateful as I am for their advice, I take sole responsibility for any shortcomings in the chapter. 

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->
