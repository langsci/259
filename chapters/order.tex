\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.5599836}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a
                                %collection or standalone
   \usepackage{../nomemoize}
   \input{../localpackages}
   \input{../localcommands}
   \input{../locallangscifixes.tex}

   \togglepaper[10]
}{}

%\input{localpackages}
%\input{localcommands}
%\input{locallangscifixes}

%\togglepaper[10]

\author{Stefan Müller\orcid{0000-0003-4413-5313}\affiliation{Humboldt-Universität zu Berlin}}
\title{Constituent order}


%\epigram{Change epigram in chapters/03.tex or remove it there }
\abstract{This chapter discusses local ordering variants and how they can be analyzed in HPSG. So-called scrambling, the local reordering of arguments of a head, can be accounted for by assuming flat rules or binary branching rules with arbitrary order of saturation. The difference between SVO and SOV is explained by assuming different mappings between the argument structure list (a list containing all arguments of a head) and valence features for subjects and complements. The position of the finite verb in initial or final position in languages like \ili{German} can be accounted for by flat rules and a separation between immediate dominance and linear precedence information or by something analogous to head-movement in transformational approaches. The chapter also addresses the analysis of languages allowing even more freedom than just scrambling arguments. It is shown how one such language, namely Warlpiri, can be analyzed with so-called constituent order domains allowing for discontinuous constituents. I discuss problems of domain-based approaches and provide an alternative account of Warlpiri that does not rely on discontinuous constituents.}

\settowidth\jamwidth{(German)}
\begin{document}
\maketitle
\label{chap-order}

\section{Introduction} 

This chapter deals with constituent order, with a focus on local order variants. \ili{English} is the
language that is treated most thoroughly in theoretical linguistics but is probably also a rather uninteresting
language as far as the possibilities of reordering constituents is concerned: the order of subject,
verb, and object is fixed in sentences like (\mex{1}):
\ea
Kim likes bagels.
\z
Of course, there is the possibility to front the object as in (\mex{1}) but this is a special,
non-local construction that is not the topic of this chapter but is treated in \crossrefchapterw{udc}.
\ea
Bagels, Kim likes.
\z
This chapter deals with scrambling\is{scrambling} (the local reordering of arguments) and with alternative
placements of heads (called \emph{head movement}\is{head movement} in some theories). Examples of the former are the
subordinate clauses in (\mex{1}) and an example of the latter is given in (\mex{2}):
\eal
\label{ex-permutation-mf}
\ex 
\gll {}[weil]          der Mann dem Kind das Buch gibt\\
     \spacebr{}because the.\NOM{} man the.\DAT{} child the.\ACC{} book gives\\\jambox{(\ili{German})}
\ex 
\gll {}[weil]          der Mann das Buch dem Kind  gibt\\
     \spacebr{}because the.\NOM{} man  the.\ACC{} book the.\DAT{} child gives\\
\ex 
\gll {}[weil]          das Buch der Mann dem Kind  gibt\\
     \spacebr{}because the.\ACC{} book the.\NOM{} man  the.\DAT{} child gives\\
\ex 
\gll {}[weil]          das Buch dem Kind  der Mann gibt\\
     \spacebr{}because the.\ACC{} book the.\DAT{} child the.\NOM{} man  gives\\
\ex 
\gll {}[weil]          dem Kind  der Mann das Buch gibt\\
     \spacebr{}because the.\DAT{} child the.\NOM{} man  the.\ACC{} book gives\\
\ex 
\gll {}[weil]          dem Kind  das Buch der Mann gibt\\
     \spacebr{}because the.\DAT{} child the.\ACC{} book the.\NOM{} man  gives\\
\zl
\ea
\gll Gibt der Mann dem Kind das Buch?\\
     gives the.\NOM{} man the.\DAT{} child the.\ACC{} book\\\jambox{(\ili{German})}
\glt `Does the man give the child the book?'
\z
(\mex{-1}) shows that in addition to the unmarked order in (\mex{-1}a) (see \citew{Hoehle82a} on the
notion of unmarked order), five other argument orders are possible in sentences with three-place
verbs. As with the examples just given, I will use \ili{German} if a phenomenon does not exist in
\ili{English}. Section~\ref{sec-warlpiri} discusses examples from \ili{Warlpiri}, a language having even freer constituent order.

(\mex{0}) shows that the verb is placed in initial position in yes/no questions in \ili{German}. This contrasts
with the verb-final order in the subordinate clause in (\mex{-1}a), which has the same order as far
as the arguments are concerned. This alternation of verb placement is usually treated as
head movement\is{head movement} in the transformational literature (\citealp{Bach62a}; \citealp*[\page34]{Bierwisch63a};
\citealp{Reis74a}; \citealp[Chapter~1]{Thiersch78a}). Declarative main clauses in \ili{German} are
V2\is{word order!V2}
clauses and the respective fronting of the preverbal constituent is usually treated as a non-local dependency (see
\crossrefchapteralt{udc}). Hence, V2 sentences will not be handled here.

The following sections explore the theoretical options within the HPSG framework for dealing with
these phenomena. I first discuss the separation of grammar rules into an immediate dominance part and
a linear precedence component in Section~\ref{sec-id-lp} and then flat vs.\ binary branching
structures (Section~\ref{sec-binary-flat}). While flat structures allow verbs to be ordered clause-finally
or clause-initially, this is not the case for binary branching structures, since only sisters can be
ordered. So, for (\mex{-1}a) one would get the bracketing in (\mex{1}a). If \emph{das Buch} `the
book' and \emph{gibt} `gives' are ordered in a different order, (\mex{1}b) results.
\eal
\ex[]{ 
\gll {}[weil]          [der Mann [dem Kind [das Buch gibt]]]\\
     \spacebr{}because \spacebr{}the.\NOM{} man \spacebr{}the.\DAT{} child \spacebr{}the.\ACC{} book gives\\
}
\ex[*]{
\gll {}[weil]          [der Mann [dem Kind [gibt das Buch]]]\\
     \spacebr{}because \spacebr{}the.\NOM{} man \spacebr{}the.\DAT{} child \spacebr{}gives
     the.\ACC{} book\\
}
\zl
Hence, local reordering is not sufficient to get clause-initial verb order and therefore, proposals with binary branching
structures are usually paired with HPSG's analogue of what is head-movement in transformational
theories. These are explained in Section~\ref{sec-head-movement-vs-flat}. Section~\ref{sec-domains} introduces an extension to
standard HPSG developed by \citet{Reape94a}: constituent order domains. Such constituent
order domains allow for discontinuous constituents and have been used to account for languages like \ili{Warlpiri}
\citep{DS99a}. In contrast, Section~\ref{sec-free-without-domains} shows how such languages can be analyzed without admitting discontinuous constituents.


\section{ID/LP format}
\label{sec-id-lp}

\is{scrambling|(}
HPSG was developed out of Generalized Phrase Structure Grammar\indexgpsg (GPSG) and Categorial\indexcg
  Grammar (\citealp{Ajdukiewicz35a-u,Pollard84a-u,Steedman2000a-u}; see also \crossrefchapteralt{evolution} on the
history of HPSG). The ideas concerning linearization of
daughters in a local tree were taken over from GPSG \citep*[Section~3.2]{GKPS85a}. In GPSG a separation between
\isi{immediate dominance} and \isi{linear precedence} is assumed. So, while in classical phrase structure
grammar, a phrase structure rule like (\mex{1}) states that the NP[nom], NP[dat] and NP[acc] have to
appear in exactly this order, this is not the case in GPSG and HPSG:
\ea
\label{rule-s-np-np-np-v}
S $\to$ NP[nom] NP[dat] NP[acc] V
\z
The HPSG schemata\is{schema} corresponding to the immediate dominance rule (ID rule) in (\mex{0}) do not express information
about ordering. Instead, there are separate linear precedence (LP) rules\is{Linear Precedence Rule}
(also called linearization rules). A schema like (\mex{0}) licenses 24 
different orders: the six permutations of the three arguments that were shown in
(\ref{ex-permutation-mf}) and all possible placements of the verb (to the right of NP[acc], between
NP[dat] and NP[acc], between NP[nom] and NP[dat], to the left of NP[nom]). Orders like NP[nom],
NP[dat], V, NP[acc] are not attested in \ili{German} and hence these orderings have to be filtered
out.\footnote{
Extraposition of NPs is possible in German \textcites[Section~13.1.1.3, 13.1.2.3]{Mueller99a}[ix--xi]{Mueller2002b}, although it is marked. Extraposition is a non-local
dependency and hence treated by a different mechanism. Like fronted NPs in V2 sentences, extraposed NPs are not affected by the
linearization rules stated here. See \citew{Keller95b}, \citew[Chapter~13]{Mueller99a} and \crossrefchapterw[Section~\ref{sec:UDC:Extraposition}]{udc} on extraposition.
} This is done by linearization rules, which can refer to features or to the function of a
daughter in a schema. (\mex{1}) shows some examples of linearization rules:

\eal
\ex X < V
\ex\label{lp-verb} X < V[\textsc{ini}$-$]
\ex\label{lp-head} X < Head [\textsc{ini}$-$]
\zl
The first rule says that all constituents have to precede a V in the local tree. The second rule
says that all constituents have to precede a V that has the \textsc{initial} value $-$. One option
to analyze \ili{German} would be the one that was suggested by \citet[Section~2.3]{Uszkoreit87a} within the framework
of GPSG\indexgpsg: one could allow for two linearization variants of finite verbs. So in addition to the
\textsc{ini}$-$ variant of verbs there could be an \textsc{ini}$+$ variant and this variant would be
linearized initially. This reduces the number of permutations licensed by (\ref{rule-s-np-np-np-v})
and LP rules to 12: verb-initial placement and 6 permutations of the NPs and verb-final placement with 6
permutations of the arguments. The ID rule in (\ref{rule-s-np-np-np-v}) together with the two
linearization rules linearizing the verb in initial or final position therefore licenses the same
orders as the following twelve phrase structure rules would do:
\eal
\ex \begin{tabular}[t]{@{}l@{ }l@{ }l@{ }l@{ }l@{ }}
S  & $\to$ NP[nom]& NP[dat] & NP[acc] & V\\
S  & $\to$ NP[nom]& NP[acc] & NP[dat] & V\\
S  & $\to$ NP[acc]& NP[nom] & NP[dat] & V\\
S  & $\to$ NP[acc]& NP[dat] & NP[nom] & V\\
S  & $\to$ NP[dat]& NP[nom] & NP[acc] & V\\
S  & $\to$ NP[dat]& NP[acc] & NP[nom] & V\\
\end{tabular}
\ex \begin{tabular}[t]{@{}l@{ }l@{ }l@{ }l@{ }l}
S  & $\to$ V NP[nom]& NP[dat] & NP[acc]\\
S  & $\to$ V NP[nom]& NP[acc] & NP[dat]\\
S  & $\to$ V NP[acc]& NP[nom] & NP[dat]\\
S  & $\to$ V NP[acc]& NP[dat] & NP[nom]\\
S  & $\to$ V NP[dat]& NP[nom] & NP[acc]\\
S  & $\to$ V NP[dat]& NP[acc] & NP[nom]\\
\end{tabular}
\zl
\is{scrambling|)}
Note that we do not need a linearization rule for every ID rule. For example, in a grammar
with rules for intransitive, transitive, and ditransitive verbs, head ordering is taken care of by
general LP rules of the type in (\ref{lp-verb}) applying to the respective ID rules. 
The LP rule in (\ref{lp-head}) is even more general than (\ref{lp-verb}) in that it does not
mention the part of speech but instead refers to the function of the constituent. The rule says that
a head that has the \textsc{ini} value `$-$' has to be linearized to the right of all other elements
in the local tree. Hence, it also applies to adjectives and postpositions and their dependents.

This separation of linearization rules from phrase structure rules also makes it possible to capture
other generalizations. For example, short elements tend to precede heavy constituents (Behaghel's
Law of Increasing Constituents, \citealp[\page 139]{Behaghel09a}). \citet[Chapter~5]{Uszkoreit87a}
captured one aspect of this more general rule by formulating a linearization statement requiring
that pronouns precede non-pronouns. The LP rules apply to a large set of ID rules, for example for
intransitive, transitive and ditransitive verbs. By factoring out the LP constraints,
generalizations over the whole set of phrase structure rules are covered.  Uszkoreit's constraints
on the order of arguments in the so-called \mf (that is, for rules like (\mex{0})) are assumed to be
violable. While violable constraints are not part of the standard HPSG formalism, this is something
desirable and something that is worked on. See also Abeillé \& Godard's work on weight-based
linearization and the (reduced) mobility of various categories: bare nominals in various languages,
certain pronouns \citep{AG99a-u}, certain adverbs \citep{AG2001a-u}, negation
\citep{AG97a-u,AG2004a-u}, and attributive adjectives \citep{AG99b-u}. In various papers, Abeillé \&
Godard propose a three valued \attrib{weight} feature to account for the ordering of light,
middle-weight and heavy constituents \citep{AG2000a,AG2004a-u}. See also
\crossrefchaptert[Section~\ref{sec-romance-complex-predicates}]{complex-predicates} on complex
  predicates\is{complex predicate} and weight.
%% \inlinetodostefan{
%% There is also the work on heaviness and lightness, to account for the (reduced) mobility of various categories: bare nominals in various languages, certains pronouns (A\&G 1999a), certains adverbs \citep{AG97a-u,AG2004a-u-French}, negation \citep{AG2001a-u}, and attributive adjectives \citep{AG99b-u-French}. In various papers, Abeillé and Godard propose a three valued WEIGHT feature to account for light, middle-weight and heavy constituents \citep{AG2000a,AG2004a-u-French}. The LP rules ususally apply to middle weight ones. The light/heavy distinction is also relevant for the Complex predicate chapter, but it should definitely be mentioned, maybe as a subsection or word order/constituent order since it is part of what LFG people call X° syntax.
%% }

\largerpage
This treatment of constraints on linearization has an advantage that was already pointed out by
researchers working in GPSG: it captures the generalizations regarding linearization. For instance,
the order of verbs with respect to their arguments is the same in embedded sentences in
\ili{German}, independent of the finiteness of the verb. Hence, as was explained above, one LP
statement captures the generalization about argument-head order for examples like (\mex{1}):

\eal
\ex 
\gll dass er dem Mann das Buch gab\\
     that he.\nom{} the.\dat{} man the.\acc{} book gave\\
\glt `that he gave the man the book'
\ex
\gll dass er versucht, [dem Mann das Buch zu geben]\\
     that he.\nom{} tried     \spacebr{}the.\dat{} man the.\acc{} book to give\\
\glt `that he tried to give the man the book'
\zl
The generalizations about linearization of arguments with respect to each other are also
captured. For example, the relative order of dative and accusative object in (\mex{0}) is the same
for in both environments. The constraints regarding linearization hold across rules. By factoring these constraints out,
generalizations regarding constituent order can be captured. See \citet[Section~3.1]{Uszkoreit87a}
for weighted constraints for the ordering of constituents in the \mf.

Furthermore, cross-linguistic generalizations about constituent structure can be captured. For
example, the two phrase structure rules in (\mex{1}) would be needed for head-initial and head-final
languages, respectively: 
\eal
\ex VP $\to$ V NP NP
\ex VP $\to$ NP NP V
\zl
In an ID/LP framework only one ID rule is needed to describe both sorts of languages. The
linearization of the head is factored out of the rules.

Similarly, HPSG has just one schema for Head-Adjunct structures, although languages like
\ili{English} have some adjuncts that precede their heads and others that follow them. The schema in
(\ref{schema-head-adjunct}) corresponds to a phrase structure rule in GPSG. The values of features
like \textsc{head-dtr} and \textsc{non-head-dtrs} are feature descriptions that correspond to
daughters in local trees or to symbols on right-hand sides of phrase structure rules (see
\crossrefchapteralt[\page \pageref{prop:sec-elements}]{properties} for the representation of
dominance structure in HPSG). The schema in (\mex{1}) does not say anything about the order of the
daughters: 
\largerpage
\ea
Head-Adjunct Schema\is{schema!Head-Adjunct}:\\
\label{schema-head-adjunct}
\type{head-adjunct-phrase} \impl\\
\avm{
[ head-dtr       [ synsem \1 ] \\
  non-head-dtrs  < [ synsem|loc|cat & [ head|mod \1\\
                                           spr   <>\\
                                           comps <>\\
                                         ] ] > ]
}
\z
There is a head daughter and a list of non-head daughters. The respective daughters are specified as
the value of a feature or as an element in a list but they are not ordered with respect to each
other in the schema. Ordering is taken care of by two LP rules saying that adjuncts marked
as pre-modifiers (\eg attributive adjectives) have to precede their head while those that are marked
as post-modifiers (noun-modifying prepositions) follow it:
\eal
\label{lp-pre-modifier}
\ex Adjunct[\textsc{pre-modifier} +] $<$ Head
\ex Head $<$ Adjunct[\textsc{pre-modifier} --]
\zl
In general, there are two options for two daughters: head-initial and head-final order. Examples are
given in (\mex{1}):\footnote{%
$\oplus$\is{$\oplus$} (\texttt{append}\isrel{append}) is a \isi{relational constraint} that concatenates two lists. 
}
\eal
\ex \oneline{%
\begin{tabular}[t]{@{}ll@{}}
head-initial:                                   & example:\\
%
\avm{
[ phon & \ibox{1} \+ \2\\
  head-dtr & [ phon \1 ]\\
  nh-dtrs & < [ phon \2 ] > ]
}&%
\avm{
[ phon & \phonliste{ squirrel, from, America }\smallskip\\
  head-dtr & [ phon \phonliste{ squirrel } ]\\
  nh-dtrs & < [ phon \phonliste{ from, America } ] > ]
}\\
\end{tabular}
}
\ex \begin{tabular}[t]{@{}ll@{}}
head-final:                            & example:\\
%
\avm{
[ phon     & \2 \+ \1\\
  head-dtr & [ phon \1 ]\\
  nh-dtrs  & < [ phon \2 ] > ]
}&%
\avm{
[ phon & \phonliste{ gray, squirrel }\smallskip\\
  head-dtr & [ phon \phonliste{ squirrel } ]\\
  nh-dtrs  & < [ phon \phonliste{ gray } ] > ]
}\\
\end{tabular}
\zl
When linearization rules enforce head-initial order, as in the case of modification by a PP in
\ili{English}, the \phonv of the head daughter is concatenated with the \phonv of the non-head daughter,
and if the order has to be the other way around as in the case of adjectives modifying nouns, the
non-head daughter is concatenated with the head daughter. An adjective is specified as
\textsc{pre-modifier}~+ and a preposition as \textsc{pre-modifier}~$-$. Since these features are
head-features (see \crossrefchaptert[\page \pageref{page-hfp}]{properties} on head features), they
are also accessible at the level of adjective phrases and prepositional phrases.

\largerpage
For languages with free variation in head-adjunct order, it would suffice to not state any LP
rule and one would get both orders with the same Head-Adjunct schema. So, the separation of
immediate dominance and linear precedence allows for an underspecification of order. Therefore HPSG
grammarians are not forced to assume several different constructions for attested patterns or
derivational processes that derive one order from another more basic one.


\section{Flat and binary branching structures}
\label{sec-binary-flat}

The previous section discussed LP rules and used flat phrase structure rules for illustration. The\is{scrambling}
corresponding flat structures are also used in HPSG. (\ref{schema-hc-flat}) shows a Head-Complement schema that
combines a head with all the complements selected via the \compsl.\footnote{
  \citet[\page 4]{GSag2000a-u} assume a list called \dtrs for all daughters including the head
  daughter. It is useful to be able to refer to specific non-head daughters without having to know a
  position in a list. For example in head-adjunct structures the adjunct is the selector. So I keep
  \dtrs for a list of ordered daughters and \textsc{head-dtr} and \textsc{non-head-dtrs} for
  material that is not necessarily ordered with respect to each other. In the case of binary
  branching, structures like head-adjunct structures, head-filler structures, head-specifier
  structures, and head-complement structures have the non-head daughter as the sole member of the \textsc{non-head-dtrs} list.%
}

\ea
Head-Complement Schema\is{schema!Head-Complement}:\\
\label{schema-hc-flat}
\type{head-complement-phrase} \impl\\*
% todo avm
\avm{
[ \punk{synsem|loc|cat|comps}{<>}\\
  \punk{head-dtr}{[ synsem|loc|cat|comps \1 ]}\\
  non-head-dtrs \upshape ! \rel{synsems2signs}(\1) ! ]
}
\z
\largerpage
\rel{synsems2signs}\isrel{synsems2signs} is a \isi{relational constraint} mapping a list of \type{synsem} objects as they are contained in
the \compsl onto a list of objects of type \type{sign} as they are contained in \textsc{head-dtr}
and \textsc{non-head-dtrs} (see \citealt[\page
  34]{GSag2000a-u} for a similar proposal).\footnote{
  In Sign-Based Construction Grammar\indexsbcg (SBCG; \citealt{Sag2012a}) the objects in valence lists are of the same type as the
  daughters. A relational constraint would not be needed in this variant of the HPSG
  theory (see \crossrefchapteralt[Section~\ref{prop:sec-sbcg}]{properties} and
  \crossrefchapteralt[Section~\ref{cxg:sec-sbcg}]{cxg} for further discussion of SBCG). Theories working with a binary branching Head-Complement Schema as (\ref{hcs-binary}) on
  page~\pageref{hcs-binary} would not need the relational constraint either, since the \type{synsem} object in
  the \compsl can be shared with the \synsemv of the element in the list of non-head daughters directly.
}
% inserted in second edition:
The effect of \rel{synsems2signs} can be sketched as in (\mex{1}):
\ea
\label{ex-schema-hc-flat-synsem-sign}
\avm{
[ synsem|loc|cat|comps <>\\
  head-dtr [ synsem|loc|cat|comps < \1, \ldots, \tag{n} > ]\smallskip\\
  non-head-dtrs < [ synsem \1 ], \ldots, [ synsem \tag{n} ] >  ]
}
\z
If the \compsl is a list with n elements, \rel{synsems2signs} returns a list with n signs with
\synsem values corresponding to the elements in the \compsl.

How the schema in (\ref{schema-hc-flat}) can be used to analyze VPs like the one in (\mex{1}) is shown in Figure~\ref{fig-gave-Sandy-a-book}.
\ea
\label{ex-gave-sandy-a-book}
Kim gave Sandy a book.
\z
\begin{figure}
\begin{forest}
sm edges
[{V[\comps \eliste]}
  [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]} [gave]]
    [\ibox{1} NP [Sandy]]
    [\ibox{2} NP [a book,roof]]]
\end{forest}
\caption{\label{fig-gave-Sandy-a-book}Analysis of the VP \emph{gave Sandy a book} with a flat structure}
\end{figure}
HPSG differs from purely phrase structure-based approaches in that the form of a linguistic object
is not simply the concatenation of the forms associated with the terminal symbols in a tree (words or
morphemes). Every linguistic object has its own phonological representation. So in principle one could
design theories in which the combination of \emph{Mickey Mouse} and \emph{sleeps} is pronounced as
\emph{Donald Duck laughs}. Of course, this is not done. The computation of the \phon value of the
mother is dependent of the \phonvs of the daughters. But the fact that the \phonvs of a linguistic
sign are not necessarily a strict concatenation of the \phonvs of the daughters can be used to model
languages having a less strict order than \ili{English}. \citet[\page 168]{ps} formulate the Constituent
Order Principle, which is given as (\ref{cop}) in adapted form:
\ea
\label{cop}
Constituent Order Principle\is{principle!Constituent Order}:\\
~\\[-3mm]
\type{phrase} \impl 
\avm{ 
[ phon & \upshape ! \rel{order-constituents}(\1) !\\
  dtrs & \1 ]
}
\z
\dtrs is a list of all daughters including the head daughter (if there is one). This setting makes
it possible to have the daughters in the order in which the elements are ordered in the \compsl
(primary object, secondary object, and obliques) and then compute a \phonv in which the secondary
object precedes the primary object. \ili{French} is a language with freer constituent order than
\ili{English} and such flat structures with appropriate reorderings are suggested by \citet{AG2000a}. For
\ili{English} the function \rel{order-constituents}\isrel{order-constituents} would just return a concatenation of the \phonvs of the
daughters, but for other languages it would be much more complicated. In fact this function and its
interaction with linear precedence constraints was never worked out in detail.

%\largerpage[2]
Researchers working on \ili{English} and \ili{French} usually assume a flat structure \parencites[\page 39--40, 362]{ps2}[\page
  479]{Sag97a}[\page 34]{GSag2000a-u}{AG2000a} but assuming binary
branching structures would be possible as well, as is clear from analyses in Categorial Grammar,
where binary combinatory rules are assumed \citep{Ajdukiewicz35a-u,Steedman2000a-u}. For languages
like \ili{German} it is usually assumed that structures are binary branching (but see \citealt[\page 156]{Reape94a} and
\citealt[\page 51]{BvN98a}). The reason for this is that
adverbials can be placed anywhere between the arguments, as the following example from \citet[\page
  145]{Uszkoreit87a} shows:
\ea
\gll \emph{Gestern} hatte \emph{in} \emph{der} \emph{Mittagspause} der Vorarbeiter \emph{in} \emph{der} \emph{Werkzeugkammer} dem Lehrling \emph{aus
Boshaftigkeit} \emph{langsam} zehn schmierige Gußeisenscheiben \emph{unbemerkt} in die Hosentasche gesteckt. \\
yesterday had during the lunch.break the foreman in the tool.shop the apprentice maliciously slowly ten
greasy cast.iron.disks unnoticed in the pocket put\\
\glt `Yesterday during the lunch break, the foreman maliciously put ten greasy cast iron disks slowly into the
apprentice's pocket unnoticed.'
\z
A way to straightforwardly analyze adjunct placement in \ili{German} and \ili{Dutch} is to assume that adjuncts can
attach to any verbal projection. For example, Figure~\ref{fig-adjunct-placement-german} shows the
analysis of (\mex{1}):
\ea
\gll weil deshalb jemand gestern dem Kind schnell das Buch gab\\
     because therefore somebody yesterday the child quickly the book gave\\%\german
\glt `because somebody quickly gave the child the book yesterday'
\z
\begin{figure}
\begin{forest}
sm edges
[V
       [Adv [deshalb;therefore]]
       [V
         [NP [jemand;somebody]]
         [V
           [Adv [gestern;yesterday]]
           [V
              [NP [dem Kind;the child,roof]]
              [V
                [Adj [schnell;quickly]]
                [V
                  [NP [das Buch;the book,roof]]
                  [V [gab;gave]]]]]]]]
\end{forest}
\caption{Analysis of [\emph{weil}] \emph{deshalb jemand gestern dem Kind schnell das Buch gab}
  `because somebody quickly gave the child the book yesterday' with binary branching structures}\label{fig-adjunct-placement-german}
\end{figure}
\largerpage
The adverbials \emph{deshalb} `therefore', \emph{gestern} `yesterday' and \emph{schnell} `quickly'
may attach to any verbal projection. For example, \emph{gestern} could also be placed at the other
adjunct positions in the clause. 

Binary branching structures with attachment of adjuncts to any verbal projection also account for \isi{recursion} and hence the
fact that arbitrarily many adjuncts can attach to a verbal projection.
%any constituent can be combined with the verb to the exclusion of any other argument \citep{Mueller2003b,MuellerLehrbuch1}\todostefan{page}:
%% \eal
%% \label{bsp-acc-dat-pvp}
%% \ex 
%% \gll Den Wählern erzählen sollte man diese Geschichte nicht.\\
%%      the.\DAT{} voters tell should one this.\ACC{} story not\\\jambox{(\ili{German})}
%% \glt `One should not tell the voters these stories.'
%% \ex 
%% \gll Märchen erzählen sollte man den Wählern nicht.\\
%%      stories.\ACC{} tell     should one the.\DAT{} voters not\\
%% \zl
%% In fact any subset of the objects can be combined with the verb and form a constituent. To account
%% for this
Of course it is possible to formulate analyses with flat structures that involve arbitrarily many
adjuncts \parencites{Kasper94a,Noord94}[Section~5]{AG2000a}[Section~4]{BMS2001a}, but these analyses involve relational
constraints\is{relational constraint} in schemata or in lexical items or an infinite lexicon. In Kasper's analysis, the
relational constraints walk through lists of daughters of unbounded length in order to compute the
semantics. In the other three analyses, (some) adjuncts are treated as valents, which may be problematic
because of \isi{scope} issues. This cannot be dealt with in detail here, but see \citew[Section~3.6]{LH2006a} and
\citew{Chaves2009a} for discussion.

The\is{scrambling|(} following schema licenses binary branching head-complement phrases:
\ea
Head-Complement Schema\is{schema!Head-Complement} (binary branching):\\
% todo avm distance after \+
\label{hcs-binary}\label{order-hcs-binary}
\type{head-complement-phrase} \impl\\*
\avm{
[ \punk{synsem|loc|cat|comps}{\1 \+ \2}\\
  head-dtr      & [ synsem|loc|cat|comps \1 \+ < \3 > \+ \2 ] \\
  non-head-dtrs & < [ synsem \3 ] > ]
}
\z
%\largerpage
The \compsl of the head daughter is split into three lists: a beginning \iboxb{1}, a list containing \ibox{3} and a rest
\iboxb{2}. \ibox{3} is identified with the \synsemv of the non-head daughter. All other elements of
the \compsl of the head daughter are concatenated and the result of this concatenation (\ibox{1}
$\oplus$ \ibox{2}) is the \compsl of the mother node. This schema is very general. It works for
languages that allow for scrambling, since it allows an arbitrary element to be taken out of the \compsl
of the head daughter and realize it in a local tree.\is{scrambling|)} The schema can also be ``parameterized\is{parameter}'' to account
for languages with fixed word order. For head-final languages with fixed order, \ibox{2} would be the
empty list (= combination with the last element in the list) and for head-initial languages with
fixed order (\eg \ili{English}), \ibox{1} would be the empty list (= combination with the first element in
the list). Since the elements in the \compsl are ordered in the order of Obliqueness\is{obliqueness} \citep{KC77a,Pullum77a} and since this
order corresponds to the order in which the complements are serialized in \ili{English}, the example in (\ref{ex-gave-sandy-a-book}) can be
analyzed as in Figure~\ref{fig-gave-sandy-a-book-binary}.\footnote{
  This structure may seem strange to those working in Mainstream Generative Grammar (MGG,
  GB/\isi{Minimalism}). In MGG, different branchings are assumed, since the form of the tree plays a role in
  \isi{Binding Theory}. This is not the case in HPSG: Binding is done on the \argstl. See
  \crossrefchaptert{binding} for a discussion of HPSG's Binding Theory and
  \crossrefchaptert{minimalism} for a comparison between HPSG and Minimalism.%
}
\begin{figure}
\scalebox{.9}{%
\begin{forest}
sm edges
[{V[\comps \eliste]},baseline
  [{V[\comps \sliste{ \ibox{2} } ]}
    [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]} [gave]]
    [\ibox{1} NP [Sandy]]]
    [\ibox{2} NP [a book,roof]]]
\end{forest}
}
\hfill
\scalebox{.9}{%
\begin{forest}
sm edges
[{V[\comps \sliste{ \ibox{1} }]},baseline
  [\ibox{2} NP [Sandy;Sandy]]
  [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]}
    [\ibox{3} NP [ein Buch;a book,roof]]
    [{V[\comps \sliste{ \ibox{1}, \ibox{2}, \ibox{3} } ]} [gab;gave]]]]
\end{forest}
}
\caption{\label{fig-gave-sandy-a-book-binary}\label{fig-sandy-ein-buch-gab}Analysis of the English VP \emph{gave Sandy a book} and
  the corresponding \ili{German} verbal projection \emph{Sandy ein Buch gab} with binary branching structures}
\end{figure}
The second tree in the figure is the \ili{German} counterpart of \emph{gave Sandy a book}: the finite verb
in final position with its two objects in normal order. Section~\ref{sec-svo-sov} explains why SOV
languages like \ili{German} and \ili{Japanese} contain their subject in the \compsl while SVO languages like
\ili{English} and \ili{Romance} languages do not.

The alternative to using relational constraints as the two appends in the schema in (\ref{hcs-binary}) is to
use sets rather than lists for the representation of valence information
(\citealp[Section~4]{Gunji86a}; \citealp[\page 8]{HN89a}; \citealp[\page 296]{Pollard90a-Eng};
\citealp[\page 187]{Oliva92b}; \citealp*[\page 205]{EEU92a}). The Head-Complement Schema would
combine the head with one of its complements. Since the elements of a set are not ordered, any
complement can be taken and hence all permutations of complements are accounted for. 

%\largerpage
The disadvantage of set-based approaches is that sets do not impose an order on their members, but
an order is needed for various subtheories of HPSG (see \crossrefchaptert{case} on case assignment,
and \crossrefchaptert{binding} on Binding Theory). In the approach proposed above and in Müller
(\citeyear[\page 7]{Mueller2005c}; \citeyear[\page 945]{MuellerHPSGHandbook}; \citeyear[\page
53--54]{MuellerCoreGram}), the valence lists are ordered but the schema allows for combination with
any element of the list. For valence representation and the order of elements in valence lists see
\crossrefchapterw{arg-st}.\label{order:page-scrambling-end}
%\todostefan{maybe cite \citet{AMM2013a}}
%) as the underlying representation and determinant of basic order


\section{SVO vs.\ SOV}
\label{sec-svo-sov}

%\largerpage[2]
The\is{word order!SVO|(}\is{word order!SOV|(} careful reader will have noticed that the \compsl of
\emph{gave} in Figure~\ref{fig-gave-sandy-a-book-binary} contains the two objects, while its
\ili{German} counterpart \emph{gab} has three elements in the \compsl. The rationale behind this
difference is explained in this section.

In principle, one could assume a rule like (\ref{rule-s-np-np-np-v}) for SVO languages like \ili{English}
as well. The SVO order would then be accounted for by linearization rules stating that
NP[\type{nom}] precedes the finite verb while other arguments follow it. This would get the facts
about simple sentences like (\mex{1}a) right but leaves the analysis of (\mex{1}b) open.
\eal
\ex Peter reads books.
\ex Peter often reads books.
\zl
The generalization about languages like \ili{English} is that adverbials can appear to the left of
verbs or to the right of the verbs' complements, that is, to the left or to the right of the unit
formed by verbs and complements: the VP. Researchers like \citet{Borsley87a} argued that subjects,
specifiers, and complements differ in crucial ways and should be represented by special (valence)
features. For example, the subject of the VP \emph{to read  more books} in (\mex{1}) is not realized but is
referred to in Control Theory \crossrefchapterp{control-raising}. 
\ea
Peter tries to read more books.
\z 
The subject in \ili{English} main clauses is similar to the determiner in nominal structures, so one
way of expressing this similarity is by using the same valence features and the same schema for
subject-VP combinations as for determiner-noun combinations.\footnote{
  This is non-standard in HPSG. Usually the \textsc{subject} feature\isfeat{subj} is used for subjects and \spr\isfeat{spr}
  for determiners (but see \citew*[100--103]{SWB2003a}, where subjects are also selected via
  \spr). I follow the \ili{German} HPSG tradition and use \subj for unexpressed subjects. See 
  also \crossrefchaptert{np} for alternative analyses of nominal structures that do not assume a
  selection of the determiner by the noun. The proposal suggested here captures the parallelism
  between the sentential and the nominal domain \citep{MyPM2021a}, a goal of analyses in
    GB/Minimalism since \citew{Abney87a}.%
}
The schema is given here as (\ref{hss}):

\ea
\label{hss}
Specifier-Head Schema\is{schema!Specifier-Head}:\\
\type{specifier-head-phrase} \impl\\*
\avm{
% AVM punk todo
[ synsem|loc|cat|spr \1\\
  \punk{head-dtr|synsem|loc|cat}{[ spr   & \1 \+ < \2 > \\
                            comps & <> ]}\vspace{-10pt}\\% bug avm
  non-head-dtrs < [ synsem \2 ] >& ]
}
\z

%\largerpage[2]
\noindent
The last element of the \sprl is realized as the non-head daughter. The remaining list is passed up
to the mother node. 
Note that the non-head daughter is taken from the end of the \sprl. For heads that have exactly one specifier this
difference is irrelevant, but in the analysis of object shift in \ili{Danish} suggested by \citet{MOe2013b},
the authors assume multiple specifiers and hence the difference in order of combination is
relevant. The head-daughter must have an empty \compsl. This way it is ensured that verbs form a
unit with their objects (the VP) and the subject is combined with the VP, rather than the subject
combining with a lexical verb and this combination combining with objects later.

The analysis of the sentence in (\mex{1}) including the analysis of the NP \emph{a book} is given
in Figure~\ref{fig-kim-gave-sandy-a-book-binary}.
\ea
Kim gave Sandy a book.
\z
\begin{figure}
\begin{forest}
sm edges
[{V\feattab{\spr \eliste,\\
            \comps \eliste}}
  [\ibox{1} NP [Kim]]
  [{V\feattab{\spr \sliste{ \ibox{1} },\\
              \comps \eliste}}
    [{V\feattab{\spr \sliste{ \ibox{1} },\\
                \comps \sliste{ \ibox{2}, \ibox{3} } }} [gave]]
    [\ibox{2} NP [Sandy]]
    [\ibox{3} NP 
      [\ibox{4} Det [a]]
      [N\feattab{\spr \sliste{ \ibox{4} },\\
                 \comps \eliste }   
        [book]]]]]
\end{forest}
\caption{\label{fig-kim-gave-sandy-a-book-binary}Analysis of \emph{Kim gave Sandy a book} with \spr and \comps feature and a flat VP structure}
\end{figure}
For \ili{German}, it is standardly assumed that the subjects of finite verbs are treated like
complements (\citealp[\page 295--296]{Pollard90a-Eng}, \citealp[Section~3.1.1]{Kiss95a}) and hence are
represented on the \compsl (as in Figure~\ref{fig-sandy-ein-buch-gab}). The assumption that
arguments of German finite verbs are complements is also made by researchers working in different research traditions
\citep[e.g.][\page 376]{Eisenberg94b}. By assuming that the subject is listed among the complements
of a verb it is explained why it can be placed in any position before, between, and after
them.\footnote{%
  An alternative way of accounting for the orders would be to keep the special feature for subjects
  and allow subjects to combine with non-maximal verbal projections. The Head-Specifier Schema in
  (\ref{hss}) would lack the constraint on the head daughter to be \comps \eliste. However, this would cause problems
  in the analysis of structures with the head in the middle. The standard analysis of (i) combines
  the head \emph{Bild} `picture' with the PP complement first and then the result \emph{Bild von Kim} with
  the determiner. 
\ea
\gll das Bild von Kim\\
     the picture of Kim\\
\z
If the constraint that the head daughter in head-specifier structures has to have an empty \comps
list is removed, two analyses are possible: the determiner can be combined with the noun first and
the \emph{von}-PP can be added later. This kind of spurious ambiguity is usually avoided.
}
So in summary, \ili{German} differs from \ili{English} in the way the arguments are distributed on the valence lists, 
in order to capture the similarity in \ili{English} between combinations of subjects with VPs and 
determiners with nouns, and to allow \ili{German} the flexible constituent order it needs. However,
HPSG has a more basic representation in which the languages do behave the same: the argument structure
represented on the \argstl. The \argstl contains \type{synsem} objects and is used for linking 
%(\citealp{Wechsler95a-u,Davis2001a-u,DK2000b-u}; \crossrefchapteralp{arg-st})
\crossrefchapterp{arg-st}, case assignment 
%(\citealp{Meurers99b,Prze99}; \crossrefchapteralp{case})
\crossrefchapterp{case}, and binding 
%(\citealp{PS92a}; \crossrefchapteralp{binding}).
\crossrefchapterp{binding}. 
Ditransitive verbs in \ili{German} and \ili{English} have three NP arguments on
their \argst and they are linked in the same way to the semantic representation \parencites[\page 62]{MuellerLFGphrasal}{MuellerGermanic}.
(\mex{1}) shows the mapping from \argst to \spr and \comps:
\ea
\begin{tabular}[t]{@{}l@{~}ll@{~}l@{}}
a. & \emph{gives} (\ili{English}, SVO language): & b. & \emph{gibt} (\ili{German}, SOV language):\\
& \avm{
[ spr    & < \1 >\\
  comps  & \2\\
% todo avm space after \1 is bigger than before \1
  arg-st & < \1 NP > \+ \2 < NP, NP >  ]
}& 
 & \avm{
[ spr    & <>\\
  comps  & \1\\
  arg-st & \1 < NP, NP, NP > ]
}
\end{tabular}
\z
In SVO languages, the first element of the \argstl is mapped to \spr and all others to \comps and in
languages without designated subject position all \argst elements are mapped to \comps.
\is{word order!SVO|)}\is{word order!SOV|)}

Having explained scrambling in HPSG and the order of subjects in SVO languages, I now turn to ``head movement''.


%% \section{Nonlocal dependencies}
%% Brief mention and pointers to Chapter~\ref{chap-udc}.

\section{Head movement vs.\ constructional approaches that assume flat structures}
\label{sec-head-movement-vs-flat}


The\is{head movement|(} \ili{Germanic} languages signal clause type by verb position. All \ili{Germanic} languages with the
exception of \ili{English} are V2\is{word order!V2} languages: the finite verb is in second position in declarative main
clauses. The first position can be filled by any other constituent, for example a subject,
objects, or adverbials. (\mex{1}) shows an example from the V2 language \ili{German} and its \ili{English} translation.
\ea 
\gll Eigentlich mag ich Katzen sehr.\\
     actually   like I cats really\\\jambox*{(\ili{German})}
\glt `I actually really like cats.'
\z
The fronted material is not necessarily from the matrix clause, clause boundary
crossing non-local dependencies are possible. The same holds for questions with \emph{w}"=phrases. 

Yes/no questions are formed by putting the verb in initial position:
\ea
\gll Magst du Katzen?\\
     like  you cats\\\jambox*{(\ili{German})}
\glt `Do you like cats?'
\z
\ili{English} is a so-called \emph{residual V2 language} \citep{Rizzi1990a-u}, that is, there are
some constructions that are parallel to what is known from V2 languages.
%\todostefan{DB: \ili{English} comes out of the blue.} 
For example, while declarative clauses are in base order (SVO), questions follow the pattern that is
known from other \ili{Germanic} languages with the finite verb in second position.\footnote{%
  SVO is not V2 although the verb is in second position in SVO sentences. Languages can be
  categorized into SOV, SVO, VSO, OSV, OVS, and VOS languages and into V2\is{word order!V2} or non-V2 languages. These two
  dimensions are independent. For example, \ili{Danish} is an SVO\is{word order!SVO} language that is V2, while \ili{German} is SOV\is{word order!SOV}
  and V2 \citep{Haftka96a,Haider2020a}. See \citew{MuellerGermanic} for discussion and the analysis of this variation in HPSG.
}
\ea
What$_i$ will Kim read \trace$_i$? 
\z
Analyses assuming flat structures (or flat linearization domains, see Section~\ref{sec-domains})
usually treat alternative orders of verbs in \ili{Germanic} languages as linearization variants
\citep{Reape94a,Kathol2001a,Mueller95c,Mueller2003a,TBjerre2006a}, but this is not necessarily so, as
Bouma and van Noord's analysis of \ili{Dutch} clauses shows \citep[\page 62, 71]{BvN98a}. The alternative to
verb placement as linearization is something that is similar to verb movement in Government \&
Binding\indexgb: an empty element takes the position of the verb in its canonical position and the verb is realized
in initial or -- if something is realized before the finite verb -- in second position. The following subsection deals with such approaches in more
detail. Subsection~\ref{sec-aux-inversion-phrasal} deals with a constructional approach.

\subsection{Head movement approaches}
\label{sec-head-movement}

%\largerpage
Building on work by \citet{Jacobson87} in the framework of Categorial Grammar\indexcg,
\citet{Borsley89} showed that in addition to the analysis of auxiliary inversion in \ili{English} that was
suggested in GPSG \citep[Section~4.3]{GKPS85a}, an analysis that is similar to the movement-based analysis in GB
is possible in HPSG as well. Head movement analyses in GPSG and HPSG are concerned with the verb
placement in pairs such as the one in (\mex{1}) rather than with adverb placement as in GB analyses
of head movement by \citet{Pollock89a-u} and \citet{Cinque99a-u}.
\eal
\ex Will Kim get the job?
\ex Kim will get the job.
\zl
The technique that is used in Borsley's analysis is basically the same that
was developed by \citet{Gazdar81a} for the treatment of nonlocal dependencies in GPSG. An empty category is
assumed and the information about the missing element is passed up the tree until it is bound off at
an appropriate place (that is, by the fronted verb). Note that the heading of this section contains
the term \emph{head movement} and I talk about traces, but it is not the case that something is
actually moved. There is no underlying structure with a verb after the subject that is transformed into one with the
verb fronted and a remaining trace in the verb's original position. Instead, the empty element
is a normal element in the lexicon and can function as the verb in the respective position.
The analysis of (\mex{0}a) is shown in Figure~\ref{fig-did-kim-get-the-job-hm}.
\begin{figure}
\begin{forest}
sm edges
[S
  [{V[\comps \sliste{ \ibox{1} } ]} 
    [{V[\textsc{loc} \ibox{2} ]} [did]]]
  [\ibox{1} S\feattab{ \head{}|\dsl \ibox{2},\\
                 \spr \sliste{ },\\
                 \comps \sliste{  } }
    [\ibox{3} NP [Kim]]
    [VP\feattab{ \head{}|\dsl \ibox{2},\\
                 \spr \sliste{ \ibox{3} },\\
                 \comps \sliste{  } }
      [V\ibox{2}\feattab{ \head{}|\dsl \ibox{2},\\
                          \spr \sliste{ \ibox{3} },\\
                          \comps \sliste{ \ibox{4} } } [\trace]]
      [\ibox{4} VP [get the job, roof]]]]]
\end{forest}
\caption{\label{fig-did-kim-get-the-job-hm}Analysis of English auxiliary constructions as head"=movement following \citet{Borsley89}}
\end{figure}
%\largerpage
A special variant of the auxiliary is licensed by a unary rule. The unary rule has as a daughter the auxiliary as
it appears in canonical SVO order as in (\mex{0}b). It licenses an auxiliary selecting a full clause
in which the daughter auxiliary (with the \locv \ibox{2}) is missing. The fact
that the auxiliary is missing is represented as the value of \textsc{double slash} (\dsl). The value of \dsl is a
\type{local} object, that is, something that contains syntactic and semantic information (\ibox{2}
in Figure~\ref{fig-did-kim-get-the-job-hm}). \dsl is a head feature and hence available everywhere
along a projection path (see \crossrefchaptert[\page \pageref{page-hfp}]{properties} for the Head Feature
Principle). The empty element for head movement is rather simple:
% \footnote{%
% Cyclic structures are common in HPSG. For example, \citet{ps2} assume that determiners are selected
% by nouns via a valence feature and they are selecting the noun via a special feature
% \textsc{spec}. This causes cycles. Further authors explicitly assuming cyclic structures are:
% \cites[\page 56]{EV94a}[\page 2007]{Meurers2000b}[\page 176]{Meurers2001a}[\page
% 638]{Samvelian2007a}. In computer systems like the LKB \citep{Copestake2002a}, cycles are sometimes excluded.% 
% }
% \itd{JP: Should you have a footnote saying that not all HPSGers allow
%   these circular feature descriptions? Stefan: Did this. But it feels inappropriate. Can I remove
%   the footnote?}
\ea
Empty element for head movement:\\*
\avm{
[ \type*{word}
  phon       & <>\\
  synsem|loc & \1 [ cat|head|dsl \1 ] ]
}
\z
It states that there is an empty element that has the local requirements that correspond to its
\dslv. For cases of verb movement it says: I am a verb that is missing itself. 
%% The special lexical item that is needed for the
%% analysis of (\mex{-1}) is given in (\mex{1}):
%% \ea
%% \catv of the lexical item used for auxiliary inversion:\\
%% \ms{
%% head & \ms[verb]{
%%         vform & fin\\
%%         dsl   & none\\
%%         }\\
%% comps & \liste{ \ms{ cat \ms{ head & \ms[verb]{ dsl & \ms{ head & \ms[verb]{ vform & fin\\
%%                                                                              aux   & +\\
%%                                                                            }\\
%%                                                          }\\
%%                                                }\\
%%                               spr & \eliste\\
%%                               comps & \eliste\\
%%                               } } }\\
%% }
%% \z

Such head-movement analyses are assumed by most
researchers working on \ili{German} (\citealp*[Section~4.7]{KW91a}; \citealp{Oliva92a}; \citealp*{Netter92};   
\citealp*{Frank94}; \citealp*[Section~2.2.4.2]{Kiss95a}; \citealp[Section~3.1.1.1]{Feldhaus97},
\citealp[Section~5.1]{Meurers2000b}; \citealp{Mueller2005c,MuellerGS}) and also by \citet[\page 62,
  71]{BvN98a} in their work on \ili{Dutch}, by \citet{MOeDanish} in their grammar of
\ili{Danish} and by \citet{MuellerGermanic} for \ili{Germanic} in general.
\is{head movement|)}

\subsection{Constructional approaches}
\label{sec-aux-inversion-phrasal}

%% \inlinetodostefan{When you talk about ``head-movement'', you should mention that it is the analysis assumed by GB for
%% adverb placement \citep{Pollock89a-u,Cinque99a-u}, and not the analysis adopted in HPSG \citep{AG97a-u,AG2004a-u,KS2002a}: in HPSG, a possible analysis is to consider certain adjuncts as complements. I
%% guess this should go in part 5.2.
%% }

%\largerpage
The alternative to head-movement-based approaches is a flat analysis with an alternative
serialization of the verb. This was already discussed with respect to \ili{German}, but I want to discuss
\ili{English} auxiliary constructions here, since they have figured prominently in linguistic
discussions.\footnote{%
  For a discussion including \ili{French} verb placement see \citew{AG97a-u} and \citew{KS2002a}.
}
In the analysis of (\mex{1}) shown in Figure~\ref{fig-did-kim-get-the-job}, the auxiliary \emph{did}
selects for the subject \emph{Kim} and a VP \emph{get the job}.
\ea
Did Kim get the job?
\z
\begin{figure}
\begin{forest}
sm edges
[S
  [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]} [did]]
  [\ibox{1} NP [Kim]]
  [\ibox{2} VP [get the job, roof]]]
\end{forest}
\caption{\label{fig-did-kim-get-the-job}Analysis of English auxiliary constructions based on
  \citet[\page 117]{Sag2020a}}
\end{figure}
The tree in Figure~\ref{fig-did-kim-get-the-job} is licensed by a schema combining a head with its
subject \iboxb{1} and its VP complement \iboxb{2} in one go.\footnote{
  An alternative is to assume a separate valence feature for the subject (\attrib{subj}) and a
  schema that combines the head with the element in the \subjl and the elements in the \compsl
  \citep[\page 36]{GSag2000a-u}.%
} As has been common in HPSG since the mid-1990s
\citep{Sag97a}, phrasal schemata are organized 
in type hierarchies and the general schema for auxiliary-initial constructions has the type
\type{aux-initial-cxt}. \citet{Fillmore99a} and \citet{Sag2020a} argue that there are various usages
of auxiliary-initial constructions and assign the respective usages to subconstructions of the
general auxiliary-initial construction. Technically this amounts to stating subtypes of
\type{aux-initial-cxt}. For example, \citet[\page 116]{Sag2020a} posit a subtype \type{polar-int-cl} for polar
interrogatives like (\mex{1}a) and another subtype \type{aux-initial-excl-cl} for exclamatives like (\mex{1}b).
\eal
\ex Are they crazy?
\ex Are they crazy!
\zl
\largerpage[2]
\citet{Chomsky2010a} compared the various clause types used in HPSG with the -- according to him --
much simpler Merge-based analysis in Minimalism. Minimalism assumes just one very general schema for
combination (External Merge is basically equivalent to our Head-Complement Schema (\ref{hcs-binary}) above, see
\citew[\page 937--939]{MuellerUnifying}), so this rule for combining linguistic objects is very simple, but this
does not help in any way when considering the facts: there are at least five different meanings
associated with auxiliary initial clauses (polar interrogative, blesses/curses, negative imperative,
exclamatives, conditionals) and these have to be captured somewhere in a grammar. One
way is to state them in a type hierarchy as is done in some HPSG analyses and in \sbcg, another way
is to use implicational constraints that assign various meanings to actual configurations
(see Section~\ref{sec-mixed-approaches}), and a third way is to do everything lexically. The only option for
Minimalism is the lexical one. This means that Minimalism has to either assume as many lexical items
for auxiliaries as there are types in HPSG or to assume empty heads that contribute the meaning that
is contributed by the phrasal schemata in HPSG (\citealp[Section~5]{Borsley2006a}; \crossrefchapteralp[Section~\ref{minimalism-sec-empty-elements-for-relative-clauses}]{minimalism}). 
% Bob: I think they would assume a variety of different C-elements attracting the auxiliary: an
% interrogative C, an exclamative C, etc.
% Stefan: Adger has different C heads.
The latter proposal is generally assumed in
Cartographic approaches \citep{Rizzi97a-u}. Since there is a fixed configuration of functional projections
that contribute semantics, one could term these Rizzi-style analyses \emph{Crypto-Constructional}.
%\itd{JP: This is too charitable. One would have to assume Aux/verb initial meanings are the same for ALL languages or you need "features" pairing these empty heads (with particular meanings) with movement where it occurs.}

Having discussed a lexical approach involving an empty element and a phrasal approach that can
account for the various meanings of auxiliary inversion constructions, I turn now to a mixed
approach in the next section and show how the various meanings associated with certain patterns can
be integrated into accounts with rather abstract schemata for combinations like the one described in
Section~\ref{sec-head-movement}.

\subsection{Mixed approaches}
\label{sec-mixed-approaches}

The situation with respect to clause types is similar in \ili{German}. Verb first sentences can be
yes/no questions (\mex{1}a), imperatives (\mex{1}b), conditional clauses (\mex{1}c), and declarative
sentences with \isi{topic drop} (\mex{1}d). 
\eal
\ex\label{ex-kommt-peter-question}
\gll Kommt Peter?\\
     comes Peter\\\german
\glt `Is Peter coming?'
\ex 
\gll Komm!\\
     come\\
\ex\label{ex-kommt-peter-conditional}
\gll Kommt Peter, komme ich nicht.\\
     comes Peter  come  I not\\
\glt `If Peter comes, I won't come.'
\ex 
\gll Kommt Peter. (Was ist morgen?)\\
     comes Peter  \hphantom{(}what is tomorrow\\
\glt `What happens tomorrow?' `Peter is coming.'
\zl
(\mex{0}a), (\mex{0}c) and (\mex{0}d) contain the same words but differ in intonation.

Verb second sentences can be \emph{w}"=questions (\mex{1}a), declarative sentences (\mex{1}b), or imperatives (\mex{1}c).
\eal
\ex 
\gll Wer kommt?\\
     who comes\\\german
\ex 
\gll Peter kommt.\\
     Peter comes\\
\ex 
\gll Jetzt komm!\\
     now   come\\
\glt `Come now!'
\zl

%\largerpage
\noindent
While one could try and capture this situation by assuming surface order-related clause types, such approaches are rarely
used in HPSG (but see \citew{Kathol2001a} and \citew{Wetta2011a}, and see Section~\ref{sec-surface-order} on why such approaches
are doomed to failure). Rather, researchers assumed binary branching head-complement structures
together with verb movement (for references see the end of
Section~\ref{sec-head-movement}).\footnote{%
I assumed linearization domains (see Section~\ref{sec-domains}) for ten
years and then switched to the head-movement approach
\citep{Mueller2005c,Mueller2005d,MuellerGS}. For a detailed discussion of all alternative proposals
and a fully worked out analysis see \citew{MuellerGS}.%
} 

As was explained in Section~\ref{sec-head-movement}, the head movement approaches are based on lexical rules or unary
projections. These license new linguistic objects that could contribute the respective semantics. In
analogy to what \citet{Borsley2006a} has discussed with respect to extraction structures, this would mean that one needs seven versions of fronted verbs to
handle the seven cases in (\mex{-1}) and (\mex{0}), which would correspond to the seven phrasal types
that would have to be stipulated in phrasal approaches. But there is a way out of this: one can
assume one lexical item with underspecified semantics. HPSG makes it possible to use implicational
constraints referring to a structure in which an item occurs. Depending on the context, the semantics
contributed by a specific item can be further specified. Figure~\ref{abb-konstruktion-implikation}
shows the construction-based and lexical-rule"=based analyses in the abstract for comparison.
%\largerpage
\begin{figure}
\hfill
\begin{subfigure}{.4\textwidth}
\centering
\begin{forest}
[{{\sc sem} f(x) (y)}
   [{{\sc sem} y}]
   [{{\sc sem} x} [\vphantom{\textsc{sem}},no edge]]]
\end{forest}
\caption{Phrasal construction}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
\centering
\begin{forest}
[{{\sc sem} f(x) (y)}
  [{{\sc sem} y}  ]
  [{{\sc sem} f(x)} [{{\sc sem} x}] ]]
\end{forest}
\caption{Unary construction and implication}
\end{subfigure}\hfill\mbox{}
\caption{\label{abb-konstruktion-implikation}Construction-based, phrasal approach and approach with
  implicational constraint}
\end{figure}
In the construction-based analysis, the daughters contribute x and y as semantic values and the whole
construction adds the construction meaning $f$. In the lexical-rule- or unary-projection"=based analysis, the lexical
rule/unary projection adds the $f$ and the output of the rule is combined with the other
daughter without any contribution by a specialized phrasal construction. Now, implicational constraints can be used to determine the exact contribution of the
lexical item \citep{MuellerSatztypen}. This is shown with the example of a question in Figure~\ref{abb-imp-interrogativ}.
% put the following sentence here for layout reasons
The implication says: when the configuration has the form that there is a question pronoun in the
\begin{figure}
\centerline{%
\begin{forest}
[{}
       [{{\sc que} \sliste{ [ ] }}]  
       [{\vphantom{t}}]
       ]
\end{forest}\hspace{1em}\raisebox{\baselineskip}{\impl}\hspace{1em}
\begin{forest}
[{}
  [{\vphantom{t}}]  
  [{int(x)}]
       ]
\end{forest}
}
\caption{\label{abb-imp-interrogativ}Implication for interrogative sentences}
\end{figure}
%The implication says: when the configuration has the form that there is a question pronoun in the
left daughter, the projection resulting from the combination of the output of the lexical rule with
the VP selected by the initial verb gets question semantics. Since HPSG represents all
linguistic information in the same attribute value matrix (AVM), such implicational constraints can refer to intonation as
well and hence, implications for establishing the right semantics for V1 questions (\ref{ex-kommt-peter-question}) vs.\ V1
conditionals (\ref{ex-kommt-peter-conditional}) can be formulated.\footnote{
  Note that coordination examples like (i) do not pose a problem:
\ea
\gll Kim [kennt und liest] das Buch.\\
     Kim \spacebr{}knows and reads the book\\
\glt `Kim knows and reads the book.'
\z
The unary schema applies to the conjunction of the two verbs. However, the situation is different
for examples like (ii):
\ea
\gll Kim [kennt$_i$      [die Schallplatte \trace$_i$]] und [liest$_j$ [das Buch \trace$_j$]].\\
     Kim \spacebr{}knows \spacebr{}the record {} and \spacebr{}reads \spacebr{}the book\\
\glt `Kim knows the record and reads the book.'
\z
The selection of the verbless verb phrase takes place in the conjuncts, but the semantics of the
clause is determined at the top-most level when \emph{Kim} is combined with the coordinated
structure. It has to be made sure that information about the syntactic combination of verb-initial
verb, about morphological information (imperative vs.\ indicative) and intonation is available at
the coordinated structure. This information will be affected by the implicational constraint and is
inserted at a place where it scopes over the coordination relation. 
%
% Note also that the lexical item \emph{kennt} `knows' in (ii) selects a verb-final clause. It can
% access the \slashv of this verb-final clause and hence can determine whether something is extracted
% or not. This way it can be determined lexically whether the verb is within a V2 or a V1
% clause. However, due to the feature geometry assumed in HPSG, the verb cannot see whether the
% fronted element contains an interrogative pronoun or not. So unless the feature geometry is changed,
% the difference between (ii) and (iii) in clause type has to be determined at the top-most level of
% the clause. 
% \ea
% \gll Wer [kennt die Schallplatte] und [liest das Buch].\\
%      who \spacebr{}knows the record and \spacebr{}reads the book\\
% \glt `Who knows the record and reads the book.'
% \z

An alternative to the underspecification + implicational constraints account would be to add the semantics contributed by clause types via a unary rule applying
to the complete clause as in \citew[\page 266--267]{GSag2000a-u}.
}
%% This is true: for one level deep constraints you can do it in constructions.
%%
%% Note that in Constructional HPSG as layed out by \citew{Sag97a} implicational constraints can refer
%% to the structure of a complete utterance. Hence items with a complex internal structure can be seen
%% as contributing a certain meaning. This is ruled out by design in Sign"=Based Construction Grammar,
%% where linguistic objects of type \type{phrase} do not have daughters.
%% \inlinetodostefan{%
%% Bob: Can't you do everything with constructions in SBCG that you could do with phrases in standard
%% HPSG? I think you need to say more if you want to make this point.
%% }


\section{Constituent order domains and linearization}
\label{sec-domains}

There\is{order domain|(} is an interesting extension to standard HPSG that opens up possibilities
for analyses that are quite different from what is usually done in theoretical linguistics: Mike
Reape \citeyearpar{Reape91,Reape92a,Reape94a} working on \ili{German} suggested formal tools that
allow for the modeling of discontinuous\is{discontinuous constituent|(} constituents.\footnote{ See also \citew[\page
  105--106]{Wells47a}, \citew{Dowty90a-Eng}, and \citew{Blevins94a-u} for proposals assuming
  discontinuous constituents in other frameworks.% 
} His original motivation was to account for \isi{scrambling} of arguments of verbs forming
verbal complexes, but this analysis was superseded by Hinrichs and Nakazawa's analysis
\citep{HN89a,HN94a} since purely linearization-based approaches are unable to account for \isi{agreement}
and the so-called remote passive\is{passive!remote} (\citealp[Section~5.1, Section~5.2]{Kathol98b};
\citealp[Chapter~21.1]{Mueller99a}). Nevertheless, Reape's work was taken up by others and was used
for analyzing \ili{German}
\citep{KP95a,Kathol2000a,Mueller95c,Babel,Mueller2004b,Wetta2011a,Wetta2014a-u}. As will be
discussed below in Section~\ref{sec-problems-dom}, there are reasons for abandoning 
linearization"=based analyses of \ili{German} that assume discontinuous constituents
\parencites{Mueller2005d}[Chapter~6]{MuellerGS} but constituent order domains still play a
role in analyzing ellipsis\is{ellipsis} \crossrefchapterp[\pageref{ellipsis:ex-beavers-coordination-with-dom}]{ellipsis} and
coordination\is{coordination} (\citealt{Yatabe2001,Crysmann2003a,BS2004a,YT2021a-u}; \crossrefchapteralt[\page
\pageref{page-linearization-domains-in-coordination-one},
\pageref{page-linearization-domains-in-coordination-two}]{coordination}).  \citet*{BGM99a} show that
complex predicate formation does not account for subject-verb inversion in \ili{French} and suggest a
domain-based approach. \citet{BG2007b-u}, also working on \ili{French}, propose an analysis of sentential adverbs within
a domain-based approach.

\subsection{A special representational layer for constituent order}
\label{order:sec-domains}

The technique that is used to model discontinuous constituents in frameworks like HPSG goes back to
Mike Reape's work on \ili{German} \citeyearpar{Reape91,Reape92a,Reape94a}. Reape uses a list called
\textsc{domain}\isfeat{domain} to represent the daughters of a sign in the order in which they
are pronounced or written. (\mex{1}) shows an example in which the \domv of a 
headed-phrase is computed from the \domv of the head and the list of non-head daughters.
\ea
\label{ex-shuffeling-daughters}
\type{headed"=phrase}\istype{headed-phrase} \impl
\avm{
[ head-dtr|dom  & \1 \\
  non-head-dtrs & \2 \\
  dom  & \1 $\bigcirc$ \2 ]
}
\z
The symbol `$\bigcirc$'\is{$\bigcirc$}\isrel{shuffle}\label{rel-shuffle}\label{order:rel-shuffle}
stands for the \emph{shuffle} relation. \emph{shuffle} relates three lists A, B and C iff C
contains all elements from A and B and the order of the elements in A and the order of the elements
of B is preserved in C. (\mex{1}) shows the combination of two lists with two elements each:

\ea
\label{ex-explanation-shuffle}
$\phonliste{ a, b } \bigcirc \phonliste{ c, d } =
\begin{tabular}[t]{@{}l}
\phonliste{ a, b, c, d } $\vee$\\*[1mm]
\phonliste{ a, c, b, d } $\vee$\\*[1mm]
\phonliste{ a, c, d, b } $\vee$\\*[1mm]
\phonliste{ c, a, b, d } $\vee$\\*[1mm]
\phonliste{ c, a, d, b } $\vee$\\*[1mm]
\phonliste{ c, d, a, b }
\end{tabular}$
\z
The result is a disjunction of six lists. \emph{a} is ordered before \emph{b} and \emph{c} before
\emph{d} in all of these lists, since this is also the case in the two lists \phonliste{ a, b } and
\phonliste{ c, d } that have been combined. But apart from this, \emph{a} and \emph{b} can be placed
before, between, or after \emph{c} and \emph{d}. 

On the linearization-based approach, every word comes with a domain value that is a list that contains the
word itself:
\ea
Domain contribution of single words, here \emph{gibt} `gives':\\*
\avm{
\1 [ phon   & \phonliste{ gibt }\\
     synsem & \ldots\\
     dom    & < \1 > ]
}
\z
The description in (\mex{0}) may seem strange at first glance, since it is cyclic\is{cyclic feature description}, but it can be understood as a statement saying that \emph{gibt} contributes
itself to the items that occur in linearization domains. 

The constraint in (\mex{1}) is responsible for the determination of the \phonvs of phrases:
\ea
\type{phrase} \impl
\avm{
[ phon & \1 \+ \ldots{} \+ \tag{n} \\ \\
  dom  & < [%\type*{sign}
            phon & \1 ], \ldots, [%\type*{sign}
                                   phon & \tag{n} ] > ]
}
\z
It states that the \phonv of a sign is the concatenation of the \phonvs of its \textsc{domain}
elements. Since the order of the \textsc{domain} elements corresponds to their surface order, this is
the obvious way to determine the \phonv of the whole linguistic object. 

%\largerpage
Figure~\ref{fig-the-child-reads-the-book-reape-binary} shows how this machinery can be used to license binary
branching structures with discontinuous constituents in the sentence
\emph{dass dem Kind ein Mann das Buch gibt} `that a man gives the
child the book'.
\begin{figure}
\centerfit{%
\begin{forest}
sm edges
[{V[\dom \phonliste{ dem Kind, ein Mann, das Buch, gibt }]}
  [{NP[\type{nom}, \dom \phonliste{ ein, Mann }]} [ein Mann;a man,roof]]
  [{V[\dom \phonliste{ dem Kind, das Buch, gibt }]}
   [{NP[\type{dat}, \dom \phonliste{ dem, Kind  }]} [dem Kind;the child,roof] ]
   [{V[\dom \phonliste{ das Buch, gibt }]}
    [{~~~NP[\type{acc}, \dom \phonliste{ das, Buch }]} [das Buch;the book,roof] ]
    [{V[\dom \phonliste{ gibt }]} [gibt;gives] ] ] ] ]
\end{forest}
}
\caption{\label{fig-the-child-reads-the-book-reape-binary}Analysis of \emph{dass dem Kind ein Mann das Buch
    gibt} `that a man gives the child the book' with binary branching structures and discontinuous
  constituents. The tree shows the order of combination, which does not correspond to the
  linearization of the \domain objects.}
\end{figure}%%
Words or word sequences that are separated by commas stand for separate domain objects, that is,
\phonliste{ das, Buch } contains the two objects \emph{das} and \emph{Buch} and \phonliste{ das
  Buch, gibt } contains the two objects \emph{das Buch} and \emph{gibt}.
The important point to note here is that the
arguments in the tree are combined with the head in the order
accusative, dative, nominative, although the elements in the
constituent order domain (i.e.\ in the list of \domain elements and in
the surface sentence) are realized in the order dative, nominative,
accusative, rather than nominative, dative, accusative, which is what
one might expect based on the order in which they are combined in the
tree. This is possible since the formulation of the computation of the \domv using the shuffle
operator allows for discontinuous constituents. The node for \emph{dem Kind das Buch gibt} `the
child the book gives' is discontinuous: \emph{ein Mann} `a man' is inserted into the domain between
\emph{dem Kind} `the child' and \emph{das Buch} `the book'.  This is more obvious in Figure~\ref{fig-the-child-reads-the-book-reape-binary-discont}, which has a serialization of NPs that
corresponds to their order.
\begin{figure}
\centerfit{%
\begin{forest}
sm edges
[{V[\dom \phonliste{ dem Kind, ein Mann, das Buch, gibt }]}
  [{NP[\type{dat}, \dom \phonliste{ dem, Kind  }]~~~~~~}, no edge, name=np-dat,tier=dat-tier, [dem Kind;the child,roof] ]
  [{NP[\type{nom}, \dom \phonliste{ ein, Mann }]} [ein Mann;a man, roof]]
  [{V[\dom \phonliste{ dem Kind, das Buch, gibt }]}, name=v
   [NP, phantom, tier=dat-tier ]
   [{V[\dom \phonliste{ das Buch, gibt }]}
    [{NP[\type{acc}, \dom \phonliste{ das, Buch }]} [das Buch;the book,roof] ]
    [{V[\dom \phonliste{ gibt }]} [gibt;gives] ] ] ] ]
\draw (v.south) -- (np-dat.north);
\end{forest}
}
\caption{\label{fig-the-child-reads-the-book-reape-binary-discont}Analysis of \emph{dass dem Kind ein Mann das Buch
    gibt} `that a man gives the child the book' with binary branching structures and discontinuous
  constituents, more clearly showing the discontinuity}
\end{figure}% 


\subsection{Absolutely free}
\label{sec-absolutely-free}\label{sec-warlpiri}

\is{scrambling|(}
While \ili{German} is more striking than \ili{English} in terms of constituent order, languages like
\ili{Warlpiri}\il{Warlpiri|(} are even more so, since they have much freer constituent order. In
\ili{Warlpiri} the auxiliary has to be in first or in second position \parencites[\page
322]{Laughren89a-u}[\page 69, 99]{Simpson91a-u}, %\citep[\page 8]{DS99a}, 
but apart from this, even parts of what are noun
phrases in \ili{German} and \ili{English} can appear separated from each other. For example, the two parts of
the NP \emph{Kurdujarrarlu witajarrarlu} `child small' may appear discontinuously since they are marked with
the same case \citep[\page 257]{Simpson91a-u}:
\ea
\label{ex-warlpiri}
\gll Kurdu-jarra-rlu  ka-pala                 maliki     wajili.pi-nyi        wita-jarra-rlu.\\
% S91:257 
     child-\DU-\ERG{} \PRS-3\DU.\textsc{sbj} dog.\ABS{} chase-\textsc{npst} small-\DU-\ERG\\%\jambox{(\ili{Warlpiri})}
\glt `Two small children are chasing the dog.' or\\
     `Two children are chasing the dog and they are small.'
\z
% Doug Ball:  
%% -- I really got into exploring the \ili{Warlpiri} example in (21) [I wonder where this example originally appeared -- probably either in Jane Simpson's 1983 dissertation or in work by Ken Hale], and here's what I found: It appears that others (like Mary Dalrymple in her 2001 book on LFG) have given the verb of this sentence as wajilipi-nyi 'chase-NPST', treating the -pi- as part of the stem. In my own investigations, including looking at the Online \ili{Warlpiri} Dictionary (http://ausil.org/Dictionary/\ili{Warlpiri}/lexicon/index.htm), it appears that this word is a kind of compound: wajili meaning something like 'fast' or 'quickly' and pi- being a root associated with a light verb of contact, which one might be inclined to translate as 'hit' or 'hunt'. The suffix -nyi is clearly the NONPAST ending (for this kind of verb). So either wajilipi-nyi 'chase-NPST' or wajili-pi-nyi 'fast-hunt-NPST' would work, but I don't think ...pinyi is a recognized \ili{Warlpiri} morphological unit. 
%
% Simpson, 1991 calls the pi VR verb root.
\citet{DS99a} develop an analysis for this that simply liberates domain elements and inserts them
into the next higher domain. (\mex{1}) shows how this is formalized:
\ea
\type{liberating-phrase} \impl\\*
\avm{
[ dom      & $\delta_0 \bigcirc \delta_1 \bigcirc \ldots \bigcirc \delta_n$\\
  head-dtr & [ dom &  $\delta_0$ ]\\
  non-head-dtrs & <  [ dom &  $\delta_1$ ], $\ldots$, [ dom &  $\delta_n$ ] > ] 
}
\z
Rather than inserting the entire daughters into the domain of the mother as in
(\ref{ex-shuffeling-daughters}), the \domvs of the daughters are shuffled into the domain of the
mothers. So instead of having the NPs in the same domain as the verb as in the \ili{German} example in the
previous section, one has all the parts of NPs in the next higher domain. Hence, a single nominal element
being placed in front of the auxiliary in second position is explained without
difficulty. Figure~\ref{fig-warlpiri} shows \citegen{DS99a} analysis of a version of
(\ref{ex-warlpiri}) with the VP constituents \emph{maliki wajilipinyi} `dog chase' serialized after \emph{witajarrarlu} `small'.
\begin{figure}
\oneline{%
\begin{forest}
sm edges
[IP\\\avm{ [ phon \phonliste{ kurdu-jarra-rlu, ka-pala, wita-jarra-rlu, \ldots{} }\\
                dom < \1\,[ phon \phonliste{ kurdu-jarra-rlu } ],
                      \2\,[ phon \phonliste{ ka-pala } ],
                      \3\,[ phon \phonliste{ wita-jarra-rlu } ], \ldots{} > ]
      }
     [NP\\  \avm{ [ dom & < \1, \3 > ]} [ kurdu-jarra-rlu wita-jarra-rlu;{child-\textsc{du}-\textsc{erg} small-\textsc{du}-\textsc{erg}}, roof]]
     [Aux\\ \avm{ [ dom & < \2 > ]} [ka-pala;{\textsc{prs}-3\textsc{du}.\textsc{subj}}]]
     [VP\\  \avm{ [ dom & < \ldots{} > ]} [maliki wajili-pi-nyi;{dog.\textsc{abs} chase-\textsc{npast}},roof]]]
\end{forest}}
\caption{\label{fig-warlpiri}Analysis of free constituent order in Warlpiri according to
  \citet[\page 9]{DS99a}}
\end{figure}
Here \emph{kurdujarrarlu} `child' and \emph{witajarrarlu} `small' form an NP. They contribute two independent domain objects (\,\ibox{1}
and \ibox{3}\,) to the domain of the mother. The second element in this domain has to be the auxiliary
\iboxb{2}, \ibox{1} is realized initially and \ibox{3} follows the auxiliary.\il{Warlpiri|)}
\is{scrambling|)}

We have seen so far an analysis that inserts complete objects into the domain of the mother (the
analysis of \ili{German}) and an analysis that inserts all domain objects of objects into the domain of the
mother (the analysis of \ili{Warlpiri}). In the next subsection I look at an intermediate case,
so-called \emph{partial compaction}.
%\inlinetodostefan{Bob: explain compaction. You also don’t explain liberation, which is probably necessary.}

\subsection{Partial compaction (extraposition)}
\label{sec-partial-compaction}

\citet{KP95a}\is{extraposition|(} develop an analysis of extraposition that is a mix of the strategies discussed in
the two previous subsections: most of one NP object is inserted into the domain of the mother as a single object, while only those
parts that are extraposed are liberated and inserted as individual domain objects into the domain of
the mother.\footnote{
  This analysis of extraposition is not the only option available in HPSG. I explain it here since
  it shows the flexibility of the domain approach. The more common analysis of extraposition is one
  that is parallel to the \slasch-based approach to extraction that is explained in
  \crossrefchaptert{udc}. Since constraints regarding locality differ for fronting to the left and
  extraposition to the right, a different feature is used (\attrib{extra}). See \citew{Keller95b} and
  \citew[Section~13.2]{Mueller99a} for discussion. More recent approaches assume the
  projection of semantic indices \citep{Kiss2005a} to be able to solve puzzles like \citegen{Link84a-u} hydra sentences and even more
  recent proposals mix index projection and \extra projection \citep{Crysmann2013a}.
}
\citeauthor{KP95a}'s analysis of (\mex{1}) is given in
Figure~\ref{fig-extraposition-via-domain-union}.\footnote{
  The figure is taken over from \citeauthor{KP95a}. Words in italics are the object language. Part of
  speech or category labels are provided at the top of AVMs.%
}
\ea
\label{ex-einen-hund-fuettern-der-hunger-hat}
\gll  einen Hund füttern, der Hunger hat\\
      a dog feed that hunger has\\\german
\glt `feed a dog that is hungry'
\z
\begin{figure}
\begin{forest}
[\avm{ 
  [ VP\\
    dom \5 < \2 [ NP\\
                  \normalfont\textit{einen Hund} ],  
                [ V\\
                  \normalfont\textit{füttern} ], 
             \3 [ REL-S\\
                  extra !$+$!\\
                  \normalfont\textit{der Hunger hat} ] > ] }
  [\avm{ 
    \1 [ NP\\
        dom < [ DET\\
                \normalfont\textit{einen} ], 
              [ N\\
                \normalfont\textit{Hund} ], 
              \3 [ REL-S\\
                   extra !$+$!\\
                   \normalfont\textit{der Hunger hat} ] > ]}
    [\avm{ 
        [ DET\\
          \normalfont\textit{einen} ]} ]
    [\avm{
        [ \nbar\\
          dom < [ N\\
                  \normalfont\textit{Hund} ], \3 [ REL-S\\
                                           extra !$+$!\\
                                           \normalfont\textit{der Hunger hat} ] > ] }  
      [\avm{ 
          [ N\\
            \normalfont\textit{Hund} ] } ] 
      [\avm{ 
          [ REL-S\\
            extra !$+$!\\
            \normalfont\textit{der Hunger hat} ]} ]]]
  [\avm{ 
      [ V\\
        dom \4 < [ V\\
                   \normalfont\textit{füttern} ] > ] } ]]
\end{forest}\mbox{}\\
\hspace{1.2cm}p-compaction(\ibox{1}, \ibox{2}, \sliste{ \ibox{3} })\hfill\mbox{}\\
\hspace{1.2cm}\ibox{5} = \sliste{ \ibox{2} } $\bigcirc$ \sliste{ \ibox{3} } $\bigcirc$ \ibox{4}\hfill\mbox{}
\caption{\label{fig-extraposition-via-domain-union}Analysis of extraposition via partial compaction
  of domain objects according to \citet[\page 178]{KP95a}}
\end{figure}
\emph{einen Hund, der Hunger hat} `a dog who is hungry' consists of three domain objects:
\emph{einen} `a', \emph{Hund} `dog', and \emph{der Hunger hat} `who is hungry'. The two initial ones
are inserted as one object (the NP \emph{ein Hund} `a dog') into the higher domain
and the relative clause is liberated. While the formation of the new domain at the mother node is
relatively straightforward in the cases discussed so far, a complex relational constraint is needed
to split the relative clause \iboxb{3} from the other domain objects and construct a new domain
object that has the determiner and the noun as constituents \iboxb{2}. \citeauthor{KP95a} have a relational constraint called
\rel{compaction}\isrel{compaction} that builds new domain objects for insertion into higher
domains. \rel{partial compaction}\isrel{partial compaction} takes an initial part of a domain and forms a new domain object
from this, returning the remaining domain objects for separate insertion into the higher domain. Due
to space limitations, this constraint will not be discussed here, but see \citew[\page
  244]{Mueller99a} for a refined version of \citeauthor{KP95a}'s constraint. The effect of partial
compaction in Figure~\ref{fig-extraposition-via-domain-union} is that there is a new object \ibox{2}
and a list containing the remaining objects, in the example \sliste{ \ibox{3} }. A list containing
the new object \sliste{ \ibox{2} } and the list containing the remaining objects \sliste{ \ibox{3} } are
shuffled with the domain list of the head \ibox{4}. Since the relative clause is now in the same domain
as the verb, it can be serialized to the right of the verb.

This subsection showed how examples like (\ref{ex-einen-hund-fuettern-der-hunger-hat}) can be
analyzed by allowing for a discontinuous constituent consisting of an NP and a relative
clause. Rather than liberating all daughters and inserting them into the domain of the mother node
as in the \ili{Warlpiri} example, determiner and noun form a new object, an NP, and the newly created NP
and the relative clause are inserted into the domain of the mother node. This explains why
determiner and noun have to stay together while the relative clause may be serialized further to the right.%
\is{discontinuous constituent|)}\is{extraposition|)}

%% \inlinetodostefan{
%% Bob: I think there should probably be some sort of reference to other approaches to extraposition of phenomena of various kinds, e.g. Kay and Sag (2012) (Kay, P. \& I. A. Sag (2012), Cleaning up the big mess: Discontinuous dependencies and complex determiners, in Boas and Sag (eds.), Sign-Based Construction Grammar, Stanford: CSLI Publications, 229-256.).}

\subsection{Problems with order domains}
\label{sec-problems-dom}

Constituent order domains may seem rather straightforward since linearization facts can be handled
easily. I assumed constituent order domains and discontinuous constituents for \ili{German} myself for
over a decade \citep{Mueller95c,Mueller2004b}. However, there are some problems that seem to suggest
that a traditional GB-like head-movement approach is the better alternative. In what follows I want
to discuss just two problematic aspects of linearization approaches: spurious ambiguities and
apparently multiple frontings.

\subsubsection{Partial fronting and spurious ambiguities}

\citet{Kathol2000a} suggests an analysis of German clause structure with binary branching structures
in which all arguments are inserted into a linearization domain and can be serialized there in any
order, provided no LP rule is violated. Normally one would have the elements of the \compsl in a
fixed order, combine the head with one element from the \compsl after another, and let the freedom
in the \doml be responsible for the various attested orders. So, both sentences in (\mex{1}) would
have analyses in which the verb \emph{erzählt} `tells' is combined with \emph{Geschichten} `stories'
first and then \emph{Geschichten erzählt} `stories tells' is combined with \emph{den Wählern} `the
voters'. Since the verb and all its arguments are in the same linearization domain they can be
ordered in any way, including the two possibilities in (\mex{1}):

\eal
\label{ex-waehlern-geschichten-erzaehlt}
\ex 
\gll weil er den Wählern Geschichten erzählt\\
     because he the voters stories tells\\\german
\glt `because he tells the voters stories'
\ex 
\gll weil er Geschichten den Wählern erzählt\\
     because he stories the voters tells\\
\zl
The problem with this approach is that examples like (\mex{1}) show that grammars have to account
for fronted combinations of the verb and any of its objects to the exclusion of the other:
\eal
\label{ex-geschichten-erzaehlen}
\ex 
\gll Geschichten erzählen sollte man den Wählern nicht.\\
     stories     tell     should one the voters not\\\german
\glt `One should not tell the voters such stories.'
\ex 
\gll Den Wählern erzählen sollte man diese Geschichten nicht.\\
     the voters  tell     should one these stories not\\
\zl
\citet[Section~8.9]{Kathol2000a} accounts for examples like (\mex{0}) by relaxing the order of the
objects in the valence list. He uses the shuffle operator $\bigcirc$, which was explained in
(\ref{ex-explanation-shuffle}) above,  in the valence representation:  
\ea
\sliste{ NP[\type{nom}] } $\oplus$ (\sliste{ NP[\type{dat}] } $\bigcirc$ \sliste{ NP[\type{acc}] })
\z
This solves the problem with examples like (\mex{-1}) but it introduces a new one: sentences like
(\ref{ex-waehlern-geschichten-erzaehlt}) now have two analyses each. One is the analysis we had
before and another one is the one in which \emph{den Wählern} `the voters' is combined with
\emph{erzählt} `tells' first and the result is then combined with \emph{Geschichten}
`stories'. Since both objects are inserted into the same linearization domain, both orders can be
derived. So we have too much freedom: freedom in linearization and freedom in the order of
combination. The proposal that I suggested in Müller (\citeyear[Section~2.1]{Mueller2005c};
\citeyear[Section~2.2.1]{MuellerGS}) and which is implemented in the schema in (\ref{hcs-binary}) above has
just the freedom in the order of combination and hence can account for both
(\ref{ex-waehlern-geschichten-erzaehlt}) and (\ref{ex-geschichten-erzaehlen}) without spurious
ambiguities.


\subsubsection{Surface order, clause types, fields within fields, and empty elements}
\label{sec-surface-order}

\is{word order!V2|(}
\citet{Kathol2001a} develops an analysis of \ili{German} that uses constituent order domains and determines the
clause types on the basis of the order of elements in such domains. He suggests the topological
fields \type{1}, \type{2}, \type{3}, and \type{4}, which correspond to the traditional
\isi{topological field}s \emph{Vorfeld} `prefield', \emph{linke Satzklammer} `left sentence
bracket', \emph{Mittelfeld} `middle field', \emph{rechte Satzklammer} `right sentence
bracket'. Domain objects may be assigned to these fields, and they are then ordered by linearization
constraints stating that objects assigned to \type{1} have to precede objects of type \type{2},
type \type{3}, and type \type{4}. Objects of type \type{2} have to precede type \type{3}, and type
\type{4} and so on. For the \vf and the left sentence bracket, he stipulates uniqueness constraints saying
that at most one constituent may be of this type. This can be stated in a nice way by using
the linearization constraints in (\mex{1}):
\eal
\ex \type{1} < \type{1}
\ex \type{2} < \type{2}
\zl
This trick was first suggested by \citet[\page 55, Fn.\,3]{GKPS85a} in the framework of GPSG\indexgpsg and it works
because, if there were two objects of type \type{1}, then each one would be required to precede the
other one, resulting in a violation of the linearization constraint. So in order to avoid such
constraint violation there must not be more than one \type{1}.

\citet[\page 58]{Kathol2001a} assumes the following definition for V2 clauses:
\ea
\type{V2-clause} \impl
\avm{
[ !S[\type{fin}]!\\
  dom & < ![1]!, [ 2\\!V[\type{fin}]! ], \ldots > ]
}
\z
This says that the constituent order domain starts with one element assigned to field \type{1},
followed by another domain object assigned to field \type{2}. While this is in accordance with
general wisdom about \ili{German}, which is a V2 language, there are problems for entirely surface-based
theories: \ili{German} allows for multiple constituents in front of the finite verb. (\mex{1}) shows some
examples:
\eal
\label{ex-mult-front}
\ex
\longexampleandlanguage{
\gll {}[Zum           zweiten Mal] [die Weltmeisterschaft] errang Clark 1965 \ldots\footnotemark\\
     \spacebr{}to.the second  time \spacebr{}the.\ACC{} world.championship won Clark.\NOM{} 1965 {}\\}{German}
\footnotetext{
Der deutsche Straßenverkehr, 1968, Heft 6, p.\,210, quoted after \citet*[\page
224]{Neumann69a-u}. See also \citew[\page 162]{Benes71}.
}\label{bsp-zum-zweiten-mal-die-Weltmeisterschaft}
\glt `Clark won the world championship for the second time in 1965.'
\ex
\label{ex-dem-saft-eine-kraeftige-farbe}
\gll [Dem Saft] [eine kräftige Farbe] geben Blutorangen.\footnotemark\\
     \spacebr{}the.\DAT{} juice \spacebr{}a.\ACC{}   strong   color give blood.oranges\\
\footnotetext{
\citet[\page 69]{BC2010a} found this example in the \emph{Deutsches Referenzkorpus} (DeReKo), hosted at Institut
für Deutsche Sprache, Mannheim: \url{http://www.ids-mannheim.de/kl/projekte/korpora}, 2021-03-21.
}
\glt `Blood oranges give the juice a strong color.'

\zl 
\citet{Mueller2003b} extensively documents this phenomenon. The categories that can appear
before the finite verb are almost unrestricted. Even subjects can be fronted together with other
material (\citealp[\page 72]{BC2010a}; \citealp[\page 371]{Bildhauer2011a}). The empirical side of
these apparent multiple frontings was further examined in the Collective Research Center 632,
Project A6, and the claim that only constituents that are dependents of the same verb can be fronted
together \parencites[\page 66]{Fanselow93a}[\page 1634]{Hoberg97a} was confirmed
\citep[Chapter~3]{MuellerGS}. A further insight is that the linearization properties of the fronted
material (NPs, PPs, adverbs, adjectives) correspond to the linearization properties they would have
in the \mf. The example in (\mex{1}) is even more interesting. It shows that there can be a right
sentence bracket (the particle \emph{los}) and an extraposed constituent (something following the
particle: \emph{damit}) before the finite verb (\emph{geht} `goes'):

\ea
\label{ex-los-damit-zwei} 
\glll \emph{Los} damit \emph{geht} es schon am 15. April.\footnotemark\\
      off there.with goes it \textsc{prt} on 15. April\\
      \type{4}   \type{5} \type{2} \type{3} \type{3} {} \type{3} {}\\\german
\footnotetext{
        taz, 01.03.2002, p.\,8.
    }
\glt `The whole thing starts on April 15th.'
\z
As far as topology is concerned, this sentence corresponds to sentences with VP fronting and
extraposition like the one in (\mex{1}) discussed in \citew[\page82]{Reis80a}.
\ea
\gll {}[Gewußt, daß du kommst,] haben wir schon seit langem.\\
       \spacebr{}known that you come have we \particle{} since long\\\german
\glt `We have known for a while that you are coming.'
\z
In (\mex{0}) \emph{gewußt, dass du kommst} `known that you come' forms a VP in which \emph{gewußt}
is the right sentence bracket and \emph{daß du kommst} `that you come' is extraposed. We have the
same situation in (\mex{-1}) with \emph{los} `off' and \emph{damit} `there.with', except that one
would not want to claim that \emph{damit} `there.with' depends on \emph{los} `off'.

In Kathol's system, \emph{los} would be of type \type{4} and \emph{damit} would have to be of type
\type{5} (an additional type for extraposed items). Without any modification of the general system,
we would get a \type{4} and a \type{5} ordered before a \type{2} (a right sentence bracket and a
postfield preceding the left sentence bracket), something that is ruled out by Kathol's linearization constraints. 

\citet{Mueller2002c}, still working in a domain-based framework, developed an analysis assuming an
empty verbal head to explain the fact that the fronted constituents have to depend on the same verb
and that there is a separate topological area that is independent of the remaining clause. So,
\emph{los} and \emph{damit} are domain objects within a larger domain object placed in the
prefield. \citet{Wetta2011a} suggests an analysis in which two or more constituents are compacted
into one domain object, so \emph{los} and \emph{damit} would form one object that is inserted into
the domain containing the finite verb. However, this begs the question of what kind of object it is
that is formed. Section~\ref{sec-partial-compaction} dealt with partial compaction of NPs. Some of
the elements from an NP domain were liberated and other elements were fused into a new object that
had the same category as the object containing all material, namely NP. But the situation with
examples like (\ref{ex-mult-front}) and (\ref{ex-los-damit-zwei}) is quite different. We have a
particle and a pronominal adverb in (\ref{ex-los-damit-zwei}) and various other combinations of
categories in the examples collected by \citet{Mueller2003b,Mueller2005e,Mueller2013a} and \citet{Bildhauer2011a}. It would
not make sense to claim that the fronted object is a particle or a pronominal adverb. Note that it
is not an option to leave the category of the fronted object unspecified, since HPSG comes with the
assumption that models of linguistic objects are total, that is, maximally specific
(\citealp{King99a-u}, see also \crossrefchaptert{formal-background}). Leaving the category and
valence properties of the item in the prefield unspecified would make such sentences infinitely
ambiguous. Of course Wetta could state that the newly created object is a verbal projection, but
this would just be stating the effect of the empty verbal head with a relational constraint, which
I consider less principled than positing an empty element.

However, the empty verbal head that I stated as part of a linearization grammar in 2002 comes as a stipulation, since its
only purpose in the grammar of \ili{German} was to account for apparent multiple
frontings. \citet{Mueller2005d,MuellerGS} drops the linearization approach and assumes head-movement
instead. The empty head that is used for accounting for the verb position in \ili{German} can also be used
to account for apparent multiple frontings. The analysis is sketched in (\mex{1}):
%\largerpage
\eal
\ex
\label{ex-zum-zweiten-anal-zwei}%
\longexampleandlanguage{
\gll {}[\sub{VP} [Zum zweiten Mal] [die Weltmeisterschaft] \_\sub{V} ]$_i$ errang$_j$ Clark 1965 \_$_i$ \_$_j$.\\
      {}         \spacebr{}to.the second time \spacebr{}the world.championship {} {} won Clark
      1965\\}{German}
\ex
\gll {}[\sub{VP} \emph{Los} \_\sub{V} damit]$_i$  \emph{geht}$_j$ es schon am 15. April \_$_i$ \_$_j$.\\
     {}          off        {}        there.with   goes it \textsc{prt} on 15. April\\
\glt `The whole thing starts on the 15th April.'

\zl 
Space precludes going into all the details here, but the analysis treats apparent multiple frontings parallel to
partial verb phrase frontings\is{partial verb phrase fronting}. A lexical rule is used for multiple frontings which is a special case
of the head-movement rule that was discussed in Section~\ref{sec-head-movement}. So, apparent
multiple frontings are analyzed with means that are available to the grammar anyway. This analysis
allows us to keep the insight that \ili{German} is a V2 language and it also gets the same-clause constraint
and the linearization of elements right. As for (\mex{0}b): \emph{los damit} `off there.with' forms
a verbal constituent placed in the \vf and within this verbal domain, we have the topological fields that are needed:
the right sentence bracket for the verbal particle and the verbal trace and the \nf for
\emph{damit} `there.with'. See \citew{Mueller2005c,Mueller2005d,MuellerGS} for details.%
\is{word order!V2|)}


%% \subsection{Other usages of constiuent order domains}
%% reference to Chapter~\ref{chap-coordination} on coordination and Chapter~\ref{chap-ellipsis} on ellipsis.
%\todostefan{comparison with Dependency Grammar (Chapter~\ref{chap-dg})?}

This chapter so far has discussed the tools that have been suggested in HPSG to account for constituent
order: flat vs.\ binary branching structures, linearization domains, head-movement via \dsl. I
showed that analyses of \ili{German} relying on discontinuous constituents and constituent order domains
are not without problems and that head-movement approaches with binary branching and continuous
constituents can account for the data. I also demonstrated in Section~\ref{sec-absolutely-free} that
languages like \ili{Warlpiri} that allow for much freer constituent order than \ili{German} can be accounted
for in models allowing for discontinuous constituents. The following section discusses a proposal by \citet{Bender2008a} that shows that even
languages like Australian free constituent order languages can be handled without discontinuous constituents.
\is{order domain|)}

\section{Free constituent order languages without order domains}
\label{sec-free-without-domains}

\is{scrambling|(}%
\citet{Bender2008a} discusses the Australian language \ili{Wambaya}\il{Wambaya} and shows how phenomena parallel to
those treated by \citet{DS99a} can be handled without discontinuous\is{discontinuous constituent|(} constituents.\il{Warlpiri|(}
Bender assumes that all arguments of a head are projected to higher nodes even when they are
combined with the head; that is, arguments are not canceled off from valence lists. See also
\citew{Meurers99b}, \citew{Prze99} and \citew{Mueller2008a} for earlier non"=cancellation approaches.\footnote{
\citew[\page 560]{Higginbotham85a} and \citew[\page 239]{Winkler97a} make similar suggestions with regard to the
representation of theta roles.%
} Example (\ref{ex-warlpiri}) from
Section~\ref{sec-absolutely-free} can be recast with continuous constituents as shown in
Figure~\ref{fig-warlpiri-non-cancellation}. 
\begin{figure}
\oneline{%
\begin{forest}
sm edges
[Aux \sliste{ \spirit{1}, \spirit{2}, \spirit{3} }
     [\ibox{1} NP  [ kurdu-jarra-rlu;{child-\textsc{du}-\textsc{erg}}]]
     [Aux \sliste{ \ibox{1}, \spirit{2}, \spirit{3} }
       [Aux \sliste{ \ibox{1}, \spirit{2}, \spirit{3} }
         [Aux \sliste{ \ibox{1}, \spirit{2}, \ibox{3} }
           [Aux \sliste{ \ibox{1}, \ibox{2}, \ibox{3} } [ ka-pala;{\textsc{prs}-3\textsc{du}.\textsc{subj}}]]
           [\ibox{2} NP  [ maliki;{dog.\textsc{abs}}]]]
         [\ibox{3} V  [ wajili-pi-nyi;{chase-\textsc{npast}}]]]
       [{NP[\textsc{mod} \ibox{1} ]}  [ wita-jarra-rlu;{small-\textsc{du}-\textsc{erg}}]]]]
\end{forest}}
\caption{\label{fig-warlpiri-non-cancellation}Analysis of free constituent order in Warlpiri using non-cancellation}
\end{figure}
The figure shows that arguments are not removed from the valence representation after combination
with the head. Rather they are marked as satisfied: \spirit{1}. Since they are still in the representation,
schemata may refer to them. Bender suggests a schema that identifies the \modv of an element that
could function as an adjunct in a normal head-adjunct structure with an element in the valence
representation. In Figure~\ref{fig-warlpiri-non-cancellation}, the \modv of the second ergative
nominal \emph{wita-jarra-rlu} `small' is identified with an argument of the auxiliary verb \iboxb{1}. The
adjunct hence has access to the referential index of the argument and it is therefore guaranteed
that both parts of the noun phrase refer to the same discourse referent. The NP for
\emph{kurdu-jarra-rlu} is combined with the projection of the auxiliary to yield a complete
sentence. Since \ibox{1} does not only contain the semantic index and hence information about number
(the dual) but also case information, it is ensured that distributed noun phrases have to bear the
same case. Since information about all arguments are projected along the head path, \ibox{2} would
also be available for an adjunct referring to it. So in the place of \emph{wita-jarra-rlu}
`small-\textsc{du}-\textsc{erg}' we could also have another adjunct referring to \emph{maliki}
`dog.\textsc{abs}'. This shows that even languages with constituent order as free as Australian
languages can be handled within HPSG without assuming discontinuous constituents.\il{Warlpiri|)}\is{discontinuous constituent|)}
\is{scrambling|)}

\section{Summary}

A major feature of constraint-based analyses is that when no constraints are stated, there is
freedom. The chapter discussed the order of head and adjunct: if the order of head and adjunct is not
constrained, both orders are admitted. 

This chapter explored general approaches to constituent order in HPSG. On the one hand, there are
approaches to constituent order that assume flat constituent structure, allowing permutation of
daughters as long as no LP constraint is violated. On the other hand, there are approaches assuming
binary branching structures. Approaches that assume flat structures can serialize the head to the
left or to the right or somewhere between other daughters in the structure. Approaches assuming
binary branching have to use other means. One possibility is ``head movement'', which is analyzed as
a series of local dependencies by passing information about the missing head up along the head
path. The alternative to head movement is linearization of elements in special linearization
domains, allowing for discontinuous constituents. I showed that there are reasons for assuming
head-movement for \ili{German} and how even languages with extremely free constituent order can be
analyzed without assuming discontinuous constituents.



%\section*{Abbreviations}
\section*{\acknowledgmentsUS}

I thank Anne Abeillé, Bob Borsley, Doug Ball and Jean-Pierre Koenig for very detailed and very useful comments. I thank
Elizabeth Pankratz for comments and for proofreading.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}



\if0
Bob:



It seems to me that whether or not constituent structures are confined to binary branching is quite important for constituent order. How far different constituent orders can be treated as a matter of alternative ordering of sisters depends on how much constituents are sisters. For example, the contrast between (1) and (2) might just show that PP sisters can appear in either order, but that is only possible if they are sisters.

 

(1) Kim talked to Lee about the weather.

(2) Kim talked about the weather to Lee.

 

I assume section 4 is concerned with verb-initial clauses. If so, perhaps the title should make that clear. For Minimalism head-movement is involved not just in verb-initial clauses but also VPs where the verb has two complements and nominal phrases where the noun precedes an attributive adjective, among other things.

 

Borsley (1989) was concerned with \ili{English}, not \ili{Welsh}. It made the point that you could have an analogue of verb-fronting for \ili{English} auxiliary-initial sentences. Borsley (2006) discusses whether \ili{Welsh} finite clauses involve some form of VP and argues that they do not and hence that they involve a flat structure.

 

Is 4.2 just about constructional approaches to \ili{English} auxiliary-initial clauses (and not the \ili{English} auxiliary system in general)? It’s not really clear. I think Pollard and Sag’s (1987) rule 3 and Pollard and Sag’s (1994) are essentially constructional approaches to verb-initial clauses. It is perhaps worth noting (at least briefly) that there has been a debate in versions of HPSG that distinguish between SUBJ and COMPS features about whether post-verbal subjects are realizations of the SUBJ feature like pre-verbal subjects or the COMPS feature like ordinary complements. The first view is adopted in Ginzburg and Sag (2000) and the second in Sag et al. (2003). I would see the first view as a constructional view (since it requires a special phrase type) and the second a lexical view (since it requires a lexical rule of some kind to derive appropriate lexical descriptions). Borsley (1989) argued that the second approach is appropriate for \ili{Welsh} verb-initial clauses and Borsley (1995) argued that the first approach is right for \ili{Arabic} verb-initial clauses.

 

Will you be saying much about extraposition phenomena? Chapter 1 currently has a brief reference to order domains, illustrating them with extraposition of relative clauses.

 

Will you say something about analyses of free constituent order languages with order domains?

 

 

REFERENCES

Borsley, R. D. (1989), An HPSG approach to \ili{Welsh}, Journal of Linguistics 25, 333‑354.

Borsley, R. D. (1995), On some similarities and differences between \ili{Welsh} and \ili{Syrian Arabic}, Linguistics 33, 99-122.

Borsley R. D. (2006), On the nature of \ili{Welsh} VSO clauses, Lingua 116, 462-490.



\inlinetodostefan{Anne: You should mention that linearization was also used for various ordering
  phenomena in \ili{Romance} languages: \ili{French} subject inversion \citep{BGM99a-u}, sentential adverbs
  \citep{BG2007b-u}, this could go in part 6.

There is also the work on heaviness and lightness, to account for the (reduced) mobility of various categories : bare nominals in various languages, certain pronouns (A\&G 1999a), certain adverbs (A\&G 1997, 2004a), negation (A\&G2001), and attributive adjectives (A\&G 1999b). In various papers, Abeillé and Godard propose a three valued WEIGHT feature to account for light, middle-weight and heavy constituents (A\&G 2000, 2004). The LP rules usually apply to middle weight ones. The light/heavy distinction is also relevant for the Complex predicate chapter, but it should definitely be mentioned, maybe as a subsection or word order/constituent order since it is part of what LFG people call X° syntax.

When you talk about ``head-movement'', you should mention that it is the analysis assumed by GB for
adverb placement (Pollock 1989, Cinque 1999), and not the analysis adopted in HPSG (A\&G 1997, 2004a,
Kim \& Sag 2002): in HPSG, a possible analysis is to consider certain adjuncts as complements. I
guess this should go in part 5.2.


A key feature of HPSG is the ability to underspecify ordering, this should go in part 2 when you talk about LP rules: if the head-adjunct order is free, you don’t have two LP rules, you have none; the same for head-subject order in lg with free inversion; it also applies nicely to complement ordering ordering in \ili{Romance} (if the ordering between complements and adjuncts is free after the verb in \ili{Romance}, modulo Weight, and I guess it could apply to the \ili{German} Mittelfeld)

It is very different from other frameworks that are forced to suppose a basic order and derive the others by movement or additional constructions. And this is what Minimalists have a hard time understanding when they try to understand our analyses (some do).

please give some simple examples of free daughters ordering before you move on to more complex scrambling example

}


\fi


%      <!-- Local IspellDict: en_US-w_accents -->

%%% Local Variables:
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
