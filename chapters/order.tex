\documentclass[output=paper]{langsci/langscibook} 
\author{%
	Stefan Müller\affiliation{Humboldt-Universität zu Berlin}%
}
\title{Constituent order}

% \chapterDOI{} %will be filled in at production

%\epigram{Change epigram in chapters/03.tex or remove it there }
\abstract{}
\maketitle

\settowidth\jamwidth{(German)}
\begin{document}
\label{chap-order}

\section{Introduction} 

\inlinetodostefan{
The chapter might strike readers as a bit German-centric. This is probably okay as long as you make
it clear at the outset. (And I suppose a discussion of V2 is not just relevant to German or even
Germanic.)}
This chapter deals with constituent order, with a focus on local order variants. English is the
language that is treated most thoroughly in theoretical linguistics but is probably also a rather uninteresting
language as far as the possibilities of reordering constituents is concerned: the order of subject,
verb, and object is fixed in sentences like (\mex{1}):
\ea
Kim likes bagels.
\z
Of course, there is the possibility to front the object as in (\mex{1}) but this is a special,
non-local construction that is not the topic of this chapter but is treated in \crossrefchapterw{udc}.
\ea
Bagels, Kim likes.
\z
This chapter deals with scrambling (the local reordering of arguments) and with alternative
placements of heads (called \emph{head movement} in some theories). Examples of the former are the
subordinate clauses in (\mex{1}) and an example of the latter is given in (\mex{2}):
\eal
\label{ex-permutation-mf}
\ex 
\gll {}[weil]          der Mann der Frau das Buch gibt\\
     \spacebr{}because the.\NOM{} man the.\DAT{} woman the.\ACC{} book gives\\\jambox{(German)}
\ex 
\gll {}[weil]          der Mann das Buch der Frau  gibt\\
     \spacebr{}because the.\NOM{} man  the.\ACC{} book the.\DAT{} woman gives\\
\ex 
\gll {}[weil]          das Buch der Mann der Frau  gibt\\
     \spacebr{}because the.\ACC{} book the.\NOM{} man  the.\DAT{} woman gives\\
\ex 
\gll {}[weil]          das Buch der Frau  der Mann gibt\\
     \spacebr{}because the.\ACC{} book the.\DAT{} woman the.\NOM{} man  gives\\
\ex 
\gll {}[weil]          der Frau  der Mann das Buch gibt\\
     \spacebr{}because the.\DAT{} woman the.\NOM{} man  the.\ACC{} book gives\\
\ex 
\gll {}[weil]          der Frau  das Buch der Mann gibt\\
     \spacebr{}because the.\DAT{} woman the.\ACC{} book the.\NOM{} man  gives\\
\zl
\ea
\gll Gibt der Mann der Frau das Buch?\\
     gives the.\NOM{} man the.\DAT{} woman the.\ACC{} book\\\jambox{(German)}
\glt `Does the man give the woman the book?'
\z
(\mex{-1}) shows that in addition to the unmarked order in (\mex{-1}a) (see \citew{Hoehle82a} on the
notion of unmarked order), five other argument orders are possible in sentences with three-place verbs.

(\mex{0}) shows that the verb is placed in initial position in questions in German. This contrasts
with the verb final order in the subordinate clause in (\mex{-1}a), which has the same order as far
as the arguments are concerned. This alternation of verb placement is usually treated as
head movement in the transformational literature (\citealp{Bach62a}; \citealp*[\page34]{Bierwisch63};
\citealp{Reis74a}; \citealp[Chapter~1]{Thiersch78a}). Declarative main clauses in German are V2
clauses and the respective fronting of the preverbal constituent is usually treated as a non-local dependency (see
\crossrefchapterw{udc}). Hence, V2 sentences will not be handled here.

The following sections deal with the theoretical options within the HPSG framework for dealing with
these phenomena. I first discuss the separation of grammar rules into an immediate dominance part and
a linear precedence component in Section~\ref{sec-id-lp} and then flat vs.\ binary branching
structures (Section~\ref{sec-binary-flat}). While flat structures allow verbs to be ordered clause-finally
or clause-initially, this is not the case for binary branching structures, since only sisters can be
ordered. So, for (\mex{-1}a) one would get the bracketing in (\mex{1}a). If \emph{das Buch} `the
book' and \emph{gibt} `gives' are ordered in a different order, (\mex{1}b) results.
\eal
\ex[]{ 
\gll {}[weil]          [der Mann [der Frau [das Buch gibt]]]\\
     \spacebr{}because \spacebr{}the.\NOM{} man \spacebr{}the.\DAT{} woman \spacebr{}the.\ACC{} book gives\\
}
\ex[*]{
\gll {}[weil]          [der Mann [der Frau [gibt das Buch]]]\\
     \spacebr{}because \spacebr{}the.\NOM{} man \spacebr{}the.\DAT{} woman \spacebr{}gives
     the.\ACC{} book\\
}
\zl
Hence, local reordering is not sufficient to get clause-initial verb order and therefore, proposals with binary branching
structures are usually paired with HPSG's analogue of what is head-movement in transformational
theories. These are explained in Section~\ref{sec-head-movement}. Section~\ref{sec-domains} introduces an extension to
standard HPSG developed by \citet{Reape94a}: constituent order domains. Such constituent
order domains allow for discontinuous constituents and have been used to account for languages like Warlipri\il{Warlpiri}
\citep{DS99a}. Section~\ref{sec-free-without-domains} shows how such languages can be analyzed without admitting discontinuous constituents.


\section{ID/LP format}
\label{sec-id-lp}

HPSG was developed out of Generalized Phrase Structure Grammar\indexgpsg (GPSG) and \isi{Categorial Grammar}. The ideas concerning linearization of
daughters in a local tree were taken over from GPSG \citep*{GKPS85a}. In GPSG a separation between
\isi{immediate dominance} and \isi{linear precedence} is assumed. So, while in classical phrase structure
grammar a phrase structure rule like (\mex{1}) states that the NP[nom], NP[dat] and NP[acc] have to
appear in exactly this order, this is not the case in GPSG and HPSG:
\ea
S $\to$ NP[nom] NP[dat] NP[acc] V
\z
The HPSG schemata corresponding to the phrase structure rule in (\mex{0}) do not express information
about ordering. Instead, there are separate linear precedence (LP) rules\is{Linear Precedence Rule}. A schema like (\mex{0}) licenses 24 
different orders: the six permutations of the three arguments that were shown in
(\ref{ex-permutation-mf}) and all possible placements of the verb (to the right of NP[acc], between
NP[dat] and NP[acc], between NP[nom] and NP[dat], to the left of NP[nom]). Orders like NP[nom],
NP[dat], V, NP[acc] are not attested in German and hence these orderings have to be filtered
out. This is done by linearization rules, which can refer to features or to the function of a
daughter in a schema. (\mex{1}) shows some examples of linearization rules:

\eal
\ex X < V
\ex X < V[\textsc{ini}$-$]
\ex X < Head [\textsc{ini}$-$]
\zl
The first rule says that all constituents have to precede a V in the local tree. The second rule
says that all constituents have to precede a V that has the \textsc{initial} value $-$. One option
to analyze German would be the one that was suggested by \citet{Uszkoreit87a} within the framework
of \isi{GPSG}: one could allow for two linearization variants of finite verbs. So in addition to the
\textsc{ini}$-$ variant of verbs there could be a \textsc{ini}$+$ variant and this variant would be
linearized initially. The LP rule in (\mex{0}c) is more general than (\mex{0}b) in that it does not
mention the part of speech but instead refers to the function of the constituent. The rule says that
a head that has the \textsc{ini} value $-$ has to be linearized to the right of all other elements
in the local tree.

This treatment of constraints on linearization has an advantage that was already pointed out by
researchers working in GPSG: it captures the generalizations regarding linearization. For instance
the order of verbs and their arguments is the same in embedded sentences in German independent of
the finiteness of the verb:
\eal
\ex 
\gll dass er dem Mann das Buch gab\\
     that he.\nom{} the.\dat{} man the.\acc{} book gave\\
\glt `that he gave the man the book'
\ex
\gll dass er versucht, [dem Mann das Buch zu geben]\\
     that he.\nom{} tried     \spacebr{}the.\dat{} man the.\acc{} book to give\\
\glt `that he tried to give the man the book'
\zl
This is also true for the relative order of dative and accusative object in (\mex{0}). The
constraints regarding linearization hold across rules. By factoring these constraints out, the
generalizations can be captured.

Furthermore, generalizations about constituent structure can be captured. For example, the two
phrase structure rules in (\mex{1}) would be needed for head-initial and head-final languages, respectively:
\eal
\ex VP $\to$ V NP NP
\ex VP $\to$ NP NP V
\zl
In an ID/LP framework only one rule is needed. The linearization of the head is factored out of the
rules. Similarly, HPSG has just one schema for Head-Adjunct structures although languages like
English allow adjuncts to precede or follow the head they modify. The schema does not say anything
about the order of the daughters. This is taken care of by two LP rules saying that adjuncts marked
as pre-modifiers have to preceede their head while those that are marked as post-modifiers follow it.


\section{Flat and binary branching structures}
\label{sec-binary-flat}

The previous section discussed LP rules and used flat phrase structure rules for illustration. The
corresponding flat structures are also used in HPSG. Schema~\ref{schema-hc-flat} shows a Head-Complement schema that
combines a head with all the complements selected via the \compsl.\footnote{
  \citet[\page 4]{GSag2000a-u} assume a list called \dtrs for all daughters including the head
  daughter. It is useful to be able to refer to specific non-head daughters without having to know a
  position in a list. For example in head-adjunct structures the adjunct is the selector. So I keep
  \dtrs for a list of ordered daughters and \textsc{head-dtr} and \textsc{non-head-dtrs} for
  material that is not necessarily ordered.%
}
\begin{schema}[Head-Complement Schema]
\label{schema-hc-flat}
\type{head-complement-phrase} \impl\\*
\ms{
\ldots|comps & \eliste\\
head-dtr      & \ms{ \ldots|comps \ibox{1}\\
                   }\\
non-head-dtrs & \upshape synsem2sign(\ibox{1})\\
}
\end{schema}
\verb+synsem2sign+ is a \isi{relational constraint} mapping \type{synsem} objects as they are contained in
the \compsl onto objects of type \type{sign} as they are contained in daughters (see \citealt[\page
  34]{GSag2000a-u} for a similar proposal).\footnote{
  In Sign-Based Construction Grammar the objects in valence lists are of the same type as the
  daughters. A relational constraint would not be needed in this variant of the HPSG
  theory. Theories working with a binary branching Head-Complement Schema as Schema~\ref{hcs-binary} on
  page~\pageref{hcs-binary} would not need the relational constraint either since the \type{synsem} object in
  the \compsl can be shared with the \synsemv of the element in the list of non-head daughters directly.
}
How this schema can be used to analyze VPs like the one in (\mex{1}) is shown in Figure~\ref{fig-gave-Sandy-a-book}.
\ea
Kim gave Sandy a book.
\z
\begin{figure}
\begin{forest}
sm edges
[{V[\comps \eliste]}
  [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]} [gave]]
    [\ibox{1} NP [Sandy]]
    [\ibox{2} NP [a book,roof]]]
\end{forest}
\caption{\label{fig-gave-Sandy-a-book}Analysis of the VP \emph{gave Sandy a book} with a flat structure}
\end{figure}

Researchers working on English usually assume a flat structure \parencite{ps2}[\page 479]{Sag97}{GSag2000a-u} but assuming binary
branching structures would be possible as well, as is clear from analyses in Categorial Grammar,
where binary combinatory rules are assumed \citep{Ajdukiewicz35a-u,Steedman2000a-u}. For languages
like German it is usually assumed that structures are binary branching (but see \citealt[\page 156]{Reape94a} and
\citealt[\page 51]{BvN98}). The reason for this is that
adverbials can be placed anywhere between the arguments as the following example from \citet[\page
  145]{Uszkoreit87a} shows:
\ea
\gll \emph{Gestern} hatte \emph{in} \emph{der} \emph{Mittagspause} der Vorarbeiter \emph{in} \emph{der} \emph{Werkzeugkammer} dem Lehrling \emph{aus
Boshaftigkeit} \emph{langsam} zehn schmierige Gußeisenscheiben \emph{unbemerkt} in die Hosentasche gesteckt. \\
yesterday had during the lunch.break the foreman in the tool.shop the apprentice maliciously slowly ten
greasy cast.iron.disks unnoticed in the pocket put\\
\glt `Yesterday during lunch break, the foreman maliciously and
unnoticed, put ten greasy cast iron disks slowly into the
apprentice's pocket.'
\z
A straightforward analysis of adjunct placement in German and Dutch is to assume that adjuncts can
attach to any verbal projection. For example, Figure~\ref{fig-adjunct-placement-german} shows the
analysis of (\mex{1}):
\ea
\gll weil deshalb jemand gestern der Frau schnell das Buch gab\\
     because therefore somebody yesteray the woman quickly the book gave\\%\german
\glt `because somebody quickly gave the woman the book yesterday'
\z
\begin{figure}
\begin{forest}
sm edges
[V
       [Adv [deshalb;therefore]]
       [V
         [NP [jemand;somebody]]
         [V
           [Adv [gestern;yesterday]]
           [V
              [NP [der Frau;the woman,roof]]
              [V
                [Adj [schnell;quickly]]
                [V
                  [NP [das Buch;the book,roof]]
                  [V [gab;gave]]]]]]]]
\end{forest}
\caption{Analysis of [\emph{weil}] \emph{deshalb jemand gestern der Frau schnell das Buch gab}
  `because somebody quickly gave the woman the book yesterday' with binary branching structures}\label{fig-adjunct-placement-german}
\end{figure}
The adverbials \emph{deshalb} `therefore', \emph{gestern} `yesterday' and \emph{schnell} `quickly'
may attach to any verbal projection. For example, \emph{gestern} could also be placed at the other
adjunct positions in the clause. 

Binary branching structures with attachment of adjuncts to any verbal projection also account for recursion and hence the
fact that arbitrarily many adjuncts can attach to a verbal projection.
%any constituent can be combined with the verb to the exclusion of any other argument \citep{Mueller2003b,MuellerLehrbuch1}\todostefan{page}:
%% \eal
%% \label{bsp-acc-dat-pvp}
%% \ex 
%% \gll Den Wählern erzählen sollte man diese Geschichte nicht.\\
%%      the.\DAT{} voters tell should one this.\ACC{} story not\\\jambox{(German)}
%% \glt `One should not tell the voters these stories.'
%% \ex 
%% \gll Märchen erzählen sollte man den Wählern nicht.\\
%%      stories.\ACC{} tell     should one the.\DAT{} voters not\\
%% \zl
%% In fact any subset of the objects can be combined with the verb and form a constituent. To account
%% for this
Of course it is possible to formulate analyses with flat structures that involve arbitrarily many
adjuncts \citep{Kasper94a,Noord94,BMS2001a}, but these analyses involve relational
constraints\is{relational constraint} in schemata or in lexical items. In Kasper's analysis the
relational constraints walk through lists of daughters of unbounded length in order to compute the
semantics. In the other two analyses adjuncts are treated as valents, which may be problematic
because of scope issues. This cannot be dealt with in detail here but see \citew{LH2006a} and
\citew{Chaves2009a} for discussion. 

The following schema licenses binary branching head-complement phrases:
\begin{samepage}
\begin{schema}[Head-Complement Schema (binary branching)]
\label{hcs-binary}
\type{head-complement-phrase} \impl\\*
\ms{
comps & \ibox{1} $\oplus$ \ibox{2}\\
head-dtr      & \ms{ comps \ibox{1} $\oplus$ \sliste{ \ibox{3} } $\oplus$ \ibox{2}\\
                   }\\
non-head-dtrs & \liste{ \ms{ synsem \ibox{3} } }\\
}
\end{schema}
\end{samepage}
$\oplus$ (append) is a \isi{relational constraint} that concatenates two lists. The \compsl of the head
daughter is split into three lists: a beginning \iboxb{1}, a list containing \ibox{3} and a rest
\iboxb{2}. \ibox{3} is identified with the \synsemv of the non-head daughter. All other elements of
the \compsl of the head daughter are concatenated and the result of this concatenation (\ibox{1}
$\oplus$ \ibox{2}) is the \compsl of the mother node. This schema is very general. It works for
languages that allow for scrambling since it allows an arbitrary element to be taken out of the \compsl
of the head daughter and realize it in a local tree. The schema can also be ``parametrized\is{parameter}'' to account
for languages with fixed word order. For head final languages with fixed order \ibox{2} would be the
empty list (= combination with the last element in the list) and for head-initial languages with
fixed order (\eg English) \ibox{1} would be the empty list (= combination with the first element in the list).

The alternative to using relational constraints as in Schema~\ref{hcs-binary} is to use sets rather
than lists for the representation of valence information
\citep*{Gunji86a,HN89a,Pollard90a,EEU92a}. The Head-Complement Schema would combine the head with
one of its complements. Since the elements of a set are not ordered, any complement can be taken and
hence all permutations of complements are accounted for.

The disadvantage of set-based approaches is that sets do
not impose an order on their members but an order is needed for various subtheories of HPSG (see
\crossrefchaptert{case} on case assignment, and \crossrefchaptert{binding} on Binding Theory). In
the approach proposed above and in \citew{Mueller2003a,MuellerHPSGHandbook,MuellerCoreGram}, the valence lists are
ordered but the schema allows for combination with any element of the list. For valence
representation and the order of elements in valence lists see \crossrefchapterw{arg-st}.
%\todostefan{maybe cite \citet{AMM2013a}}
%) as the underlying representation and determinant of basic order





%% \section{Nonlocal dependencies}
%% Brief mention and pointers to Chapter~\ref{chap-udc}.

\section{Head movement vs.\ constructional approaches assuming flat structures}
\label{sec-head-movement}

The Germanic languages signal the clause type by verb position. All Germanic languages with the
exception of English are V2 languages: the finite verb is in second position in declarative main
clauses. The same holds for questions with \emph{wh} phrases. Yes/no questions are formed by putting
the verb in initial position. English is a so-called \emph{residual V2 language}, that is, there are
some constructions that are parallel to what is known from V2 languages. For example, while declarative
clauses are in base order (SVO), questions follow the pattern that is known from other Germanic
languages.
\ea
What$_i$ did Kim read \trace$_i$? 
\z
Analyses assuming flat structures (or flat linearization domains, see Section~\ref{sec-domains})
usually treat alternative orders of verbs in Germanic languages as linearization variants
\citep{Reape94a,Kathol2001a,Mueller95c,Mueller2003a,TBjerre2006a}, but this is not necessarily so as
Bouma and van Noord's analysis of Dutch clauses show \citep[\page 62, 71]{BvN98}. The alternative to
verb placement as linearization is something that is similar to verb movement in Government \&
Binding: an empty element takes the position of the verb in its canonical position and the verb is realized
in initial or -- in case something is realized before the finite verb -- in second position. The following subsection deals with such approaches in more
detail. Subsection~\ref{sec-aux-inversion-phrasal} deals with a constructional approach.

\subsection{Head movement approaches}

Building on work by \citet{Jacobson87} in the framework of Categorial Grammar\indexcg, \citet{Borsley89} showed that in addition to the analysis of auxiliary inversion in English that was
suggested in GPSG \citep{GKPS85a} an analysis that is similar to the movement-based analysis in GB
is possible in HPSG as well. The technique that is used in the analysis is basically the same that
was developed by \citet{Gazdar81} for the treatment of nonlocal dependencies in GPSG. An empty category is
assumed and the information about the missing element is passed up the tree until it is bound off at
an appropriate place (that is by the fronted verb). Note that the heading of this section contains
the term \emph{head movement} and I talk about traces, but it is not the case that something is
actually moved. There is no underlying structure with a verb that is transformed into one with the
verb fronted and a remaining trace in the original position of the verb. Instead, the empty element
is a normal element in the lexicon and can function as the verb in the respective position.
The analysis of (\mex{1}) is shown in Figure~\ref{fig-did-kim-get-the-job-hm}.
\ea
Did Kim get the job?
\z
\begin{figure}
\begin{forest}
sm edges
[S
  [{V[\comps \sliste{ \ibox{1} } ]} 
    [{V[\textsc{loc} \ibox{2} ]} [did]]]
  [\ibox{1} S\feattab{ \head{}|\dsl \ibox{2}\\
                 \spr \sliste{ }\\
                 \comps \sliste{  } }
    [\ibox{3} NP [Kim]]
    [VP\feattab{ \head{}|\dsl \ibox{2}\\
                 \spr \sliste{ \ibox{3} }\\
                 \comps \sliste{  } }
      [V\ibox{2}\feattab{ \head{}|\dsl \ibox{2}\\
                          \spr \sliste{ \ibox{3} }\\
                          \comps \sliste{ \ibox{4} } } [\trace]]
      [\ibox{4} VP [get the job, roof]]]]]
\end{forest}
\caption{\label{fig-did-kim-get-the-job-hm}Analysis of English auxiliary constructions as head"=movement following \citep{Borsley89}}
\end{figure}
A special variant of the auxiliary selects a full clause in which an auxiliary is missing. The fact
that the auxiliary is missing is represented as the value of \dsl. The value of \dsl is a
\type{local} object, that is something that contains syntactic and semantic information (\ibox{2}
in Figure~\ref{fig-did-kim-get-the-job-hm}). \dsl is a head feature and hence available everywhere
along a projection path (see \crossrefchaptert{properties} for the Head Feature
Principle).\todostefan{check reference to Chapter~\ref{chap-properties}.} The empty element for
head movement is rather simple:
\ea
empty element for head movement:\\*
\ms[word]{
phon & \eliste\\
synsem|loc & \ibox{1} \ms{ cat|head|dsl \ibox{1}\\
                         }\\
}
\z
It states that there is an empty element that has the local requirements that correspond to its
\dslv. For cases of verb movement it says: I am a verb that is missing itself. The fronted auxiliary
is licensed by a lexical rule that maps a non-fronted auxiliary onto one that selects a complete
clause from which the input auxiliary is missing. 
%% The special lexical item that is needed for the
%% analysis of (\mex{-1}) is given in (\mex{1}):
%% \ea
%% \catv of the lexical item used for auxiliary inversion:\\
%% \ms{
%% head & \ms[verb]{
%%         vform & fin\\
%%         dsl   & none\\
%%         }\\
%% comps & \liste{ \ms{ cat \ms{ head & \ms[verb]{ dsl & \ms{ head & \ms[verb]{ vform & fin\\
%%                                                                              aux   & +\\
%%                                                                            }\\
%%                                                          }\\
%%                                                }\\
%%                               spr & \eliste\\
%%                               comps & \eliste\\
%%                               } } }\\
%% }
%% \z

Such head-movement analyses are assumed by most
researchers working on \ili{German} (\citealp*[Section~4.7]{KW91a}; \citealp{Oliva92a}; \citealp*{Netter92};   
\citealp*{Kiss93}; \citealp*{Frank94}; \citealp*{Kiss95a}; \citealp{Feldhaus97},
\citealp{Meurers2000b}; \citealp{Mueller2005c,MuellerGS}) and also by \citep[\page 62,
  71]{BvN98} in their work on \ili{Dutch}, by \citet{MOeDanish} in their grammar of
\ili{Danish} and by \citet{MuellerGermanic} for Germanic in general.


\subsection{Constructional approaches}
\label{sec-aux-inversion-phrasal}

The alternative to head-movement-based approaches is a flat analysis with an alternative
serialization of the verb. This was already discussed with respect to German, but I want to discuss
English auxiliary constructions here, since the figured prominently in linguistic discussions.
In the analysis of (\mex{1}) shown in Figure~\ref{fig-did-kim-get-the-job}, the auxiliary \emph{did}
selects for the subject \emph{Kim} and a VP \emph{get the job}.
\ea
Did Kim get the job?
\z
\begin{figure}
\begin{forest}
sm edges
[S
  [{V[\comps \sliste{ \ibox{1}, \ibox{2} } ]} [did]]
  [\ibox{1} NP [Kim]]
  [\ibox{2} VP [get the job, roof]]]
\end{forest}
\caption{\label{fig-did-kim-get-the-job}Analysis of English auxiliary constructions according to \citep{Sag2018a}}
\end{figure}
The tree in Figure~\ref{fig-did-kim-get-the-job} is licensed by a schema combining a head with its
subject \iboxb{1} and its VP complement \iboxb{2} in one go.\footnote{
  An alternative is to assume a separate valence feature for the subject (\feat{subj}) and assume a
  schema that combines the head with the element in the \subjl and the elements in the \compsl
  \citep[\page 36]{GSag2000a-u}.%
} As is common in HPSG since 1995
\citep{Sag97a} phrasal schemata are organized 
in type hierarchies and the general schema for auxiliary initial constructions has the type
\type{aux-initial-cxt}. \citet{Fillmore99a} and \citet{Sag2018a} argue that there are various usages
of auxiliary-initial constructions and assign the respective usages to subconstructions of the
general auxiliary-initial construction. Technically this amounts to stating subtypes of
\type{aux-initial-cxt}. For example, \citet{Sag2018a} posit a subtype \type{polar-int-cl} for polar
interrogatives like (\mex{1}a) and another subtype \type{auxinitial-excl-cl} for exclamatives like (\mex{1}b).
\eal
\ex Are they crazy?
\ex Are they crazy!
\zl
\citet{Chomsky2010a} compared the various clause types used in HPSG with the -- according to him --
much simpler Merge-based analysis in Minimalism. Minimalism assumes just one very general schema for
combination (External Merge is basically equivalent to our Schema~\ref{hcs-binary} above, see
\citew{MuellerUnifying}), so this rule for combining linguistic objects is very simple, but this
does not help in any way when considering the facts: there are at least five different meanings
associated with auxiliary initial clauses (polar interrogative, blesses/curses, negative imperative,
exclamatives, conditionals) and these have to be captured somewhere in a grammar. One
way is to state them in a type hierarchy as is done in some HPSG analyses and in SBCG, another way
is to use implicational constraints that assign meaning with respect to actual configurations
(see Section~\ref{sec-mixed-approaches}) and a third way is to do everything lexically. The only option for
Minimalism is the lexical one. This means that Minimalism has to either assume as many lexical items
for auxiliaries as there are types in HPSG or to assume empty heads that contribute the meaning that
is contributed by the phrasal schemata in HPSG
\parencites[Section~5]{Borsley2006a}{BM2018Minimalism}. 
% Bob: I think they would assume a variety of different C-elements attracting the auxiliary: an
% interrogative C, an exclamative C, etc.
% Stefan: Adger has different C heads.
The latter proposal is generally assumed in
Cartographic approaches \citep{Rizzi97a-u}. Since there is a fixed configuration of functional projections
that contribute semantics, one could term these Rizzi-style analyses \emph{Crypto-Constructional}.

\subsection{Mixed approaches}
\label{sec-mixed-approaches}

The situation with respect to clause types is similar in German. Verb first sentences can be yes/no
questions (\mex{1}a), imperatives (\mex{1}b), conditional clauses (\mex{1}c), and declarative
sentences with topic drop (\mex{1}d). 
\eal
\ex\label{ex-kommt-peter-question}
\gll Kommt Peter?\\
     comes Peter\\\german
\glt `Does Peter come?'
\ex 
\gll Komm!\\
     come\\
\ex\label{ex-kommt-peter-conditional}
\gll Kommt Peter, komme ich nicht.\\
     comes Peter  come  I not\\
\glt `If Peter comes, I won't come.'
\ex 
\gll Kommt. (Was ist mit Peter?)\\
     comes  \hspaceThis{(}what is with Peter\\
\glt `What about Peter?' `He comes.'
\zl
Verb second sentences can be questions (\mex{1}a), declarative sentences (\mex{1}b), or imperatives (\mex{1}c).
\eal
\ex 
\gll Wer kommt?\\
     who comes\\
\ex 
\gll Peter kommt.\\
     Peter comes\\\german
\ex 
\gll Jetzt komm!\\
     now   come\\
\glt `Come now!'
\zl
While one could try and capture this situation by assuming surface order-related clause types, such approaches are rarely
assumed (but see \citew{Kathol2001a} and \citew{Wetta2011a}. See Section~\ref{sec-surface-order} on why such approaches
are doomed to failure). Rather researchers assumed binary branching head-complement structures
together with verb movement (I assumed linearization domains (see Section~\ref{sec-domains}) for ten
years and then switched to the head-movement approach \citep{Mueller2005c,Mueller2005d,MuellerGS}). 

As was explained above, the head movement approaches are based on lexical rules or unary
projections. These license new linguistic objects that could contribute the respective semantics. In
analogy to what \citet{Borsley2006a} has discussed with respect to extraction structures, this would mean that one needs seven versions of fronted verbs to
handle the seven cases in (\mex{-1} and (\mex{0}), which would correspond to the seven phrasal types
that would have to be stipulated in phrasal approaches. But there is a way out of this: one can
assume one lexical item with underspecified semantics. HPSG makes it possible to use implicational
constraints referring to a structure in which an item occurs. Depending on the context the semantics
contributed by a specific item can be further specified. Figure~\ref{abb-konstruktion-implikation}
shows the construction-based and the lexical rule"=based analysis for comparison.
\begin{figure}
\hfill
\begin{subfigure}{.4\textwidth}
\centering
\begin{forest}
[{{\sc sem} f(x) (y)}
   [{{\sc sem} y}]
   [{{\sc sem} x} [\vphantom{\textsc{sem}},no edge]]]
\end{forest}
\caption{Phrasal construction}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
\centering
\begin{forest}
[{{\sc sem} f(x) (y)}
  [{{\sc sem} y}  ]
  [{{\sc sem} f(x)} [{{\sc sem} x}] ]]
\end{forest}
\caption{Unary construction and implication}
\end{subfigure}\hfill\mbox{}
\caption{\label{abb-konstruktion-implikation}Construction-based, phrasal approach and approach with
  implicational constraint}
\end{figure}
In the construction-based analysis the daughters contribute x and y as semantic values and the whole
construction adds the construction meaning $f$. In the lexical rule- or unary projection"=based analysis, the lexical
rule/unary projection adds the $f$ and the output of the rule is combined compositionally with the other
daughter. Now, implicational constraints can be used to determine the exact contribution of the
lexical item \citep{MuellerSatztypen}. This is shown with the example of a question in Figure~\ref{abb-imp-interrogativ}.
\begin{figure}
\centerline{%
\begin{forest}
[{}
       [{{\sc que} \sliste{ [ ] }}]  
       [{\vphantom{t}}]
       ]
\end{forest}\hspace{1em}\raisebox{\baselineskip}{\impl}\hspace{1em}
\begin{forest}
[{}
  [{\vphantom{t}}]  
  [{int(x)}]
       ]
\end{forest}
}
\caption{\label{abb-imp-interrogativ}Implication for interrogative sentences}
\end{figure}
The implication says: when the configuration has the form that there is a question pronoun in the
left daughter, the output of the lexical rule gets question semantics. Since HPSG represents all
linguistic information in the same AVM, such implicational constraints can refer to intonation as
well and hence, implications for establishing the right semantics for V1 questions (\ref{ex-kommt-peter-question}) vs.\ V1
conditionals (\ref{ex-kommt-peter-conditional}) can be formulated.

Note that in Constructional HPSG as layed out by \citew{Sag97a} implicational constraints can refer
to the structure of a complete utterance. Hence items with a complex internal structure can be seen
as contributing a certain meaning. This is ruled out by design in Sign"=Based Construction Grammar,
where linguistic objects of type \type{phrase} do not have daughters.
\inlinetodostefan{%
Bob: Can't you do everything with constructions in SBCG that you could do with phrases in standard
HPSG? I think you need to say more if you want to make this point.
}


\section{Constituent order domains}
\label{sec-domains}

There\is{order domain|(} is an interesting extension to standard HPSG that opens up possibilities for analyses that are
quite different from what is done otherwise in theoretical linguistics: 
\inlinetodostefan{
Bob: Dowty?
}
Mike Reape
\citeyearpar{Reape91,Reape92a,Reape94a} working on German suggested formal tools that allow for the modeling of
discontinuous constituents. His original motivation was to account for scrambling of arguments in
verbal complexes but this analysis was superseded by Hinrichs and Nakazawa's analysis
\citep{HN89a,HN94a} since purely linearization-based approaches are unable to account for \isi{agreement}
and the so-called remote passive\is{passive!remote} (\citealp[Section~5.1, Section~5.2]{Kathol98b};
\citealp[Chapter~21.1]{Mueller99a}). Nevertheless, his work was taken up by others and was used for analyzing German
\citep{KP95a,Kathol2000a,Mueller95c,Babel,Mueller2004b,Wetta2011a,Wetta2014a-u}. As will be
discussed below, there were reasons for dropping analyses of German assuming discontinuous constituents \citep{Mueller2005d,MuellerGS} but constituent order
domains still play a major role in analyzing ellipsis\is{ellipsis} \crossrefchapterp{ellipsis} and
coordination\is{coordination} \crossrefchapterp{coordination}.

\subsection{A special representational layer for constituent order}

The technique that is used to model discontinuous constituents in frameworks like HPSG goes back to Mike Reape's work on German
\citeyearpar{Reape91,Reape92a,Reape94a}. 
Reape uses a list called \textsc{domain}\isfeat{domain} to represent the daughters of a sign in the order in
which they appear at the surface of an utterance. (\mex{1}) shows an example in which the \domv of a
headed-phrase is computed from the \domv of the head and the list of non-head daughters.
\ea
\label{ex-shuffeling-daughters}
\type{headed"=phrase}\istype{headed-phrase} \impl
\ms{
  head-dtr$|$dom  & \ibox{1} \\
  non-head-dtrs   & \ibox{2} \\
  dom  & \ibox{1} $\bigcirc$ \ibox{2} \\
}
\z
The symbol `$\bigcirc$'\is{$\bigcirc$}\is{relation!$\bigcirc$}\isrel{shuffle}\label{rel-shuffle}
stands for the \emph{shuffle} relation. \emph{shuffle} relates three lists A, B and C iff C
contains all elements from A and B and the order of the elements in A and the order of the elements
of B is preserved in C. (\mex{1}) shows the combination of two sets with two elements each:

\ea
$\phonliste{ a, b } \bigcirc \phonliste{ c, d } =
\begin{tabular}[t]{@{}l}
\phonliste{ a, b, c, d } $\vee$\\*[1mm]
\phonliste{ a, c, b, d } $\vee$\\*[1mm]
\phonliste{ a, c, d, b } $\vee$\\*[1mm]
\phonliste{ c, a, b, d } $\vee$\\*[1mm]
\phonliste{ c, a, d, b } $\vee$\\*[1mm]
\phonliste{ c, d, a, b }
\end{tabular}$
\z
The result is a disjunction of six lists. \emph{a} is ordered before \emph{b} and \emph{c} before
\emph{d} in all of these lists, since this is also the case in the two lists \phonliste{ a, b } and
\phonliste{ c, d } that have been combined. But apart from this, \emph{b} can be placed before, between or
after \emph{c} and \emph{d}. 

Every word comes with a domain value that is a list that contains the
word itself:
\ea
Domain contribution of single words, here \emph{gibt} `gives':\\*
\ibox{1} \ms{
phon & \phonliste{ gibt }\\
synsem & \ldots\\
dom  & \sliste{ \ibox{1} } \\
}
\z
The description in (\mex{0}) may seem strange at first glance, since it is cyclic\is{cycle!in
  feature description}, but it can be understood as
a statement saying that \emph{gibt} contributes itself to the items that occur in linearization domains.

The constraint in (\mex{1}) is responsible for the determination of the \phonvs of phrases:
\ea
\type{phrase} \impl
\ms{
 phon & \ibox{1} $\oplus$ \ldots{} $\oplus$ \ibox{n} \\ \\
     dom  & \liste{ \ms[sign]{ phon & \ibox{1} \\ }, \ldots, \ms[sign]{ phon & \ibox{n} \\ }
                  } \\
   }
\z
It states that the \phonv of a sign is the concatenation of the \phonvs of its \textsc{domain}
elements. Since the order of the \textsc{domain} elements corresponds to their surface order, this is
the obvious way to determine the \phonv of the whole linguistic object. 

%\largerpage
Figure~\ref{fig-the-child-reads-the-book-reape-binary} shows how this machinery can be used to license binary
branching structures with discontinuous constituents.
\begin{figure}
\centerfit{%
\begin{forest}
sm edges
[{V[\dom \phonliste{ der Frau, ein Mann, das Buch, gibt }]}
  [{NP[\type{nom}, \dom \phonliste{ ein, Mann }]} [ein Mann;a man,roof]]
  [{V[\dom \phonliste{ der Frau, das Buch, gibt }]}
   [{NP[\type{dat}, \dom \phonliste{ der, Frau  }]} [der Frau;the woman,roof] ]
   [{V[\dom \phonliste{ das Buch, gibt }]}
    [{~~~NP[\type{acc}, \dom \phonliste{ das, Buch }]} [das Buch;the book,roof] ]
    [{V[\dom \phonliste{ gibt }]} [gibt;gives] ] ] ] ]
\end{forest}
}
\caption{\label{fig-the-child-reads-the-book-reape-binary}Analysis of \emph{dass der Frau ein Mann das Buch
    gibt} `that a man gives the woman the book' with binary branching structures and discontinuous constituents}
\end{figure}%%
Words or word sequences that are separated by commas stand for separate domain objects, that is,
\phonliste{ das, Buch } contains the two objects \emph{das} and \emph{Buch} and \phonliste{ das
  Buch, gibt } contains the two objects \emph{das Buch} and \emph{gibt}.
The important point to note here is that the arguments are combined with the head in the order
accusative, dative, nominative, although the elements in the constituent order domain are realized in
the order dative, nominative, accusative rather than nominative, dative, accusative, as one would
expect. This is possible since the formulation of the computation of the \domv using the shuffle
operator allows for discontinuous constituents. The node for \emph{der Frau das Buch gibt} `the
woman the book gives' is discontinuous: \emph{ein Mann} `a man' is inserted into the domain between
\emph{der Frau} `the woman' and \emph{das Buch} `the book'.  This is more obvious in Figure~\ref{fig-the-child-reads-the-book-reape-binary-discont}, which has a serialization of NPs that
corresponds to their order.
\begin{figure}
\centerfit{%
\begin{forest}
sm edges
[{V[\dom \phonliste{ der Frau, ein Mann, das Buch, gibt }]}
  [{NP[\type{dat}, \dom \phonliste{ der, Frau  }]~~~~~~}, no edge, name=np-dat,tier=dat-tier, [der Frau;the woman,roof] ]
  [{NP[\type{nom}, \dom \phonliste{ ein, Mann }]} [ein Mann;a man, roof]]
  [{V[\dom \phonliste{ der Frau, das Buch, gibt }]}, name=v
   [NP, phantom, tier=dat-tier ]
   [{V[\dom \phonliste{ das Buch, gibt }]}
    [{NP[\type{acc}, \dom \phonliste{ das, Buch }]} [das Buch;the book,roof] ]
    [{V[\dom \phonliste{ gibt }]} [gibt;gives] ] ] ] ]
\draw (v.south) -- (np-dat.north);
\end{forest}
}
\caption{\label{fig-the-child-reads-the-book-reape-binary-discont}Analysis of \emph{dass der Frau ein Mann das Buch
    gibt} `that a man gives the woman the book' with binary branching structures and discontinuous
  constituents showing the discontinuity}
\end{figure}% 


\subsection{Absolutely free}
\label{sec-absolutely-free}

While German is more exciting than English in terms of constituent order it is still boring in
comparison to languages like Warlpiri\il{Warlpiri|(} which have much freer constituent order. In
Warlpiri the auxiliary has to be in first or in second position \citep[\page 8]{DS99a} and apart from this even parts of what are noun
phrases in German and English can appear separated from each other. For example, the two parts of
the NP \emph{Kurdu-jarra-rlu wita-jarra-rlu} `child small' may appear discontinuously since they are marked with
the same case:
\ea
\label{ex-warlpiri}
\gll Kurdu-jarra-rlu  ka-pala                 maliki     wajili.pi-nyi        wita-jarra-rlu.\\
% S91:257 
     child-\DU-\ERG{} \PRS-3\DU.\textsc{subj} dog.\ABS{} chase-\textsc{npast} small-\DU-\ERG\\\jambox{(Warlpiri)}\todostefan{Mistake in glossing. Should \emph{pi-nyi} be \emph{pi.nyi}?}
\glt `Two small children are chasing the dog.' or\\
     `Two children are chasing the dog and they are small.'
\z
% Doug Ball:  
%% -- I really got into exploring the Warlpiri example in (21) [I wonder where this example originally appeared -- probably either in Jane Simpson's 1983 dissertation or in work by Ken Hale], and here's what I found: It appears that others (like Mary Dalrymple in her 2001 book on LFG) have given the verb of this sentence as wajilipi-nyi 'chase-NPST', treating the -pi- as part of the stem. In my own investigations, including looking at the Online Warlpiri Dictionary (http://ausil.org/Dictionary/Warlpiri/lexicon/index.htm), it appears that this word is a kind of compound: wajili meaning something like 'fast' or 'quickly' and pi- being a root associated with a light verb of contact, which one might be inclined to translate as 'hit' or 'hunt'. The suffix -nyi is clearly the NONPAST ending (for this kind of verb). So either wajilipi-nyi 'chase-NPST' or wajili-pi-nyi 'fast-hunt-NPST' would work, but I don't think ...pinyi is a recognized Warlpiri morphological unit. 
%
% Simpson, 1991 calls the pi VR verb root.
\citet{DS99a} developed an analysis for this that simply liberates domain elements and inserts them
into the next higher domain. (\mex{1}) shows how this is formalized:
\ea
\type{liberating-phrase} \impl\\*
\ms{
dom & $\delta_0 \bigcirc \delta_1 \bigcirc \ldots \bigcirc \delta_n$\\
head-dtr & \ms{ dom &  $\delta_0$\\
              }\\
non-head-dtrs & \liste{  \ms{ dom &  $\delta_1$\\}, \ldots, \ms{ dom &  $\delta_n$\\} }\\
}
\z
Rather than inserting the complete daughters into the domain of the mother as in
(\ref{ex-shuffeling-daughters}), the \domvs of the daughters are shuffled into the domain of the
mothers. So instead of having the NPs in the same domain as the verb as in the German example in the
previous section one has all the parts of NPs in the next higher domain. Hence, a single nominal element
being placed in front of the auxiliary in second position is explained without
problems. Figure~\ref{fig-warlpiri} shows the analysis of \citew{DS99a}.
\begin{figure}
\oneline{%
\begin{forest}
sm edges
[IP\\\ms{ phon \phonliste{ kurdu-jarra-rlu, ka-pala, wita-jarra-rlu, \ldots }\\
        dom \liste{ \ibox{1} \ms{ phon \phonliste{ kurdu-jarra-rlu } },
                    \ibox{2} \ms{ phon \phonliste{ ka-pala } },
                    \ibox{3} \ms{ phon \phonliste{ wita-jarra-rlu} } }\\
      }
     [NP\\ \ms{ dom & \sliste{ \ibox{1}, \ibox{3} }} [ kurdu-jarra-rlu wita-jarra-rlu;{child-\textsc{du}-\textsc{erg} small-\textsc{du}-\textsc{erg}}, roof]]
     [Aux\\ \ms{ dom & \sliste{ \ibox{2} }} [ka-pala;{\textsc{prs}-3\textsc{du}.\textsc{subj}}]]
     [VP\\ \ms{ dom & \sliste{ \ldots }} [maliki wajili-pi-nyi;{dog.\textsc{abs} chase-\textsc{npast}},roof]]]
\end{forest}}
\caption{\label{fig-warlpiri}Analysis of free constituent order in Warlpiri according to \citet{DS99a}}
\end{figure}
\emph{child} and \emph{small} form an NP. They contribute two independent domain objects (\ibox{1}
and \ibox{3}) to the domain of the mother. The second element in this domain has to be the auxiliary
\iboxb{2}, \ibox{1} is realized initially and \ibox{3} follows the auxiliary.\il{Warlpiri|)}

We have seen so far an analysis that inserts complete objects into the domain of the mother, an
analysis that inserts all domain objects of objects into the domain of the mother and in the next
subsection I want to look at an intermediate case, so-called \emph{partial compaction}.
\inlinetodostefan{Bob: explain compaction. You also don’t explain liberation, which is probably necessary.}

\subsection{Partial compaction (extraposition)}
\label{sec-partial-compaction}

\citet{KP95a} developed an analysis of extraposition that is a mix of the strategies discussed in
the two subsections: most of one NP object is inserted into the domain of the mother as a single object, only those
parts that are extraposed are liberated and inserted as individual domain objects into the domain of
the mother.\footnote{
  This analysis of extraposition is not the only option available in HPSG. I explain it here since
  it shows the flexibility of the domain approach. The more common analysis of extraposition is one
  that is parallel to the \slasch-based approach to extraction that is explained in
  \crossrefchaptert{udc}. Since constraints regarding locality differ for fronting to the left and
  extraposition to the right a different feature is used (\feat{extra}). See \citew{Keller95b} and \citew{Mueller99a} for discussion. More recent approaches assume the
  projection of semantic indices \citep{Kiss2005a} to be able to solve puzzles like \citegen{Link84a-u} hydra sentences and even more
  recent proposals mix index projection and \extra projection \citep{Crysmann2013a}.
}
Kathol \& Pollard's analysis of (\mex{1}) is given in Figure~\ref{fig-extraposition-via-domain-union}.
\ea
\gll  einen Hund füttern, der Hunger hat\\
      a dog feed that hunger has\\\german
\glt `feed a dog that is hungry'
\z
\begin{figure}
\begin{forest}
[\ms{ VP\\
      dom \ibox{5} \liste{ \ibox{2} \ms{ NP\\
                       \phonshape{einen Hund}}, \ms{ V\\
                                         \phonshape{füttern}}, \ibox{3} \ms{ REL-S\\
                                                        extra $+$\\
                                                        \phonshape{der Hunger hat}} } }
  [\ibox{1} \ms{ NP\\
        dom \liste{ \ms{ DET\\
                         \phonshape{einen} }, \ms{ N\\
                                       \phonshape{Hund}}, \ibox{3} \ms{ REL-S\\
                                                        extra $+$\\
                                                        \phonshape{der Hunger hat}} } } 
    [\ms{ DET\\
        \phonshape{einen} }]
    [\ms{ \nbar\\
          dom \liste{ \ms{ N\\
                           \phonshape{Hund}}, \ibox{3} \ms{ REL-S\\
                                                   extra $+$\\
                                                   \phonshape{der Hunger hat}} } }  
      [\ms{ N\\
          \phonshape{Hund}} ] 
      [\ms{ REL-S\\
          extra $+$\\
          \phonshape{der Hunger hat}}]]]
  [\ms{ V\\
        dom \ibox{4} \liste{ \ms{ V\\
                         \phonshape{füttern} } } }]]
\end{forest}\\
\hspace{1.2cm}p-compaction(\ibox{1}, \ibox{2}, \sliste{ \ibox{3} })\hfill\mbox{}\\
\hspace{1.2cm}\ibox{5} = \sliste{ \ibox{2} } $\bigcirc$ \sliste{ \ibox{3} } $\bigcirc$ \ibox{4}\hfill\mbox{}
\caption{\label{fig-extraposition-via-domain-union}Analysis of extraposition via partial compaction
  of domain objects according to \citet{KP95a}}
\end{figure}
\emph{einen Hund, der Hunger hat} `a dog who is hungry' consists of three domain objects:
\emph{einen} `a', \emph{Hund} `dog', and \emph{der Hunger hat} `who hungry is'. The two initial ones
are inserted as one object (the NP \emph{ein Hund} `a dog') into the higher domain
and the relative clause is liberated. While the formation of the new domain at the mother node is
relatively straight-forward in the cases discussed so far, a complex relational constraint is needed
to split the relative clause \iboxb{3} from the other domain objects and construct a new domain
object that has the determiner and the noun as constituents \iboxb{2}. Kathol and Pollard have a relational constraint called
\textit{compaction} that builds new domain objects for insertion into higher
domains. \textit{partial compaction} takes an initial part of a domain and forms a new domain object
from this returning the remaining domain objects for separate insertion into the higher domain. Due
to space limitations, this constraint will not be discussed here but see \citew[\page
  244]{Mueller99a} for a refined version of Kathol and Pollard's constraint. The effect of partial
compaction in Figure~\ref{fig-extraposition-via-domain-union} is that there is a new object \ibox{2}
and a list containing the remaining objects, in the example \sliste{ \ibox{3} }. A list containing
the new object \sliste{ \ibox{2} }, a list containing the remaining objects \sliste{ \ibox{3} } are
shuffled with the domain list of the head \ibox{4}. Since the relative clause is in the same domain
as the verb, it can be serialized to the right of the verb.

%% \inlinetodostefan{
%% Bob: I think there should probably be some sort of reference to other approaches to extraposition of phenomena of various kinds, e.g. Kay and Sag (2012) (Kay, P. \& I. A. Sag (2012), Cleaning up the big mess: Discontinuous dependencies and complex determiners, in Boas and Sag (eds.), Sign-Based Construction Grammar, Stanford: CSLI Publications, 229-256.).}

\subsection{Problems with order domains}

Constituent order domains may seem rather straight-forward since linearization facts can be handled
easily. I assumed constituent order domains and discontinuous constituents for German myself for
over a decade \citep{Mueller95c,Mueller2004b}. However, there are some problems that seem to suggest
that a traditional GB-like head-movement approach is the better alternative. In what follows I want
to discuss just two problematic aspects of linearization approaches: spurious ambiguities and
apparently multiple frontings.

\subsubsection{Partial fronting and spurious ambiguities}

\citet{Kathol2000a} suggests an analysis with binary branching structures in which all arguments are
inserted into a linearization domain and can be serialized there in any order provided no LP rule is
violated. Normally one would have the elements of the \compsl in a fixed order, combine the head
with one element from the \compsl after the other, and let the freedom in the \doml be responsible
for the various attested orders. So both sentences in (\mex{1}) would have analyses in which the
verb \emph{erzählt} `tells' is combined with \emph{Geschichten} `stories' first and then
\emph{Geschichten erzählt} `stories tells' is combined with \emph{den Wählern} `the voters'. Since
the verb and all its arguments are in the same linearization domain they can be ordered in any order including the two orders in (\mex{1}):
\eal
\label{ex-waehlern-geschichten-erzaehlt}
\ex 
\gll weil er den Wählern Geschichten erzählt\\
     because he the voters stories tells\\\german
\glt `because he tells the voters stories'
\ex 
\gll weil er Geschichten den Wählern erzählt\\
     because he stories the voters tells\\
\zl
The problem with this approach is that examples like (\mex{1}) show that grammars have to account
for combinations of any of the objects to the exclusion of the other:
\eal
\label{ex-geschichten-erzaehlen}
\ex 
\gll Geschichten erzählen sollte man den Wählern nicht.\\
     stories     tell     should one the voters not\\\german
\glt `One should not tell the voters such stories.'
\ex 
\gll Den Wählern erzählen sollte man diese Geschichten nicht.\\
     the voters  tell     should one these stories not\\
\zl
\citet[Section~8.9]{Kathol2000a} accounts for examples like (\mex{0}) by relaxing the order of the objects in the
valence list. He uses the shuffle operator in the valence representation:
\ea
\sliste{ NP[\type{nom}] } $\oplus$ (\sliste{ NP[\type{dat}] } $\bigcirc$ \sliste{ NP[\type{acc}] })
\z
This solves the problem with examples like (\mex{-1}) but it introduces a new one: sentences like
(\ref{ex-waehlern-geschichten-erzaehlt}) now have two analyses each. One is the analysis we had before and another one is the one
in which \emph{den Wählern} `the voters' is combined with \emph{erzählt} `tells' first and the result is then combined
with \emph{Geschichten} `stories'. Since both objects are inserted into the same linearization domain, both
orders can be derived. So we have too much freedom: freedom in linearization and freedom in the
order of combination. The proposal that I suggested has just the freedom in the order of combination
and hence can account for both (\ref{ex-waehlern-geschichten-erzaehlt}) and
(\ref{ex-geschichten-erzaehlen}) without spurious ambiguities.


\subsubsection{Surface order, clause types, fields within fields, and empty elements}
\label{sec-surface-order}

\citet{Kathol2001a} develops an analysis of German that uses constituent order domains and determines the
clause types on the basis of the order of elements in such domains. He suggests the topological
fields \type{1}, \type{2}, \type{3}, and \type{4}, which correspond to the traditional
\isi{topological field}s \emph{Vorfeld} `prefield', \emph{linke Satzklammer} `left sentence
bracket', \emph{Mittelfeld} `middle field', \emph{rechte Satzklammer} `right sentence
bracket'. Domain objects may assigned to these fields and they are then ordered by linearization
constraints stating that objects assigned to \type{1} have to precede objects of type \type{2},
type \type{3}, and type \type{4}. Objects of type \type{2} have to precede type \type{3}, and type
\type{4} and so on. For the \vf and the left sentence he stipulates uniqueness constraints saying
that at most one constituent may be of this type. This can be stated in a nice way by using
the linearization constraints in (\mex{1}):
\eal
\ex \type{1} < \type{1}
\ex \type{2} < \type{2}
\zl
This trick was first suggested by \citet[\page 55, Fn.\,3]{GKPS85a} in the framework of \isi{GPSG} and it works
since if there were two objects of type \type{1} than each one would be required to precede the
other one resulting in a violation of the linearization constraint. So in order to avoid such
constraint violation there must not be more than one \type{1}.

\citet[]{Kathol2001a} assumes the following definition for V2 clauses:
\ea
\type{V2-clause} \impl
\ms{
S[\type{fin}]\\
dom & \liste{ [1], \ms{ 2\\V[\type{fin}] }, \ldots }
}
\z
This says that the constituent order domain starts with one element assigned to field \type{1}
followed by another domain object assigned to field \type{2}. While this is in accordance with
general wisdom about German, which is a V2 language, there are problems for entirely surface-based
theories: German allows for multiple constituents in front of the finite verb. (\mex{1}) shows some
examples:
\eal
\label{ex-mult-front}
\ex
\gll {}[Zum zweiten Mal] [die Weltmeisterschaft] errang Clark 1965 \ldots\\
	   \spacebr{}to.the second time \spacebr{}the world.championship won Clark 1965 {}\\
\footnote{
        \citep*[\page 162]{Benes71}
      }\label{bsp-zum-zweiten-mal-die-Weltmeisterschaft}
\glt `Clark won the world championship for the second time in 1965.'
\ex
\label{ex-dem-saft-eine-kraeftige-farbe}
\gll [Dem Saft] [eine kräftige Farbe] geben Blutorangen.\footnotemark\\
     \spacebr{}the.\DAT{} juice \spacebr{}a.\ACC{}   strong   color give blood.oranges\\
\footnotetext{
\citet{BC2010a} found this example in the \emph{Deutsches Referenzkorpus} (DeReKo), hosted at Institut
für Deutsche Sprache, Mannheim: \url{http://www.ids-mannheim.de/kl/projekte/korpora}, 2018-09-13.
}
\glt `Blood oranges give the juice a strong color.'

\zl
\citet{Mueller2003b} extensively documents this phenomenon. The categories that can appear
before the finite verb are almost unrestricted. Even subjects can be fronted together with other
material (\citealp[\page 72]{BC2010a}; \citealp[\page 371]{Bildhauer2011a}). The empirical side of
these apparent multiple frontings was further examined in the Collective Research Center 632,
Project A6 and the claim that only constituents depending on the same verb can be fronted together
\parencites{Fanselow93a}[\page 1634]{Hoberg97a} was confirmed \citep[Chapter~3]{MuellerGS}. A further insight is that
the linearization properties of the fronted material (NPs, PPs, adverbs, adjectives) correspond to the linearization properties they
would have in the \mf. The example in (\mex{1}) are even more interesting. It shows that there can
be a right sentence bracket (the particle \emph{los}) and an extraposed constituent (something
following the particle: \emph{damit}) before the finite verb (\emph{geht} `goes'):
\ea
\label{ex-los-damit-zwei} 
\glll \emph{Los} damit \emph{geht} es schon am 15. April.\footnotemark\\
      off there.with goes it PRT on 15. April\\
      \type{4}   \type{5} \type{2} \type{3} \type{3} {} \type{3} {}\\
\footnotetext{
        taz, 01.03.2002, p.\,8.
    }
\glt `The whole thing starts on the 15th April.'
\z
In Kathol's system, \emph{los} would be of type \type{4} and \emph{damit} would have to be of type
\type{5} (an additional type for extraposed items). Without any modification of the general system,
we would get a \type{4} and a \type{5} ordered before a \type{2} (a right sentence bracket and a
postfield preceding the left sentence bracket), something that is ruled out by Kathol's linearization constraints. 

\citet{Mueller2002b}, still working in a domain-based framework, developed an analysis assuming an
empty verbal head to explain the fact that the fronted constituents have to depend on the same verb
and that there is a separate topological area that is independent of the remaining clause. So,
\emph{los} and \emph{damit} are domain objects within a larger domain object placed in the
prefield. \citet{Wetta2011a} suggests an analysis in which two or more constituents are compacted
into one domain object, so \emph{los} and \emph{damit} would form one object that is inserted into
the domain containing the finite verb. However, this begs the question what kind of object it is
that is formed. Section~\ref{sec-partial-compaction} dealt with partial compaction of NPs. Some of
the elements from an NP domain were liberated and other elements were fused into a new object that
had the same category as the object containing all material, namely NP. But the situation with examples like
(\ref{ex-mult-front}) and (\mex{0}) is quite different. We have a particle and a pronominal adverb in (\mex{0}) and various
other combinations of categories in the examples collected by \citet{Mueller2003b} and
\citet{Bildhauer2011a}. It would not make sense to claim that the fronted object is a particle or a
pronominal adverb. Note that it is neither an option to leave the category of the fronted object
unspecified since HPSG comes with the assumption that models of linguistic objects are total, that
is, maximally specific (\citealp{King99a-u}, see also \crossrefchaptert{formal-background}). Leaving the
category and valence properties of the item in the prefield unspecified would make such sentences
infinitely many times ambiguous. Of course Wetta could state that the newly created object is a
verbal projection but this would just be stating the effect of the empty verbal head within a
relational constraint, which I consider less principled than stating the empty element.

However, the empty verbal head that I stated as part of a linearization grammar in 2002 comes as a stipulation since its
only purpose in the grammar of German was to account for apparent multiple
frontings. \citet{Mueller2005d,MuellerGS} drops the linearization approach and assumes head-movement
instead. The empty head that is used for accounting for the verb position in German can also be used
to account for apparent multiple frontings. The analysis is sketched in (\mex{1}):
\ea
\label{ex-zum-zweiten-anal-zwei}%
\gll {}[\sub{VP} [Zum zweiten Mal] [die Weltmeisterschaft] \_\sub{V} ]$_i$ errang$_j$ Clark 1965 \_$_i$ \_$_j$.\\
      {}         \spacebr{}to.the second time \spacebr{}the world.championship {} {} won Clark 1965\\
\z 
The details cannot be explained here but the analysis treats apparent multiple frontings parallel to
partial verb phrase frontings. A lexical rule is used for multiple frontings which is a special case
of the head-movement rule that was discussed in Section~\ref{sec-head-movement}. So apparent
multiple frontings are analyzed with means that are available to the grammar anyway. This analysis
allows us to keep the insight that German is a V2 language and it also gets the same-clause constraint
and the linearization of elements right. See \citew{Mueller2005c,Mueller2005d,MuellerGS} for details.



%% \subsection{Other usages of constiuent order domains}
%% reference to Chapter~\ref{chap-coordination} on coordination and Chapter~\ref{chap-ellipsis} on ellipsis.
%\todostefan{comparison with Dependency Grammar (Chapter~\ref{chap-dg})?}

The paper so far discussed the tools that have been suggested in HPSG to account for constituent
order: flat vs.\ binary branching structures, linearization domains, head-movement via \dsl. I
showed that analyses of German relying on discontinuous constituents and constituent order domains
are not without problems and that head-movement approaches with binary branching and continuous
constituents can account for the data. I also demonstrated in Section~\ref{sec-absolutely-free} that
languages like Warlpiri that allow for much freer constituent order than German can be accounted
for in models allowing for discontinuous constituents. The following section discusses a proposal by \citet{Bender2008a} showing that even
languages like the Australian free constituent order languages can be handled without discontinuous constituents.
\is{order domain|)}

\section{Free constituent order languages without order domains}
\label{sec-free-without-domains}

\citet{Bender2008a} discusses the Australian language Wambaya\il{Wambaya} and shows how phenomena parallel to
those treated by \citet{DS99a} can be handled without discontinuous constituents.\il{Warlpiri|(}
Bender assumes that all arguments of a head are projected to higher nodes even when they are
combined with the head, that is, arguments are not canceled off from valence lists. See also \citew{Meurers99b,Prze99} and
\citew{Mueller2008a} for earlier non-cancellation approaches.\footnote{
\citew{Higginbotham85a} and \citew{Winkler97a} make similar suggestions with regard to the
representation of theta roles.%
} The example (\ref{ex-warlpiri}) from
Section~\ref{sec-absolutely-free} can be recast with continuous constituents as is shown in
Figure~\ref{fig-warlpiri-non-cancellation}. 
\begin{figure}
\oneline{%
\begin{forest}
sm edges
[Aux \sliste{ \spirit{1}, \spirit{2}, \spirit{3} }
     [\ibox{1} NP  [ kurdu-jarra-rlu;{child-\textsc{du}-\textsc{erg}}]]
     [Aux \sliste{ \ibox{1}, \spirit{2}, \spirit{3} }
       [Aux \sliste{ \ibox{1}, \spirit{2}, \spirit{3} }
         [Aux \sliste{ \ibox{1}, \spirit{2}, \ibox{3} }
           [Aux \sliste{ \ibox{1}, \ibox{2}, \ibox{3} } [ ka-pala;{\textsc{prs}-3\textsc{du}.\textsc{subj}}]]
           [\ibox{2} NP  [ maliki;{dog.\textsc{abs}}]]]
         [\ibox{3} V  [ wajili-pi-nyi;{chase-\textsc{npast}}]]]
       [{NP[\textsc{mod} \ibox{1} ]}  [ wita-jarra-rlu;{small-\textsc{du}-\textsc{erg}}]]]]
\end{forest}}
\caption{\label{fig-warlpiri-non-cancellation}Analysis of free constituent order in Warlpiri using non-cancellation}
\end{figure}
The figure shows that arguments are not removed from the valence representation after combination
with the head. Rather they are marked as satisfied. Since they are still in the representation,
schemata may refer to them. Bender suggests a schema that identifies the \modv of an element that
could function as an adjunct in a normal head-adjunct structure with an element in the valence
representation. In Figure~\ref{fig-warlpiri-non-cancellation} the \modv of the second ergative
nominal \emph{wita-jarra-rlu} `small' is identified with an argument of the auxiliary verb \iboxb{1}. The
adjunct hence has access to the referential index of the argument and it is therefore guaranteed
that both parts of the noun phrase refer to the same discourse referent. The NP for
\emph{kurdu-jarra-rlu} is combined with the projection of the auxiliary to yield a complete
sentence. Since \ibox{1} not just contains the semantic index and hence information about number
(the dual) but also case information, it is ensured that distributed noun phrases have to bear the
same case. Since information about all arguments are projected along the head path, \ibox{2} would
also be available for an adjunct referring to it. So in the place of \emph{wita-jarra-rlu}
`small-\textsc{du}-\textsc{erg}' we could also have another adjunct referring to \emph{maliki}
`dog.\textsc{abs}'. This shows that even languages with constituent order as free as the Australian
languages can be handled within HPSG without assuming discontinuous constituents.\il{Warlpiri|)}

\section{Summary}

This paper discussed general approaches to constituent order in HPSG. On the one hand there are
approaches assuming flat constituent structure allowing permutation of daughters as long as no LP
constraints are violated and on the other hand, there are approaches assuming binary branching
structures. Approaches that assume flat structures can serialize the head to the left or to the
right or somewhere between other daughters in the structure. Approaches assuming binary branching
have to use other means. One such means is ``head movement'', which is analyzed as a series of
local dependencies by passing information about the missing head up along the head path. The
alternative to head movement is linearization of elements in special linearization domains, allowing
for discontinuous constituents. I showed that there are reasons for assuming head-movement for
German and how even languages with extremely free constituent order can be analyzed without assuming
discontinuous constituents. 

%\section*{Abbreviations}
\section*{Acknowledgements}

I thank Bob Borsley and Doug Ball for very detailed and very useful comments.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}



\if0
Bob:



It seems to me that whether or not constituent structures are confined to binary branching is quite important for constituent order. How far different constituent orders can be treated as a matter of alternative ordering of sisters depends on how much constituents are sisters. For example, the contrast between (1) and (2) might just show that PP sisters can appear in either order, but that is only possible if they are sisters.

 

(1) Kim talked to Lee about the weather.

(2) Kim talked about the weather to Lee.

 

I assume section 4 is concerned with verb-initial clauses. If so, perhaps the title should make that clear. For Minimalism head-movement is involved not just in verb-initial clauses but also VPs where the verb has two complements and nominal phrases where the noun precedes an attributive adjective, among other things.

 

Borsley (1989) was concerned with English, not Welsh. It made the point that you could have an analogue of verb-fronting for English auxiliary-initial sentences. Borsley (2006) discusses whether Welsh finite clauses involve some form of VP and argues that they do not and hence that they involve a flat structure.

 

Is 4.2 just about constructional approaches to English auxiliary-initial clauses (and not the English auxiliary system in general)? It’s not really clear. I think Pollard and Sag’s (1987) rule 3 and Pollard and Sag’s (1994) are essentially constructional approaches to verb-initial clauses. It is perhaps worth noting (at least briefly) that there has been a debate in versions of HPSG that distinguish between SUBJ and COMPS features about whether post-verbal subjects are realizations of the SUBJ feature like pre-verbal subjects or the COMPS feature like ordinary complements. The first view is adopted in Ginzburg and Sag (2000) and the second in Sag et al. (2003). I would see the first view as a constructional view (since it requires a special phrase type) and the second a lexical view (since it requires a lexical rule of some kind to derive appropriate lexical descriptions). Borsley (1989) argued that the second approach is appropriate for Welsh verb-initial clauses and Borsley (1995) argued that the first approach is right for Arabic verb-initial clauses.

 

Will you be saying much about extraposition phenomena? Chapter 1 currently has a brief reference to order domains, illustrating them with extraposition of relative clauses.

 

Will you say something about analyses of free constituent order languages with order domains?

 

 

REFERENCES

Borsley, R. D. (1989), An HPSG approach to Welsh, Journal of Linguistics 25, 333‑354.

Borsley, R. D. (1995), On some similarities and differences between Welsh and Syrian Arabic, Linguistics 33, 99-122.

Borsley R. D. (2006), On the nature of Welsh VSO clauses, Lingua 116, 462-490.

\fi


%      <!-- Local IspellDict: en_US-w_accents -->
