\documentclass[output=paper]{langsci/langscibook} 
\title{The evolution of HPSG}
\author{%
	Dan Flickinger\affiliation{Stanford University}%
	\and Carl Pollard\affiliation{Ohio State Universtiy}
	\lastand Tom Wasow\affiliation{Stanford University}%
}
% \chapterDOI{} %will be filled in at production

\epigram{Change epigram in chapters/01.tex or remove it there }
\abstract{Change the  abstract in chapters/02.tex \lipsum[4]}
\maketitle

\begin{document}

\label{chap-evolution}


\section*{Introduction} 

From its inception in 1983, HPSG was intended to serve as a framework for the formulation and implementation of natural language grammars which are (i) linguistically motivated, (ii)
formally explicit, and (iii) computationally tractable. These desiderata are reflective of HPSG's dual origins as an academic linguistic theory and as part of an industrial grammar implementation project with an eye toward potential practical applications. Here (i) means that the grammars are intended as scientific theories about the languages in question, and that the analyses the grammars give rise to are transparently relatable to the predictions (empirical consequences) of those theories. Thus HPSG shares the general concerns
of the theoretical linguistics literature, including distingushing between well-formed and ill-formed expressions and capturing
linguistically significant generalizations.  (ii) means that the notation for the grammars and its interpretation have a precise grounding in logic, mathematics, and theoretical computer science, so that there is never any ambiguity about the intended meaning of a rule or principle of grammar, and so that grammars have determinate empirical consequences. (iii) means that the grammars can be translated into computer programs that can handle linguistic
expressions embodying the full range of complex interacting phenomena that naturally occur in the target languages, and can do so with a
tolerable cost in space and time resources.

% HPSG, while 
% a direct descendent of Generalized Phrase Structure Grammar (GPSG), has been eclectic in its sources of ideas.  It was born in an industrial project to implement a GPSG for English, with an eye toward potential practical applications.  Throughout its history, it has drawn on academic work in linguistics, computer science, logic, philosophy, and (occasionally) psychology.  Its evolution has always been constrained by two fundamental design considerations:  (i) computational tractability and (ii) linguistic motivation.  Under (i) we include the requirements both that analyses be expressed with sufficient explicitness and precision to permit translation into computer code, and that such computational implementations be efficient enough to handle the kinds of sentences people use without running out of time or space.  By (ii) we mean that the analyses are supportable with the types of arguments employed in the theoretical linguistics literature.  These include distinguishing between well-formed and ill-formed sentences, and capturing linguistically significant generalizations.

The two principal architects of HPSG were Carl Pollard and Ivan Sag, but a great many other people made important contributions to its development.  Many, but by no means all, are cited in the chronology presented in the following sections.  There are today a number of groups of HPSG researchers around the world, in many cases involved in building HPSG-based computational systems.  While the number of practitioners is relatively small, it is a very active community that holds annual meetings and publishes quite extensively.  Hence, although Pollard no longer works on HPSG and Sag died in 2013, the theory is very much alive, and still evolving. 

\section{Precursors}

Head-driven Phrase Structure Grammar arose between 1983 and 1985 from the complex interaction between two lines of research in theoretical linguistics: (i) work on context-free generative grammar (CFG) intitiated in the late 1970's by Gerald Gazdar and Geoffrey Pullum, soon joined by Ivan Sag, Ewan Klein, Tom Wasow, and others, and collectively referred to as Generalized Phrase Structure Grammar (GPSG); and (ii) Carl Pollard's Stanford dissertation research, under Sag and Wasow's supervision, on generalized context-free grammar, and more specifically Head Grammar (HG).

\subsection{Generalized Phrase Structure Grammar}

In the earliest versions of generative grammar \citep{Chomsky57a}, the focus was on motivating transformations to express generalizations about classes of sentences.  In the 1960's, as generative linguists began to attend more explicitly to meaning, a division arose between those advocating using the machinery of transformations to capture semantic generalizations and those advocating the use of other types of formal devices.  This division became quite heated, and was subsequently dubbed ``the linguistic wars'' (see \citet{Newmeyer:1980}, Chapter 5).  Much of the work in theoretical syntax and semantics during the 1970's explored ways to constrain the power of transformations (see especially, \citet{Chomsky73a} and \citet{ChomLas1977}), and non-transformational approaches to the analysis of meaning (see especially \citet{Montague74a-ed} and \citet{Partee75a-u}).

These developments led a few linguists to begin questioning the central role transformations had played in the syntactic research of the preceding two decades (notably, \citet{Bresnan78a}).  This questioning of transformational grammar culminated in a series of papers by Gerald Gazdar, which (in those pre-internet days) were widely distributed as paper manuscripts.  The project that they laid out was succinctly summarized in one of Gazdar's later publications \citep{Gazdar81} as follows:

\begin{quote}
{\em Consider eliminating the transformational component of a generative grammar.}
\end{quote}

\noindent
The framework that emerged became known as Generalized Phrase Structure Grammar; a good account of
its development is Ted Briscoe's interview of Gazdar in November
2000. \footnote{\url{https://nlp.fi.muni.cz/~xjakub/briscoe-gazdar/}, 2018-08-21}

GPSG developed in response to several criticisms leveled against transformational grammar. First, TG was highly underformalized, to the extent that it was unclear what its claims---and the empirical consequences of those claims---amounted to; CFG, by comparison, was a simple and explicit mathematical formalism. Second, given the TG architecture of a context-free base together with a set of transformations, the claimed necessity of tranformations was standardly justified on the basis of arguments that CFGs were insufficiently expressive to serve as a general foundation for NL grammar; but Gazdar and Pullum showed all such arguments presented up to that point in time to be logically flawed or else based on false empirical claims. And third, closely related to the previous point, they showed that transformational grammarians had been insufficiently resourceful in exploiting what expressive power CFGs {\em did} possess; for example, coordinate constructions and unbounded dependency constructions had long served as prime examplars of the need for transformations, but Gazdar was able to show that both kinds of constructions, as well as interactions between them, did in fact yield to straightforward analysis within the framework of CFG.
 
Gazdar and Pullum's early work in this vein was quickly embraced by Sag and Wasow at Stanford University, both formally inclined former students of Chomsky's, who saw it as the logical conclusion of a trend in Chomskyan syntax toward constraining the tranformational component. That trend, in turn, was a response, at least in part, to (i) the demonstration by \citet{PR73a-u} that \citet{Chomsky65a}'s Standard Theory, when precisely formalized, was totally unconstrained, in the sense of generating all recursively enumerable languages; and (ii) the insight of \citet{Emonds76a-u} that most of the transformations proposed up to that time were ``structure- preserving'' in the sense that the trees they produced were isomorphic to ones that were base-generated. Besides directly addressing these issues of excess power and structure preservation, the hypothesis that NLs were context-free also had the advantage that CFGs were well-known by computer scientists to have decidable recognition problems and efficient parsing algorithms, facts which seemed to have some promise of bearing on questions of the psychological plausibility and computational tractability of the grammars in question.

Aside from serving as a framework for theoretical linguistic research, GPSG also provided the theoretical underpinnings for a natural language processing (NLP) project established in 1981 by Egon Loebner at Hewlett-Packard (HP) Laboratories in Palo Alto, not far from the Stanford campus. This project, which led in due course to the first computer implementation of HPSG, is described below.

\subsection{Head Grammar}

Pollard, with a background in pure mathematics, Chinese historical phonology, and 1930's-1950's-style American structural linguistics, arrived at Stanford in 1979 with the intention of getting a Ph.D. in Chinese linguistics, but was soon won over to theoretical syntax by Wasow and Sag. He had no exposure to Chomskyan linguistics, but was immediately attracted to the emerging nontransformational approaches, especially the early GPSG papers and the contemporaneous forms of CG due to Bach and Dowty, in part because of their formal simplicity and rigor, but also because the formalism of CFG was (and is) easy to read as a more technically precise rendering of structualist ideas about syntax (as presented, e.g., in \citet{Bloomfield33-u} and \citet{Hockett1958}).

Although \citet{PG82a-u-kopiert-original-pdf}) successfully refuted all published arguments to date that CFGs were inadequate for analyzing NLs, by the following year, Stuart Shieber had developed an argument (published in \citet{Shieber85a}), which was (and remains) generally accepted as correct, that there could not be a CFG that accounted for the cross-serial dependencies in Swiss German; and Chris Culy showed, in his Stanford M.A. thesis (cf. \citet{Culy85a}), that the presence of reduplicative compounding in Bambara precluded a CF analysis of that language. At the same time, Bach and Dowty (independently) had been experimenting with generalizations of traditional A-B (Ajdukiewicz-Bar Hillel) CG which allowed for modes of combining strings (such as reduplication, wrapping, insertion, cliticization, and the like) in addition to the usual concatenation. This latter development was closely related to a wider interest among nontransformational linguists of the time in the notion of discontinous constituency, and also had an obvious affinity to Hockett's item-and-process conception of linguistic structure, albeit at the level of words and phrases rather than morphemes. One of the principal aims of Pollard's dissertation work was to provide a general framework for syntactic (and semantic) analysis that went beyond---but not too far beyond---the limits of CFG in a way that took such developments into account. 

Among the generalizations of CFG that Pollard studied, special attention was given to head grammars, which differ fron CFGs in two respects: (i) the role of strings was taken over by headed strings, essentially strings with a designation of one of its words as its head; and (ii) besides concatenation, headed strings can also be combined by inserting one string directly to the left or right of another string's head. An appendix of his dissertation \citep{Pollard84a-u} provided an analysis of discontinuous constituency in Dutch, and that analysis also works for Swiss German. In another appendix, Pollard used a generalization of the CKY algorithm to prove that the head languages (HLs, the languages analyzed by HGs) shared with CFLs the property of deterministic polynnomial time recognition complexity, but of order $n^{7}$, subsequently reduced by \citet{Kasamietal1989} to $n^{6}$, as compared with order $n^{3}$ for CFLs. For additional formal properties of HGs, see \citet{Roach1987}. \citet{VijayWeir1994} proved that HGs had the same weak generative capacities as three other grammar formalisms (combinatory categorial grammars, lexicalized tree-adjoining grammars, and linear indexed grammars), and the corresponding class of languages became known as `mildly context sensitive'.

Although the handling of linearization in HG seems not to have been pursued further within the HPSG framework, the ideas that (i) linearization had to involve data structures richer than strings of phoneme strings, and (ii) that the way these structures were linearized had to involve operations other than mere concatenation, were implicit in subsequent HPSG work, starting with Pollard and Sag's Constituent Order Principle (which was really more of a promissory note than an actual principle).  These and related ideas would become more fully fleshed out a decade later within the linearization-grammar avatar of HPSG developed by \citet{Kathol95a}. On the other hand, two other innovations of HG, both related to the system of syntactic features, were incorporated into HPSG, and indeed should probably be considered the defining characteristics of that framework, namely the list-valued SUBCAT and SLASH features, discussed below.


\section{The HP NLP project}

Work on GPSG culminated in the 1985 book {\it Generalized Phrase Structure Grammar} by Gazdar,  Klein, Pullum, and Sag.  During the writing of that book, Sag taught a course on the theory, with participation of his co-authors.  The course was attended not only by Stanford students and faculty, but also by linguists from throughout the area around Stanford, including the Berkeley and Santa Cruz campuses of the University of California, as well as people from nearby industrial labs.  One of the attendees at this course was a programmer from Hewlett-Packard (HP) Laboratories in Palo Alto (about 3 kilometers from the Stanford campus) named Anne Paulson, who had some background in linguistics from her undergraduate education at Brown University.  Paulson told her supervisor at HP Labs about the course, saying she thought the theory could be implemented and might be turned into something useful.  Egon Loebner, the supervisor, was a multi-lingual engineer, with no background in linguistics; but he was intrigued.  He invited Sag to meet and discuss setting up a natural language processing project at HP, and Sag brought along Gazdar, Pullum, and Wasow.   This led to the creation of the project that eventually gave rise to HPSG.  Gazdar, who would be returning to England relatively soon, declined the invitation to be part of the new project, but Pullum had accepted a position at the University of California at Santa Cruz (about an hour's drive from Palo Alto).  So the project began with Sag, Pullum, and Wasow hired on a part-time basis to work with Paulson and two other HP programmers, John Lamping and Jonathan King, to implement a GPSG of English at HP Labs.  J. Mark Gawron, a linguistics graduate student from Berkeley who had attended Sag's course, was very soon added to the team.

The initial stages consisted of the linguists and programmers coming up with a notation that would serve the purposes of both.  Once this was accomplished, the linguists set to work writing a grammar of English on the DEC-20 mainframe computer that they all worked on.   The first publication coming out of this project was a 1982 Association for Computational Linguistics paper \citet{Gaw:Kin:Lam:82}.  The paper's conclusion begins:
\begin{quote}
What we have outlined is a natural language system that is a direct implementation of a linguistic theory.  We have argued that in this case the linguistic theory has the special appeal of computational tractability (promoted by its context-freeness), and that the system as a whole offers the hope of a happy marriage of linguistic theory, mathematical logic, and advanced computer applications. 
\end{quote}
This goal has been carried over into HPSG.

It should be mentioned that the HP group was by no means alone in these concerns.  The early 1980's was a period of rapid growth in computational linguistics (due at least in part to the rapid growth in the power and accessibility of computers).  In the immediate vicinity of Stanford and HP Labs, there were at least two other groups working on developing natural language systems that were both computationally tractable and linguistically motivated.  One such group was at the Xerox Palo Alto Research Center, where Ron Kaplan and Joan Bresnan (in collaboration with a number of other researchers, notably Martin Kay) were developing Lexical Functional Grammar; the other was at SRI International, where a large subset of SRI's artificial intelligence researchers  (including Barbara Grosz, Jerry Hobbs, Bob Moore, Hans Uszkoreit, Fernando Pereira, and Stuart Shieber) worked on natural language.  Thanks to the founding of the Center for the Study of Language and Information (CSLI) at Stanford in the early 1980's, there was a great deal of interaction among these three research groups.  Although some aspects of the work being done at the three non-Stanford sites were proprietary, most of the research was basic enough that there was a fairly free flow of ideas among the three groups about building linguistically motivated natural language systems.

Other projects seeking to develop theories that combined computational tractability with linguistic motivation were also underway outside of the immediate vicinity of Stanford, notably at the Universities of Pennsylvania and Edinburgh.  Aravind Joshi and his students were working on Tree Adjoining Grammars, while Mark Steedman and others were developing Combinatory Categorial Grammar.  Ideas from these projects also influenced the development of HPSG.

During the first few years of the HP NLP project, several Stanford students were hired as part-time help.   One was Pollard, who was writing his doctoral dissertation under Sag's supervision.  Ideas from his thesis work played a major role in the transition from GPSG to HPSG.  Two other students who became very important to the project were Dan Flickinger, a doctoral student in Linguistics, and Derek Proudian, who was working on an individually-designed undergraduate major when he first began at HP and later became a master's student in Computer Science.  Both Flickinger and Proudian became full-time HP employees after finishing their degrees.   Over the years, a number of other HP employees also worked on the project and made substantial contributions.  They included Susan Brennan, Lewis Creary, Marilyn Friedman (now Walker), Dave Goddeau, Brett Kessler, Joachim Laubsch, and John Nerbonne.  Brennan, Walker, Kessler, and Nerbonne all later went on to academic careers at major universities, doing research dealing with natural language processing.

The HP NLP project lasted until nearly the end of the 1980's.  By then, a fairly large and robust grammar of English had been implemented.  The period around 1990 combined an economic recession with what has sometimes been termed an ``AI winter'' -- that is, a period in which enthusiasm and hence funding for artificial intelligence research was at a particularly low ebb.  Since NLP was considered a branch of AI, support for it waned.  Hence, it was not surprising that the leadership of HP Labs decided to terminate the project.  Flickinger and Proudian came to an agreement with HP that allowed them to use the NLP technology developed by the project to launch a new start-up company, which they named Eloquent Software.  They were, however, unable to secure the capital necessary to turn the existing system into a product, so the company never got off the ground.

\section{The emergence of HPSG}

A few important features of GPSG that were later carried over into HPSG are worth mentioning here. First, GPSG borrowed from Montague the idea that each phrase structure rule was to be paired with a semantic rule providing a recipe for computing the meaning of the mother from the meanings of its daughters; this design feature was shared with contemporaneous forms of categorial grammar (CG) being studied by such linguists as Emmon Bach and David Dowty.  Second, the specific inventory of features employed in GPSG for making fine-grained categorial distinctions (such as case, agreement, verb inflectional form, and the like), was largely preserved, though the technical implementation of morphosyntactic features in HPSG was somewhat different. And third, the SLASH feature, which originated in \citet{Gazdar81}'s derived categories (e.g. S/NP), and which was used to keep track of unbounded dependencies, was generalized in HPSG to allow for multiple unbounded dependencies (as in the notorious violins-and-sonatas examples). As will be discussed below, this SLASH feature bears a superficial---and misleading---resemblance to the categorial grammar connectives written as / and $\backslash$. On the other hand, a centrally important architectural feature of GPSG absent from HPSG (and from HG) was the device of metarules, higher-order rules used to generate the full set of context-free phrase structure rules (PSRs) from an initial inventory of basic PSRs. Among the metarules were ones used to introduce non-null SLASH values and propagate them upward through trees to a position where they were discharged by combination with a matching constituent called a filler (analogous to a wh-moved expression in TG).

A note is in order about the sometimes confusing use of the names  `Head Grammar (HG)' and `HPSG'. Strictly speaking, HG was a specific subtype of generalized CFG developed in Pollard's dissertation work, but the term `HG' did not appear in academic linguistic publications with the exception of the \citet{PollardSag1983} WCCFL paper, which introduced the distinction between head features and binding features (the latter were incorporated into GPSG under the name `foot features'). In summer 1982, Pollard had started working part time on the HP NL project; and the term `HPSG' was first employed (by Pullum) in reference to an extensive reworking by Pollard and Anne Paulson of the then-current HP GPSG implementation, incorporating some of the main features of Pollard's dissertation work in progress, carried out over the summer of 1983, while much of the HP NLP team (including Pullum and Sag) was away at the LSA Institute in Los Angeles.  The implication of the name change was that whatever this new system was, it was no longer GPSG.

Once this first HPSG implementation was in place, the NLP work at HP was considered to be within the framework of HPSG, rather than GPSG. After Pollard completed his dissertation, he continued to refer to `HG' in invited talks as late as autumn 1984; but his talk at the (December~1984) LSA Binding Theory Symposium used `HPSG' instead, and after that, the term `HG' was supplanted by `HPSG' (except in publications by non-linguists about formal language theory). One additional complication is that until the \citet{GKPS85a} volume appeared, GPSG and HPSG were developing side by side, with  considerable interaction.  Pollard, together with Flickinger, Wasow, Nerbonne, and others did HPSG; Gazdar, Pullum, and Klein did GPSG; and Sag worked both sides of the street.

HPSG papers, about both theory and implementation, began to appear in 1985, starting with Pollard's WCCFL paper `Phrase structure grammar without metarules' \citep{Pollard85a-u}, and his paper at the categorial grammar conference in Tucson \citep{Pollard88a}, comparing and contrasting HPSG with then-current versions of categorial grammar (CG) due to Bach, Dowty, and Steedman. These were followed by a trio of ACL papers documenting the current state of the HPSG implementation at HP Labs: \citet{creary-pollard:1985:ACL}, \citet{FPW85a}, and \citet{PP85}. Of those three, the most significant in terms of its influence on the subsequent development of the HPSG framework was the second, which showed how the lexicon could be (and in fact was) organized using multiple- inheritance knowledge representation; Flickinger's Stanford dissertation \citep{Flickinger87} was an in-depth exploration of that idea.

\section{Early HPSG}

Setting aside implementation details, early HPSG can be characterized by the following architectural features:\\

\noindent
{\bf Elimination of metarules}. Although metarules were a central feature of GPSG, they were also problematic: \citet{UszPet1982} had shown  that if metarules were allowed to apply to their own outputs, then the resulting grammars were no longer guaranteed to generate CFLs; indeed, such grammars could generate all recursively enumerable languages. And so, in GPSG, the closure of a set of base phrase structure rules (PSRs) under a set of metarules was defined in such a way that no metarule could apply to a PSR whose own derivation involved an application of that metarule. This definition was intended to ensure that the closure of a finite set of PSRs remained finite, and therefore still constituted a CFG.

So, for example, the metarule STM1 was used in GPSG to convert a PSR into another PSR one of whose daughters is [+NULL] (informally speaking, a `trace'), and feature cooccurence restrictions (FCR) guaranteed that such daughters would bear a SLASH value, and that this SLASH value would also appear on the mother. Unfortunately, the finite closure definition described above does not preclude the possibility of derived PSRs whose mother carries multiple, in fact unboundedly many
SLASH values (e.g. NP/NP, (NP/NP)/NP, etc.). And this in turn leads to an infinite set of PSRs, outside the realm of CF-ness.  Of course, one could rein in this excess power by imposing another FCR that disallows categories of the form (X/Y)/Z; but then there is no way to analyze sentences containing a constituent with two undischarged unbounded dependencies, such as the VP complement of {\em easy} in the following example:

\ea
\label{ex-violins}
Violins this finely crafted, even the most challenging sonatas are easy to [play \_ on \_]
\z

\noindent
GPSG avoided this problem by not analyzing such examples. In HPSG, by contrast, such examples were analyzed straightforwardly by replacing GPSG's category-valued SLASH feature with one whose values were lists (or sets) of categories. This approach still gave rise to an infinite set of rules, but since maintaining context-freeness was no longer at stake, this was not seen as problematic. The infinitude of rules in HPSG arose not through a violation of finite closure (since there were no longer any metarules at all), but because each of the handful of schematic PSRs (see below) could be directly instantiated in an infinite number of ways, given that the presence of list-valued
features gave rise to an infinite set of categories.\\

\noindent
{\bf Lexical rules}. GPSG, generalizing a suggestion of \citet{Flickinger1983}, constrained metarules to apply only to PSRs that introduced a lexical head. \citet{Pollard85a-u} took this idea a step further, noting that many proposed metarules could be reformulated as lexical rules that (among other effects) operated on the subcategorization frames (encoded by the SUBCAT feature discussed below) of lexical entries. Indeed, lexical rules of essentially this kind were already employed by Bach, Dowty, and other categorial grammarians. Examples of constructions handled by metarules in GPSG but in HPSG by lexical rules included sentential extraposition, subject extraction, and passive. \citet{FPW85a} argued for an architecture for the lexicon that combined lexical rules with multiple inheritance using a
frame-based knowledge representation system, on the basis of both overall grammar simplicity and efficient, easily modifiable implementation.\\

\noindent
{\bf CG-like treatment of subcategorization}. In GPSG, subcategorization was treated by an integer-valued feature called SUBCAT that in effect indexed each lexical item with the rule that introduced and provided its subcategorization frame; e.g. {\em weep} was listed in the lexicon with SUBCAT value 1 while {\em devour} was listed with SUBCAT value 2, and then PSRs of roughly the form

\begin{center}
\begin{tabular}{l}
          VP $\rightarrow$ V[SUBCAT 1] \\
          VP $\rightarrow$ V[SUBCAT 2] NP
\end{tabular}
\end{center}

\noindent
guaranteed that lexical heads would have the right kinds of complements.  In HPSG, by contrast, the SUBCAT feature directly characterized the grammatical arguments selected by a sign (not just the complements, but the subject too) as a list of categories, so that e.g.~{\em weep} was listed as V[SUBCAT <NP>] but {\em devour} as V[SUBCAT <NP,NP>] (where the first occurrence of `NP' refers to the object and the second to the subject). This treatment of argument selection was inspired by categorial grammar, where the same verbs would have been categorized as NP$\backslash$S and (NP$\backslash$S)/NP respectively; the main differences are that (i) the CG treatment also encodes the directionality of the argument relative to the head, and (ii) in HPSG, all the arguments appear on one list, while in CG they are `picked up' one at a time, with as many
connectives (/ or $\backslash$) as there are arguments. In particular, as in the CG of \citet{Dowty82b-ohne-crossref}, the subject was defined as the last argument, except that in HPSG, `last' now referred to the rightmost position on the SUBCAT list, not to the most deeply embedded connective. In HPSG, this
ordering of the categories on the SUBCAT list was related not just to CG, but also to the traditional grammatical notion of obliqueness, and also to the accessibility hierarchy of \citet{KC77a}.\\

\noindent
{\bf Schematic rules}. Unlike CFG, but like CG, HPSG had only a handful of schematic rules. For example, in \citet{Pollard85a-u}, a substantial chunk of English `local' grammar (i.e.~leaving aside unbounded dependencies) was handled by three rules: (i) a rule (used for subject-auxiliary inversion) that forms a sentence from an inverted (+INV) lexical head and all its arguments; (ii) a rule that forms a phrase from a head with SUBCAT list of length > 1 together with all its non-subject arguments; and (iii) a rule that forms a sentence from a head with a SUBCAT value of length 1 together with its single (subject) argument.\\

\noindent
{\bf List- (or set-) valued SLASH feature}. The list-valued SLASH was introduced in \citet{Pollard85a-u} to handle multiple unbounded dependencies, in favor of the GPSG category-valued SLASH (which in turn originated as the `derived categories' of  \citet{Gazdar81}, e.g.~S/NP). In spite of the notational similarity, though, the PSG SLASH is not an analog of the CG slashes / and $\backslash$ (though HPSG's SUBCAT is, as
explained above). In fact, HPSG SLASH has no analog in the kinds of CG's being developed by Montague semanticists such as Bach and Dowty in the late 1970's and early 1980's, which followed the CGs of \citet{Bar-Hillel54-u-kopiert} in having only rules for eliminating (or canceling) slashes:

\begin{center}
\begin{tabular}{ccc}
A A$\backslash$B  & &     B/A A \\ \cline{1-1} \cline{3-3}
B & & B 
\end{tabular}
\end{center}

\noindent
To find an analog to HPSG SLASH in CG, we have to turn to the kinds of CGs invented by \citet{Lambek1958}, which unfortunately were not yet well-known to linguists (though that would soon change starting with Lambek's appearance at the 1985 categorial grammar conference in Tucson). What sets apart grammars 
of this kind (and their elaborations by Moortgat, Oehrle, Morrill, and many others), is the existence of rules for hypothetical proof (not given here), which allow a hypothesized category occurrence introduced into a tree (thought of as a proof) to be discharged. 

In the Gentzen style of natural deduction, hypothesized categories are written to the left of the symbol |- (turnstile), so that the two slash elimination rules above take the following form (where $\Gamma$ and $\Delta$ are  lists of categories, and comma represents list concatenation:

\begin{center}
\begin{tabular}{ccc}
$\Gamma$ $\vdash$ A \ \ $\Delta$ $\vdash$ A$\backslash$B  & &     $\Gamma$ $\vdash$ B/A \ \ $\Delta$ $\vdash$ A \\ \cline{1-1} \cline{3-3}
$\Gamma$,$\Delta$ $\vdash$ B & & $\Gamma$,$\Delta$ $\vdash$ B 
\end{tabular}
\end{center}

\noindent
These rules propagate hypotheses (analogous to linguists' traces) downward through the proof tree (downward because logicians' trees are
upside down with the conclusion (`root') at the bottom). In HPSG notation, these same rules can be written as one rule (since SUBCAT is
nondirectional):

\begin{center}
\begin{tabular}{c}
B[SUBCAT <..., A>,SLASH $\Gamma$] \ \ A[SLASH $\Delta$] \\ \cline{1-1}
B[SUBCAT <...>][SLASH $\Gamma$,$\Delta$]
\end{tabular}
\end{center}

This in turn is a special case of an HPSG principle first known as the Binding Inheritance Principle (BIP) and later as the Nonlocal Feature Principle (binding features included SLASH as well as the features QUE
and REL used for tracking undischarged interrogative and relative pronouns). The original statement of the BIP (Pollard HPSG tutorial at ICOT, July 1986) treated SLASH as set- rather than list-valued):

\begin{quote}
The value of a binding feature on the mother is the union of the values of that feature on the daughters.
\end{quote}

\noindent
For example, the doubly-gapped VP in the violins-and-sonatas example is analyzed in HPSG roughly  as
is shown in Figure~\ref{fig-play-on}
\begin{figure}
\centerfit{%
\begin{forest}
sm edges
[{V[SUBCAT <NP>,SLASH <NP,NP>]}
  [{V[SUBCAT <PP,NP,NP>]} [play]]
  [{NP[SLASH <NP>]} [t]]
  [{PP[SLASH <NP>]}
    [{P[SUBCAT <NP>]} [on]]
    [{NP[SLASH <NP>]} [t]]]]
\end{forest}
}
\caption{\label{fig-play-on}\emph{play on} as part of \emph{Violins this finely crafted, even the most challenging sonatas are easy to play on.}}
\end{figure}
and essentially the same way in Lambek-style CG:

\begin{center}
\begin{tabular}{ccccccc}
\multicolumn{2}{c}{\em play} & {\em t} & & {\em on} & \ \ \ \ \ \ {\em t} & \\
\multicolumn{2}{c}{$\vdash$ ((NP$\backslash$S)/PP)/NP} & NP $\vdash$ NP & & $\vdash$ PP/NP & \multicolumn{2}{c}{NP $\vdash$ NP} \\ \cline{1-3} \cline{5-7}
 & \multicolumn{2}{c}{NP $\vdash$ (NP$\backslash$S)/PP} & & \multicolumn{3}{c}{NP $\vdash$ PP} \\ \cline{2-6}
\multicolumn{7}{c}{NP,NP $\vdash$ S}
\end{tabular}
\end{center}

\noindent
Aside from the binary branching of the Lambek analysis, the main difference is that HPSG traces of the form A[SLASH <A>] correspond
to Lambek axioms of the form A |- A, which is the standard mechanism for introducing hypotheses in Gentzen-style natural deduction.

An overview and elaboration of early HPSG is provided by the two books \citet{ps} and \citet{ps2}. Confusingly, the former is called \textit{Information-Based Syntax and Semantics, Volume 1: Fundamentals}, and the second simply \textit{Head-Driven Phrase Structure Grammar} (not \textit{Information-Based Syntax and Semantics, Volume 2}). The reason for the title change had to do with a change in the underlying mathematical theory of feature structures. In the first book, following work in theoretical computer science by \citet{RoundsKasper1986} and \citet{MoshierRounds1987}, feature structures were treated as data structures that supplied partial information about the linguistic objects being theorized about; this perspective in turn was based on \citet{Scott1982}'s mathematical theory of computation in terms of what he called information systems. Subsequently, Paul King persuaded Pollard and Sag that it was more straightforward to distinguish between feature structures, thought of as formal models of the linguistic objects, and feature descriptions or formulas of feature logic, which provided partial information about them, as described in his Manchester dissertation \citep{King89}.  Although the formal issues involved in distingishing between the two approaches are of interest in their own right, they seem not to have had a lasting effect on how theoretical linguists used HPSG, or on how computational linguists implemented it. As for subject matter, \citet{ps} was limited to the most basic notions, including syntactic features and categories (including the distinction between head features and binding features); subcategorization and the distinction between complements and adjuncts (the latter of which necessitated one more rule schema beyond the three proposed by \citet{Pollard85a-u}); basic principles of grammar (especially the Head Feature Principle and the Subcategorization Principle); the obliqueness order and constituent ordering; and the organization of the lexicon by means of a multiple inheritance hierarchy and lexical rules. \citet{ps2} used HPSG to analyze a wide range of phenomena that had figured prominently in the syntactic literature of the 1960s--1980s, including agreement, expletive pronoun constructions, raising, control, filler-gap constructions (including island constraints and parasitic gaps); so-called binding theory (the distribution of reflexive pronouns, nonreflexive pronouns, and non-pronominal NPs), and scope of quantificational NPs.

\section{The LinGO Project}

In the early 1990's, a consortium of research centers in Germany secured funding from the German government for a large project in spoken language machine translation, called Verbmobil \citep{Wahlster2000a-ed}, which aimed to combine a variety of methods and frameworks in a single implemented state-of-the-art demonstrator system.  Grammars of German and English were to be implemented in HPSG, to be used both for parsing and for generation in the translation of human-human dialogues, with the German grammar to be developed at the German AI Research Center (DFKI), coordinator for the Verbmobil project.  The DFKI contracted in 1993 with Sag at CSLI to design and implement the English grammar, with Flickinger brought over from HP Labs to help lead the effort, forming a new research group at CSLI initially called ERGO (for English Resource Grammar Online), later generalized to the name LinGO (Linguistic Grammars Online).  Early LinGO members included Wasow and linguistics graduate student Rob Malouf, who authored the initial implementation for the English Resource Grammar (ERG), along with two other linguistics graduate students: Kathryn Campbell-Kibler, who contributed to the development of the lexicon, and Tony Davis, who helped in refining the lexical type heirarchy.

During the first of the two four-year phases of Verbmobil, project focus was on designing and implementing core syntactic and semantic analyses, initially using the DISCO/PAGE platform developed at the DFKI, and largely informed by the framework presented in \citet{ps2}.  However, a more computationally useful semantic formalism emerged, called Minimal Recursion Semantics (MRS: \citet{mrs}), which Ann Copestake, formerly of the European ACQUILEX project, helped to design.  Copestake also expanded the LKB system \citep{Copestake2002a-Short} which had been used in ACQUILEX, to serve as the grammar development environment for the LinGO project, including both a parser and a generator for typed feature structure grammars.

The second four years of the Verbmobil project emphasized development of the generation capabilities of the ERG, along with steady expansion of linguistic coverage, and elaboration of the MRS framework.  LinGO contributors in this phase, in addition to Sag, Wasow, Flickinger, Malouf, and Copestake, included Stanford Linguistics graduate students Emily Bender and Susanne Riehemann, along with a regular visitor and steady contributor from the DFKI, Stephan Oepen.  Verbmobil had meanwhile added Japanese alongside German and English for more translation pairs, giving rise to another relatively broad-coverage HPSG grammar, JaCY, authored by Melanie Siegel at the DFKI.  Work continued at the DFKI, of course, on the German HPSG grammar, which had been authored initially by Klaus Netter, then revised and extended by Stefan M{\"u}ller and Walter Kasper.

Before the end of Verbmobil funding in 2000, the LinGO project had already begun to diversify into other application and research areas using the ERG, including over the next several years work on augmented/adaptive communication, multiword expressions, and hybrid processing with statistical methods, variously funded by the National Science Foundation, the Scottish government, and industrial partners including IBM and NTT.  At the turn of the millenium, Flickinger joined the software start-up boom, co-founding YY Software funded through substantial venture capital to use the ERG for automated response to customer emails for e-commerce companies.  YY produced the first commercially viable software system using an HPSG implementation, processing email content in English with the ERG and the PET parser \citep{callmeier00} which had been developed by Ulrich Callmeier at the DFKI, as well as in Japanese with JaCY, further developed by Siegel and by Bender.  While technically capable, the product was not commercially successful enough to enable YY to survive the bursting of the dot-com bubble, and it closed down in 2003.  Flickinger returned to the LinGO project with a considerably more robust ERG, and soon picked up the translation application thread again, this time using the ERG for generation in the LOGON Norwegian-English MT project based in Oslo.


\section{Research and Teaching Networks}

The first international conference on HPSG was held in 1993 in Columbus, Ohio, in conjunction with
the Linguistic Society of America's Summer Institute.  The conference has been convened every year
since then with the exception of 1995, with locations in Europe, Asia, and North America.  Two of
these annual meetings have been held jointly with the annual Lexical Functional Grammar conference,
in 2000 in Berkeley and in 2016 in Warsaw.  Proceedings of these conferences since 2000 are
available on-line from CSLI
Publications. \footnote{\url{http://web.stanford.edu/group/cslipublications/cslipublications/HPSG}, 2018-08-21.}  Since 2012, HPSG researchers in Europe have also held a regional workshop almost every year either in Frankfurt or in Paris, to foster informal discussion of current work in HPSG.  These follow in the footsteps of European HPSG workshops starting with one on German grammar, held in Saarbr{\"u}cken in 1991, and including others in Edinburgh and Copenhagen in 1994, and in T{\"u}bingen in 1995.

In 1994, the HPSG mailing list was initiated, \footnote{Links to its archives can be found at
  \url{https://hpsg.hu-berlin.de/HPSG/MailingList}.} and from 1996 to 1998, the electronic
newsletter, the HPSG Gazette, was
distributed, \footnote{\url{http://www.sfs.uni-tuebingen.de/~gazette}, 2018-08-21.} with its function then taken over by the HPSG mailing list.

Courses introducing HPSG to students became part of the curriculum during the late 1980's and early 1990's at universities in Osaka, Paris, Saarbr{\"u}cken, Seoul, and T{\"u}bingen, along with Stanford and OSU.  Summer courses and workshops on HPSG have also been offered since the early 1990's at the LSA Summer Institute in the U.S., including a course by Sag and Pollard on binding and control in 1991 in Santa Cruz, and at the European Summer School in Logic, Language and Information (ESSLLI), including a workshop in Prague in 1996 on Romance (along with two HPSG-related student papers at the first-ever ESSLLI student session), and courses in 1998 in Saarbr{\"u}cken on Germanic syntax, on grammar engineering, and on unification-based formalisms.
    

\section{Implementations and Applications of HPSG}

The first implementation of a grammar in the HPSG framework emerged in the Hewlett-Packard Labs natural language project, for English, with a lexical type hierarchy \citep{FPW85a}, a set of grammar rules that provided coverage of core syntactic phenomena including unbounded dependencies and coordination, and a semantic component called Natural Language Logic \citep{LaubNerb1991}.  The corresponding parser for this grammar was implemented in Lisp \citep{PP85}, as part of a system called HP-NL \citep{NerProud1987} which provided a natural language interface for querying relational databases.  The grammar and parser were shelved when HP Labs terminated their natural langauge project in 1993, leading Sag and Flickinger to begin the LinGO project and development of the English Resource Grammar at Stanford.

By this time, grammars in HPSG were being implemented in university research groups for several other languages, using a variety of parsers and grammar engineering platforms for processing typed feature structure grammars.  Early platforms included the DFKI's DISCO system \citep{DISCO94} with a parser and graphical development tools, which evolved to the PAGE system; Bob Carpenter and Gerald Penn's ALE system \citep{CP96}, which evolved in T{\"u}bingen to TRALE; and Ann Copestake's LKB \citep{Copestake2002a-Short} which grew out of the ACQUILEX project.  Other early systems included ALEP within the Eurotra project \citep{SimpGron1994}, ConTroll at T{\"u}bingen \citep{GoetzMeurers1997}, CUF at IMS in Stuttgart \citep{DD93a-u}, ProFIT at the University of Saarland \citep{Erbach95a}, CL-ONE at Edinburgh \citep{Manandhar1994}, TFS also at IMS \citep{Emele94a-u}, and HDrug at Groningen \citep{NB97b-u}.  

Relatively early broad-coverage grammar implementations in HPSG, in addition to the English Resource Grammar at Stanford \citep{erg}, included one for German at the DFKI \citep{MK2000a} and one for Japanese (Jacy: \citet{Siegel2000a}), all used in the Verbmobil machine translation project; a separate German grammar \citep{Babel}; a Dutch grammar in Groningen \citep{BvNM2001a-u}; and a separate Japanese grammar in Tokyo \citep{MNT2005a-u}.  Moderately large HPSG grammars were also developed during this period for Korean \citep{KY2003a-u} and Polish \citep{myk:etal:02}.  

In 1999, research groups at the DFKI, Stanford, and Tokyo set up a consortium called DELPH-IN
(Initiative for Deep Linguistic Processing in HPSG), to foster broader development of both grammars
and platform components, described in \citet{OFTU2002a-ed}.  Over the next two decades, substantial
DELPH-IN grammars were developed for Norwegian, Porguguese, and Spanish, along with
moderate-coverage grammars for Bulgarian, Greek, Hausa, Hebrew, Indonesian, Mandarin Chinese, Thai,
and Wambaya, all described at \url{http://delph-in.net}.  Several of these grammars are based on the
Grammar Matrix \citep{BFO2002a-u}, a starter kit generalized from the ERG and Jacy for rapid
prototyping of HPSG grammars, along with a much larger set of coursework
grammars. \footnote{\url{http://moin.delph-in.net/MatrixTop}, 2018-08-21.} Moderate-coverage grammars developed in the TRALE system include Mandarin Chinese \citep{ML2013a}, Georgian \citep{Abzianidze2011a-u}, Persian \citep{MG2010a}, and Spanish \citep{Machicao-y-Priemer2015a}.

These grammars and systems have been used in a wide variety of applications, primarily as vehicles for research in computational linguistics, but also for some commercial software products.  Research applications already mentioned include database query (HP Labs) and machine translation (Verbmobil and LOGON), with additional applications developed for use in anthology search \citep{Sch:Kie:Spu:11}, grammar tutoring in Norwegian \citep{Hellanetal:13}, ontology acquisition \citep{Herb:Cope:06}, virtual robot control \citep{packard2014uw}, visual question answering \citep{DBLP:journals/corr/KuhnleC17}, and logic instruction \citep{Flickinger:17}, among many others.  Commercial applications include ecommerce customer email response (for YY Software), and grammar correction in education (for Redbird Advanced Learning, now part of McGraw-Hill Education: \citet{Suppesetal:12}).

For most practical applications, some approximate solution to the challenge of parse selection (disambiguation) must be provided, so several of the DELPH-IN grammars, including the ERG, follow the approach of \citet{OFTM2004a-u-platte}, which uses a manually-annotated treebank of sentences parsed by a grammar to train a statistical model which is applied at run-time to identify the most likely analysis for each parsed sentence.  These treebanks can also serve as repositories of the analyses intended by the grammarian for the sentences of a corpus, and some resources, notably the Alpino treebank \citep{BvNM2001a-u}, include analyses which the grammar may not yet be able to produce automatically.  

\section{Conclusion}

As we noted early in this chapter, HPSG's origins are rooted in the desire simultaneously to address the theoretical concerns of linguists and the practical issues involved in building a useful natural language processing system.  In the decades since the birth of HPSG, the mainstream of work in both theoretical linguistics and in NLP developed in ways that could not have been anticipated at the time.  NLP is now dominated by statistical methods, with almost all practical applications making use of machine learning technologies.  It is hard to see any influence of research by linguists in most NLP systems.  Mainstream grammatical theory, on the other hand, is now dominated by the Minimalist Program (MP), a vaguely formulated\footnote{Most work in MP is presented without rigorous definitions of the technical apparatus, but Edward Stabler and his collaborators have written a number of papers aimed at formalizing MP.  See in particular \citet{CollStab2016}.} variant of Categorial Grammar (see \citet{RetStab2004} for a comparison).  Concern with computational implementation plays virtually no role in MP research.  

It might seem, therefore, that HPSG is further from the mainstream of both fields than it was at its inception, raising questions about how realistic the objectives of HPSG are.  We believe, however, that there are grounds for optimism.

With regard to implementations, there is no incompatibility between the use of HPSG and the machine learning methods of mainstream NLP.  Indeed, as noted above, HPSG-based systems that have been put to practical use have necessarily included components induced via statistical methods from annotated corpora.  Without such components, the systems cannot deal with the full variety of forms encountered in usage data.  On the other hand, existing NLP systems that rely solely on machine learning from corpora do not exhibit anything that can reasonably be called understanding of natural language.  Current technologies for machine translation, automatic summarization, and various other linguistic tasks fall far short of what humans do on these tasks, and are useful primarily as tools to speed up the tasks for the humans carrying them out.  Many NLP researchers are beginning to recognize that developing software that can plausibly be said to understand language will require representations of linguistic structure and meaning like those that are the stock in trade of linguists.

Evidence for a renewed interest in linguistics among NLP researchers is the fact that major technology companies with natural language groups have recently begun (or in some cases, resumed) hiring linguists, and increasing numbers of new linguistics PhDs have taken jobs in the software industry.  

In the domain of theoretical linguistics, it is arguable that the distance between HPSG and the mainstream of grammatical research (that is, MP) has narrowed, given that both crucially incorporate ideas from categorial grammar.  Rather than trying to make that argument, however, we will point to connections that HPSG has made with other work in theoretical linguistics.  Perhaps the most obvious of these is the work of Peter Culicover and Ray Jackendoff on what they call ``Simpler Syntax.''  Their influential 2005 book with that title \citep{CJ2005a} argues for a theory of grammar that differs little in its architecture and motivations from HPSG.

More interesting are the connections that have been forged between research in HPSG and work in Construction Grammar (CxG).  \citet{Fillmore88a} describes this approach as follows:
\begin{quote}
Construction grammars differ from phrase-structure grammars which use {\bf complex symbols} and allow the {\bf transmission of information} between lower and higher structural units, in that we allow the direct representation of the required properties of subordinate constituents.  (Should it turn out that there are completely general principles for predicting the kinds of information that get transmitted upwards or downwards, this may not be a real difference.)
\end{quote}
This (admittedly vague) description would include HPSG as a form of CxG.  Nevertheless, work that labels itself CxG tends to look very different from HPSG.  This is in part because of the difference in their origins: many proponents of CxG come from the tradition of ``cognitive grammar'' or typological
studies, whereas HPSG's roots are in computational concerns. Hence, most of the CxG literature is not precise enough to allow a straightforward comparison with HPSG.

In the last years of his life, Ivan Sag sought to bridge this gap through collaboration with construction grammarians at the University of California at Berkeley, particularly Chuck Fillmore, Paul Kay, and Laura Michaelis.  He developed a theory he called ``Sign-Based Construction Grammar'' (SBCG), which would combine the insights of CxG with the explicitness of HPSG.  \citet{Sag2012a} wrote, ``To readers steeped in HPSG theory, SBCG will no doubt seem like a minor variant of constructional HPSG.''  Indeed, despite the name change, the main feature of SBCG that differs from HPSG is that it posits an inheritance hierarchy of constructs, which includes feature structure descriptions for such partially lexicalized multi-word expressions as Ved - X's - {\it way} - PP, instantiated in such VPs as {\it ad-libbed his way through a largely
secret meeting}.  While this is a non-trivial extension to HPSG, there is no fundamental change to the technical machinery.  Indeed, it has been a part of the LinGO implementation for years. 

Finally, another point of convergence between work in HPSG and other work in both theoretical linguistics and NLP is the increasing importance of corpus data. In the early years of the HP NLP project, the methodology was the same as that employed in almost all work in theoretical syntax and semantics:  the grammar was based entirely on examples invented by the researchers.  At one point during the decade of the HP NLP project, Flickinger, Wasow, and Pullum compiled a list of sentences intended to exemplify many of the sentence types that they hoped the system would eventually be able to analyze.  That list, 1328 sentences long, continues to be useful as a test suite for the LinGO system and various other NLP groups.  But it does not come close to covering the variety of sentence forms that are found in corpora of speech and various written genres.  As the goals of the HPSG implementations have broadened from database query to dealing with ``language in the wild,'' the use of corpora to test such systems and motivate extensions to them has increased.  This parallels a development in other areas of linguistics, which have also increasingly made use of large on-line corpora as sources of data and tests of
their theories.  This is a trend that we expect will continue.
 
In short, there are signs of convergence between work on HPSG and work in other areas, and it seems plausible to think that the market for HPSG research will grow in the future.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}
