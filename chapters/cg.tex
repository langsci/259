\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.5599876}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[31]
}{}

\author{Yusuke Kubota\orcid{0000-0002-8468-5857}\affiliation{National Institute for Japanese Language and Linguistics}}
\title{HPSG and Categorial Grammar}


\abstract{This chapter aims to offer an up-to-date comparison of HPSG and Categorial Grammar (CG). Since the CG research itself consists of two major types of approaches with overlapping but distinct goals and research strategies, I start by giving an overview of these two variants of CG. This is followed by a comparison of HPSG and CG at a broad level, in terms of the general architecture of the theory, and then, by a  more focused comparison of specific linguistic analyses of some selected phenomena. The chapter ends by briefly touching on issues related to computational implementation and human sentence processing. Throughout the discussion, I  attempt to highlight both the similarities and differences between HPSG and CG research, in the hope of stimulating further research in the two research communities on their respective open questions, and so that the two communities can continue to learn from each other.}


\begin{document}
\maketitle
\label{chap-cg}

\vspace{-\baselineskip}
\section{Introduction}

%\largerpage
The goal of this chapter is to provide a comparison between HPSG and
Categorial\indexcg Grammar (CG). The two theories share certain important
insights, mostly due to the fact that they are among the so-called
\emph{lexicalist}, \emph{non"=transformational} theories of syntax that were
proposed as major alternatives to the mainstream transformational
syntax in the 1980s (see \citealt{BB2011a-ed} and \citealt{MuellerGT-Eng3}
for overviews of these 
theories). However, due to the differences in the main research goals
in the respective communities in which these approaches have been
developed, there are certain nontrivial differences between them as
well. The present chapter assumes researchers working in HPSG or other
non-CG theories of syntax as its main audience, and aims to inform
them of key aspects of CG which make it distinct from other theories
of syntax. While computational implementation and investigations of the
formal properties of grammatical theory have been important in both
HPSG and CG research, I will primarily focus on the linguistic aspects
in the ensuing discussion, with pointers (where relevant) to literature on mathematical
and computational issues. Throughout the discussion, I presuppose
basic familiarity with HPSG (with pointers to relevant chapters in the
handbook). The present handbook contains chapters that compare HPSG
with other grammatical theories, including the present one. I
encourage the reader to take a look at the other theory comparison
chapters too (as well as other chapters dealing with specific
aspects of HPSG in greater detail), in order to obtain a fuller picture of the
theoretical landscape in current (non-transformational) generative
syntax research.

The rest of the chapter is structured as follows. I start by giving an
overview of Combinatory Categorial Grammar and
Type-Logical Categorial Grammar, two major variants of CG (Section~\ref{cgexposition}).
This is followed by a comparison of HPSG and CG at a broad level, in
terms of the general architecture of the theory
(Section~\ref{architecture}), and then, by a more focused comparison
of specific linguistic analyses of some selected phenomena
(Section~\ref{phenomena}). The chapter ends by briefly touching on
issues related to computational implementation and human sentence
processing (Section~\ref{implementation}).
 

\section{Two varieties of CG \label{cgexposition}}

\largerpage
CG is actually not a monolithic theory, but is a family of related
approaches \emdashUS or, perhaps more accurately, it is much \emph{less of} a
monolithic theory than either HPSG or Lexical Functional Grammar (LFG;
\citealt{KB82a-u,BATW2015a}; \crossrefchapteralt{lfg}) is. For this reason, I will
start my discussion by sketching some important features of two major
varieties of CG, Combinatory Categorial Grammar\indexccg (CCG;
\citealt{Steedman2000a-u,steedman2012}) and Type-Logical Categorial Grammar\is{Type-Logical
  Categorial Grammar (TLCG)} (TLCG; or \emph{Type-Logical Grammar};
\citealt{Morrill94a-u,Moortgat2011a-u,KubotaLevineBook}).\footnote{For 
more detailed introductions to these different variants of CG, see 
\citet{steedman2011ccg} (on CCG) and \citet{oehrle2011} (on 
TLCG), both included in \citet{BB2011a-ed}.} After presenting the ``core''
component of CG that is shared between the two approaches \emdashUS which is
commonly referred to as the \term{AB Grammar} \emdashUS I introduce
aspects of the respective approaches in which they diverge from each
other.


\subsection{Notation and presentation }

Before getting started, some comments are in order as to the notation
and the mode of presentation adopted. Two choices are made for the
notation. First, CCG and TLCG traditionally adopt different notations
of the slash. I stick to the TLCG notation throughout this chapter for
notational consistency. Second, I present all the fragments below in
the so-called \term{labeled deduction} notation of (Prawitz-style)
natural deduction. In particular, I follow \citet{oehrle1994} and
\citet{Morrill94a-u} in the use of ``term labels'' in labeled deduction
to encode prosodic and semantic information of linguistic expressions.
This involves writing linguistic expressions as \emph{tripartite
signs}, formally, tuples of prosodic form, semantic interpretation and
syntactic category (or syntactic type). Researchers familiar with HPSG
should find this notation easy to read and intuitive; the idea is
essentially the same as how linguistic signs are conceived of in HPSG.
In the CG literature, this notation has its roots in the conception of
``multidimensional'' linguistic signs in earlier work by Dick Oehrle
\citeyearpar{oehrle88}. But the reader should be aware that this is
\emph{not} the standard notation in which either CCG or TLCG is
typically presented.\footnote{CCG derivations are typically presented
  as upside-down parse trees (see, for example,
  \citealt{Steedman2000a-u,steedman2012}) whereas  TLCG derivations are
  typically presented as proofs in Gentzen sequent calculus (see, for
  example, \citealt{Moortgat2011a-u,barkershan2015}).}
Also, logically savvy readers may find this
notation somewhat confusing since it (unfortunately) obscures certain
aspects of CG pertaining to its logical properties. In any event, it
is important to keep in mind that different notations co-exist in the
CG literature (and the logic literature behind it), and that, just as
in mathematics in general, different notations can be adopted for the
same formal system to highlight different aspects of it in different
contexts. As noted in the introduction,  for the mode of presentation, the emphasis is consistently
on linguistic (rather than computational or logical) aspects.
Moreover, I have taken the liberty to gloss over certain
minor differences among different variants of CG for the sake of
presentation. The reader is therefore encouraged
to consult primary sources as well, especially when details matter.


\subsection{The AB Grammar \label{ab}}

\largerpage
I start with a simple fragment of CG called the \term{AB Grammar},
consisting of just two syntactic rules in (\ref{SE0}) (here,
$\circ$ designates string concatenation):

\ea
\label{SE0}%
\label{rseone}% was a, a now manually
\label{lseone}% was b, b now manually
\twomulticolexamples{Forward Slash Elimination:\\[.5\baselineskip]
\begin{prooftree}
\NoSem
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ }}{\syncat{\textit{A}/\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ }}{\syncat{\textit{B}}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ }}{\syncat{\textit{A}}}}
\end{prooftree}
}{
Backward Slash Elimination:\\[.5\baselineskip]
\begin{prooftree}
\NoSem
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ }}{\syncat{\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ }}{\syncat{\textit{A}}}}
\end{prooftree}
}
\z


\noindent
With the somewhat minimal lexicon in (\ref{lex01}), the sentence
\textit{John loves Mary} can be licensed
as in (\ref{cg-tree1}). The two slashes $/$ and \ensuremath{\backslash}\
are used to form ``complex'' syntactic categories (more on this below)
indicating valence information: the transitive verb \textit{loves} is
assigned the  category
\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP} since it first
combines with an NP to 
its  right (i.e.\  the direct object) and then another NP to its left
(i.e.\  the subject).

\begin{exe}
 \ex\label{lex01}
  \begin{xlist}
 \ex
    \pt{john}; \  \syncat{NP}
 \ex
    \pt{mary}; \  \syncat{NP}
 \ex\label{introne}
    \pt{ran}; \  \syncat{NP\ensuremath{\backslash}{}S}
 \ex\label{trone}
    \pt{loves}; \ \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}
  \end{xlist}
\end{exe}

\begin{exe}%\todo[inline]{Is horizontal alignment OK?}
 \ex\label{cg-tree1}
 \attop{\begin{prooftree}
\NoSem
%\DerivSize
\hypo{\LexEnt{\pt{john}}{\sem{ }}{\syncat{NP}}}
\hypo{\LexEnt{\pt{mary}}{\sem{ }}{\syncat{NP}}}
\hypo{\LexEnt{\pt{loves}}{\sem{ }}{\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{loves \ensuremath{\circ}\xspace mary}}{\sem{ }}{\syncat{NP\ensuremath{\backslash}{}S}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace mary}}{\sem{ }}{\syncat{S}}}
\end{prooftree}}
\end{exe}

\noindent
In the notation adopted here, the linear order of words is explicitly
represented in the prosodic component of each derived sign.
Thus,  just like the analysis trees in Linearization-based HPSG (see \crossrefchapteralt[Section~\ref{sec-domains}]{order} for an overview), the
left-to-right order of elements in the proof tree does not
necessarily correspond to the surface order of words. 
The object NP \textit{Mary} is deliberately placed on the left of the
transitive verb \textit{loves} in the proof tree in (\ref{cg-tree1})
in order to  underscore this point.

\largerpage[-1]
At this point, the analysis in (\ref{cg-tree1}) is just like the familiar PSG analysis of the
form in Figure~\ref{fig-john-loves-mary}, except that the symbol VP is
replaced by \syncat{NP\ensuremath{\backslash}{}S}.

\begin{figure}
\begin{forest}
sm edges
[S [NP [John] ] [VP [V [loves] ] [NP [Mary] ] ] ]
\end{forest}
\caption{PSG analysis of \emph{John loves Mary.}}\label{fig-john-loves-mary}
\end{figure}
\noindent
Things will start looking more interesting as one makes the fragment
more complex (and also by adding the semantics), but before doing so,
I first introduce
some basic assumptions, first about syntactic categories (below) and then
about semantics (next section).

\emph{Syntactic categories}\is{syntactic category} (or \emph{syntactic types}\is{syntactic type}) are defined recursively
in CG. This can be concisely written using the so-called
``BNC notation'' as follows:\footnote{See Section~\ref{sec:morphology} below
for the treatment of syntactic features (such as those used for
agreement). I ignore this aspect for the fragment developed below for the sake of
exposition. The treatment of syntactic features (or its analog)
is a relatively underdeveloped aspect of CG syntax literature,
as compared to HPSG research (where the whole linguistic theory is
built on the basis of a theory/formalism of complex feature
structures). CCG seems to assume something similar to 
feature unification in HPSG, though details are typically not
worked out explicitly. In TLCG, there are occasional suggestions in the
literature (see, for example, \citealt[Chapter~6, Section~2]{Morrill94a-u};
\citealt{pogodallapompigne2011})
that syntactic features can be formalized in terms of
dependent types \citep{MartinLofIntuitionistic,ranta94}, but
there is currently no in-depth study working out a theory of
syntactic features along these lines.}$^,$\footnote{Recognizing PP as
a basic type is somewhat  
non-standard, although there does not seem to be any consensus on
what should be regarded as a (reasonably complete) set of
basic syntactic types for natural language syntax.}

\begin{exe}
 \ex\label{cat-def}
  \begin{xlist}
 \ex\label{bascat}
    BaseType := \{ N,  NP, PP, S \}
 \ex\label{complex-cat}
    Type := BaseType $|$ (Type\ensuremath{\backslash}Type) $|$ (Type\ensuremath{/}Type)
  \end{xlist}
\end{exe}
In words, anything that is a BaseType is a Type, and
any complex expression of form
\syncat{\textit{A}\ensuremath{\backslash}\textit{B}}  or
\textit{A}\ensuremath{/}\kern-.07em\textit{B} where \textit{A} and \textit{B} are both
Types is a Type. To give some examples, the following expressions are
syntactic types according to the definition in
(\ref{cat-def}):\footnote{I omit
parentheses for a sequence of the same type of slash, for which
disambiguation is obvious \emdashUS for example, \syncat{\textit{A}\ensuremath{\backslash}{}\textit{A}\ensuremath{\backslash}{}\textit{A}} is an abbreviation for \syncat{(\textit{A}\ensuremath{\backslash}{}(\textit{A}\ensuremath{\backslash}{}\textit{A}))}.}

\begin{exe}
 \ex
  \begin{xlist}
 \ex
    \syncat{S\ensuremath{\backslash}{}S}
 \ex
    \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP\ensuremath{/}NP}
 \ex
    \syncat{(S\ensuremath{/}(NP\ensuremath{\backslash}{}S))\ensuremath{\backslash}{}(S\ensuremath{/}NP)}
 \ex
    \syncat{((NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S))\ensuremath{\backslash}{}((NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S))}
  \end{xlist}
\end{exe}

\largerpage[-1]
One important feature of CG is that, like HPSG, it lexicalizes the
valence (or subcategorization) properties of linguistic expressions.
Unlike HPSG, where this is done by a list (or set) valued syntactic
feature, in CG, complex syntactic categories directly represent the
combinatoric (i.e.~valence) properties of lexical items. For example,
lexical entries for intransitive and transitive verbs in \ili{English} will
look like the following (semantics is omitted here but will be
supplied later):

\begin{exe}
 \ex\label{lex1}
  \begin{xlist}
 \ex\label{intr}
    \pt{ran}; \  \syncat{NP\ensuremath{\backslash}{}S}
 \ex\label{tr}
    \pt{read}; \ \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}
 \ex\label{trthree}
    \pt{introduces}; \ \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}PP\ensuremath{/}NP}
  \end{xlist}
\end{exe}
(\ref{intr}) says that the verb \textit{ran} combines with its argument NP \emph{to its
left} to become an S. Likewise, (\ref{tr}) says that \textit{read} first
combines with an NP \emph{to its right} and then another NP to its left to
become an S.

One point to keep in mind (though it may not seem to make much difference at this point) is that in
CG, syntactic rules are thought of as logical rules and the derivations of sentences like
(\ref{cg-tree1}) as \emph{proofs} of the well-formedness of particular strings as sentences.  From
this logical point of view, the two slashes should really be thought of as
directional variants of implication (that is, both \syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}} and
\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}} essentially mean `\emph{if} there is a
\textit{B}, \emph{then} there is an \textit{A}'), and the two rules of Slash Elimination introduced
in (\ref{SE0}) should be thought of as directional variants of \term{modus ponens}
($B \ensuremath{ \rightarrow } A, B \ensuremath{\vdash\xspace } A$). This analogy between natural
language syntax and logic is emphasized in particular in TLCG research.


\subsection{Syntax-semantics interface in CG \label{interface}}

One attractive property of Categorial Grammar as a theory of natural language syntax
is its straightforward syntax-semantics
interface. In particular, there is a 
functional mapping from syntactic categories to semantic
types.\footnote{Technically, this is ensured in TLCG by the 
homomorphism from the syntactic type logic to the semantic type logic
(the latter of which is often implicit) and the so-called Curry-Howard
correspondence between proofs and terms \citep{vanBenthem88}.}
For the sake of exposition, I assume an extensional fragment of
Montagovian model-theoretic semantics in what follows, but it should be noted that the
CG syntax is mostly neutral to the choice of the specific variant of
semantic theory to go with it.\footnote{See,  for example,  \citet{martin2013} and
\citet{bekkimineshima17} for recent proposals on adopting compositional variants
of (hyper)intensional dynamic semantics and proof theoretic semantics,
respectively, for the semantic component of CG-based theories of
natural language.}

Assuming the standard recursive definition of semantic types
as in (\ref{semtyp-def}) (with basic types $e$ for
individuals and $t$ for truth values), the function \SemTyp\ (which returns, for each
syntactic category given as input, its semantic type) can be defined as in
(\ref{semtyp-base}) and (\ref{semtyp-recur}).

\begin{exe}
 \ex\label{semtyp-def}
  \begin{xlist}
 \ex
    BaseSemType := \{ $e$, $t$ \}
 \ex
    SemType := BaseSemType $|$ SemType \ensuremath{ \rightarrow } SemType
  \end{xlist}
 \ex\label{semtyp-base}
  (Base Case)
  \begin{xlist}
 \ex\label{semtyp-np}
    \SemTyp(NP) = \SemTyp(PP) = $e$
 \ex\label{semtyp-n}
    \SemTyp(N) = $e \ensuremath{ \rightarrow } t$
 \ex\label{semtyp-s}
    \SemTyp(S) = $t$
  \end{xlist}
 \ex\label{semtyp-recur}
  (Recursive Clause)
  
  For any complex syntactic category of the form
     \syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}} (or \syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}), 
  
     \SemTyp(\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}})
     (= \SemTyp(\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}})) =
     \SemTyp(\textit{B}) \ensuremath{ \rightarrow } \SemTyp(\textit{A})
\end{exe}
For example, \SemTyp(\syncat{S\ensuremath{/}(NP\ensuremath{\backslash}{}S)}) =  $(e \ensuremath{ \rightarrow } t) \ensuremath{ \rightarrow } t$
(for subject position quantifier in CCG).

Syntactic rules with semantics can then be written as in (\ref{SE-rev})
(where the semantic effect of these rules is \term{function application})
and a sample derivation with semantic annotation is given in (\ref{tree02}).
 
\ea
\label{SE-rev} %\\[-15pt]
%\label{rsetwo}% a has to be added manually
%\label{lsetwo}% b has to be added manually
\twomulticolexamples{
 Forward Slash Elimination:\\[.33\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}
}{Backward Slash Elimination:\\[.5\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \sF(\sG) }}{\syncat{\textit{A}}}}
\end{prooftree}
}
\z

\begin{exe}
 \ex\label{tree02}
\attop{\begin{prooftree}
%\DerivSize
\hypo{\LexEnt{\pt{john}}{\sem{ \trns{j} }}{\syncat{NP}}}
\hypo{\LexEnt{\pt{mary}}{\sem{ \trns{m} }}{\syncat{NP}}}
\hypo{\LexEnt{\pt{loves}}{\sem{ \trns{love} }}{\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{loves \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m}) }}{\syncat{NP\ensuremath{\backslash}{}S}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m})(\trns{j}) }}{\syncat{S}}}
\end{prooftree}}
\end{exe}

\noindent
A system of CG with only the Slash Elimination rules like the fragment
above is called the \term{AB Grammar}, so called because it corresponds to
the earliest form of CG formulated by
\citet{Ajdukiewicz35a-u} and \citet{bar-hillel53}.


\subsection{Combinatory Categorial Grammar \label{sec:CCG}}

\subsubsection{An ``ABC'' fragment: AB Grammar with order-preserving combinatory rules \label{preCCG}}

Some\indexccgstart more machinery is needed to do some interesting linguistic analysis.
I now extend the AB fragment above by adding two types of
rules: \term{Type Raising} and (\emph{Harmonic}) \term{Function Composition}. These are
a subset of rules typically entertained in CCG. I call the resultant
system \term{ABC Grammar} (AB + Function
\underline{C}omposition).\footnote{This is not a standard terminology,
  but giving a name to this fragment is convenient for the purpose of the
discussion below.} Though it is an impoverished version of CCG, the
ABC fragment 
already enables an interesting and elegant analysis of
\emph{nonconstituent coordination}\is{coordination!nonconstituent} (NCC), originally due to \citet{Steedman85a-u} and
\citet{Dowty88a-u}, which is essentially identical to the analysis of NCC in
the current versions of both CCG and TLCG. I will then discuss the
rest of the rules constituting CCG in the next section. The reason for
drawing a distinction between the ``ABC'' fragment and (proper) CCG is
just for the sake of exposition. The rules introduced in the present
section have the property that they are all derivable as \emph{theorems} in
the (associative) Lambek calculus, the calculus that underlies most
variants of TLCG. For this reason, separating the two sets of rules
helps clarify the similarities and differences between CCG and TLCG.

The \term{Type Raising} and \term{Function Composition} rules are defined as in
(\ref{FC}) and (\ref{TR}), respectively.%\largerpage[2]

\ea
\label{FC}
\twomulticolexamples{
\scalebox{.94}{%
\mbox{Forward Function Composition:}}\\[.5\baselineskip]
\scalebox{.94}{%
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG}}{\syncat{\textit{B}\ensuremath{/}\kern-.1em\textit{C}}}}
\infer2[FC]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{A}\ensuremath{/}\kern-.1em\textit{C}}}}
\end{prooftree}}
}{\scalebox{.94}{%
\mbox{Backward Function Composition:}}\\[.5\baselineskip]
\scalebox{.94}{%
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG}}{\syncat{\textit{C}\ensuremath{\backslash}{}\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\infer2[FC]{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{C}\ensuremath{\backslash}{}\textit{A}}}}
\end{prooftree}}}
\z

\ea
\label{TR}
\twomulticolexamples{
\scalebox{.94}{%
\mbox{Forward Type Raising:}}\\[.5\baselineskip]
\scalebox{.94}{
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}}}}
\infer1[TR]{\LexEnt{\pt{\ptv{a} }}{\sem{ \lambda v. v(\sF) }}{\syncat{\textit{B}\ensuremath{/}(\textit{A}\ensuremath{\backslash}{}\textit{B})}}}
\end{prooftree}}}{
\scalebox{.94}{\mbox{Backward Type Raising:}}\\[.5\baselineskip]
\scalebox{.94}{
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}}}}
\infer1[TR]{\LexEnt{\pt{\ptv{a} }}{\sem{ \lambda v. v(\sF) }}{\syncat{(\textit{B}\ensuremath{/\!}\textit{A})\ensuremath{\backslash}{}\textit{B}}}}
\end{prooftree}}}
\z

\noindent
The Type Raising rules are essentially rules of ``type lifting''
familiar in the formal semantics literature, except that they specify
the ``syntactic effect'' of type lifting explicitly (such that the
function-argument relation is reversed). Similarly
Function Composition rules can be understood as function composition
in the usual sense (as in mathematics and functional programming),
except, again, that the syntactic effect is explicitly specified.

As noted by \citet{Steedman85a-u},
% whole paper
with Type Raising and Function
Composition, a string of words such as \textit{John loves}
can be analyzed  as
a constituent of type S\ensuremath{/}NP, that is, an expression that is looking for
an NP to its right to become an S:\footnote{\trns{love} is a function
  of type $e \rightarrow e \rightarrow t$, where the first argument
  corresponds to the direct object. Thus, \ensuremath{\trns{love}(x)(y)}
is equivalent to the two-place relation notation
\ensuremath{\trns{love}(y,x)} in which the subject argument is written first.}

\begin{exe}
 \ex\label{}
\attop{\begin{prooftree}
\hypo{\LexEnt{\pt{john }}{\sem{ \trns{j} }}{\syncat{NP}}}
\infer1[TR]{\LexEnt{\pt{john }}{\sem{ \lambda f. f(\trns{j}) }}{\syncat{S\ensuremath{/}(NP\ensuremath{\backslash}{}S)}}}
\hypo{\LexEnt{\pt{loves}}{\sem{ \trns{love} }}{\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\infer2[FC]{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves }}{\sem{ \lambda x. \trns{love}(x)(\trns{j})}}{\syncat{S\ensuremath{/}NP}}}
\end{prooftree}}
\end{exe}

\noindent Intuitively, Function Composition has the effect of delaying
the application of a function. The verb is looking for a direct object
to its right before it can be taken as an argument (of type
\syncat{NP\ensuremath{\backslash}S})
of the type raised subject NP. Function
Composition directly combines the subject and the verb before the
direct object argument of the latter is saturated. The resultant
category inherits the unsaturated argument both in the syntactic
category (\syncat{S/NP})
and semantics (of type $e \rightarrow t$).

Assuming generalized conjunction (with the standard definition for
the generalized conjunction operator \ensuremath{ \sqcap\xspace } \emph{Ã  la}
\citet{partee-rooth1983a}
% whole paper
and the polymorphic syntactic category
\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})\ensuremath{/}\textit{X}} for \textit{and}),
the analysis for a \term{Right Node Raising}
(RNR) sentence such as (\ref{RNR-CCG}) is straightforward, as in (\ref{RNR-CCG-DV}).

\begin{exe}
 \ex\label{RNR-CCG}
  John loves, and Bill hates, Mary.
\end{exe}

\begin{exe}
\ex\label{RNR-CCG-DV}
\attop{\oneline{\begin{prooftree}[separation=.5em]
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace loves  }; \\ \sem{ \lambda x. \trns{love}(x)(\trns{j})}; \syncat{S\ensuremath{/}NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont and  };\\
\sem{ \ensuremath{ \sqcap\xspace } }; \syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})\ensuremath{/}\textit{X}}}}
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace hates };\\
\sem{ \lambda x. \trns{hate}(x)(\trns{b})}; \syncat{S\ensuremath{/}NP}}}
\infer2[FA]{\MultiLine{\mathsf{ \ptfont and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates };\\
\sem{\ensuremath{ \sqcap\xspace } (\lambda x. \trns{hate}(x)(\trns{b}))}; \syncat{(S\ensuremath{/}NP)\ensuremath{\backslash}{}(S\ensuremath{/}NP)}}}
\infer2[FA]{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates}}{\sem{ (\lambda x. \trns{love}(x)(\trns{j})) \ensuremath{ \sqcap\xspace } (\lambda x. \trns{hate}(x)(\trns{b}))}}{\syncat{S\ensuremath{/}NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont mary  };\\ \sem{ \trns{m} }; \syncat{NP}}}
\infer2[FA]{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m})(\trns{j}) \ensuremath{ \wedge\xspace } \trns{hate}(\trns{m})(\trns{b})}}{\syncat{S}}}
\end{prooftree}}}
\end{exe}


\noindent
\citet{Dowty88a-u} showed  that this analysis extends straightforwardly to
the (slightly) more complex case of \term{Argument Cluster Coordination}
(ACC), such as (\ref{DCC-EG}), as in (\ref{ACC-CCG-DV}) (here, VP, TV and DTV are
abbreviations of \syncat{NP\ensuremath{\backslash}{}S}, \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP} and \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP\ensuremath{/}NP}, respectively).
 
\begin{exe}
 \ex\label{DCC-EG}
  Mary gave Bill the book and John the record.
\end{exe}
\begin{exe}
 \ex\label{ACC-CCG-DV}
\hspace{-2em}\attop{\resizebox{\linewidth+1.8em}{!}{\begin{prooftree}[separation=.25em,label separation=.25em]
\hypo{\MultiLine{\mathsf{ \ptfont mary  }; \\ \sem{ \trns{m}  }; \\\syncat{NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont gave  }; \\ \sem{ \trns{give} }; \\\syncat{DTV}}}
\hypo{\MultiLine{\mathsf{ \ptfont bill  }; \\ \sem{ \trns{b}  }; \syncat{NP}}}
\infer1[TR]{\MultiLine{\mathsf{ \ptfont bill  }; \\ \sem{ \lambda P. P(\trns{b})  }; \\\syncat{DTV\ensuremath{\backslash}{}TV}}}
\hypo{\MultiLine{\mathsf{ \ptfont the \ensuremath{\circ}\xspace book  }; \\ \sem{ \I(\trns{bk})}; \syncat{NP}}}
\infer1[TR]{\MultiLine{\mathsf{ \ptfont the \ensuremath{\circ}\xspace book  }; \\ \sem{ \lambda Q. Q(\I(\trns{bk}))}; \\\syncat{TV\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book  }; \\ \sem{ \lambda R. R(\trns{b})(\I(\trns{bk})) }; \syncat{DTV\ensuremath{\backslash}\kern-.12em VP}}}
\hypo{\MultiLine{\mathsf{ \ptfont and  }; \\ \sem{ \ensuremath{ \sqcap\xspace } }; \\\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})\ensuremath{/}\textit{X}}}}
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \lambda R. R(\trns{j})(\I(\trns{rc})) }; \\\syncat{DTV\ensuremath{\backslash}\kern-.12em VP}}}
\infer2{\MultiLine{\mathsf{ \ptfont and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \ensuremath{ \sqcap\xspace } (\lambda R. R(\trns{j})(\I(\trns{rc}))) }; \\\syncat{(DTV\ensuremath{\backslash}\kern-.12em VP)\ensuremath{\backslash}{}(DTV\ensuremath{\backslash}\kern-.12em VP)}}}
\infer2{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \lambda R. R(\trns{b})(\I(\trns{bk})) \ensuremath{ \sqcap\xspace } \lambda R. R(\trns{j})(\I(\trns{rc})) }; \syncat{DTV\ensuremath{\backslash}\kern-.12em VP}}}
\infer2{\MultiLine{\mathsf{ \ptfont gave \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \trns{give}(\trns{b})(\I(\trns{bk})) \ensuremath{ \sqcap\xspace } \trns{give}(\trns{j})(\I(\trns{rc})) }; \syncat{VP}}}
\infer2{\MultiLine{\mathsf{ \ptfont mary \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \trns{give}(\trns{b})(\I(\trns{bk}))(\trns{m}) \ensuremath{ \wedge\xspace } \trns{give}(\trns{j})(\I(\trns{rc}))(\trns{m}) }; \syncat{S}}}
\end{prooftree}}}
\end{exe}

\noindent
Here, by Type Raising, the indirect and direct objects become
functions that can be combined via Function Composition, to form
a non-standard constituent that can then be coordinated.  After two such
expressions are conjoined, the verb is fed as an argument to return a VP.
Intuitively, the idea behind this analysis is that
\textit{Bill the book} is of type
\syncat{DTV\ensuremath{\backslash}\kern-.12em VP} since if it were to combine
with an actual ditransitive verb (such as \textit{gave}), 
a VP (\textit{gave Bill the book}) would be obtained. 
Note that in both the RNR and ACC examples above, the right semantic\label{cg:page-ACC-semantics}
interpretation for the whole sentence is
assigned compositionally via the rules given above in (\ref{FC}) and (\ref{TR}).


\subsubsection{From ABC to CCG}
\label{abc2ccg}

CCG is a version of CG developed by Mark Steedman since the 1980s with
extensive linguistic application. The best sources for CCG are the
three books by Steedman \citep{Steedman97a,Steedman2000a-u,steedman2012},
which present treatments of major linguistic phenomena in CCG and
give pointers to earlier literature. CCG is essentially a rule-based
extension of the AB Grammar. The previous
section has already introduced two key components that constitute this extension: Type
Raising and (Harmonic) Function Composition.\footnote{There is
actually a subtle point about Type Raising rules. Recent
versions of CCG \citep[80]{steedman2012} do not take them to be syntactic rules,
but rather assume that Type Raising is an operation in the lexicon.
This choice seems to be motivated by 
parsing considerations (so as to eliminate as  many unary rules as
possible from the syntax). It is also worth noting in this
connection that the CCG-based syntactic fragment that
\citet{Jacobson1999a,Jacobson2000a} assumes for her Variable-Free Semantics
is actually a quite different system from Steedman's version of CCG in
that it crucially assumes Geach rules, another type of unary rules
likely to have similar computational consequences as Type Raising rules, in the syntactic
component. (Incidentally,  the Geach rules are often attributed
to \citet{Geach70a},  but \citegen{Humberstone2005} careful
historical study suggests that this attribution is highly misleading, if not
totally groundless.)} There are 
aspects of natural language syntax that cannot 
be handled adequately in this simple system, and in such situations,
CCG makes (restricted) use of additional rules. This point can be
illustrated nicely with two issues that arise in connection with the
analysis of long-distance dependencies.

%\largerpage
The basic idea behind the CCG analysis of long-distance dependencies,
due originally to \citet{AS82a}, is very simple and is similar in spirit to
the HPSG analysis in terms of \slasch feature percolation (see
\crossrefchapteralt{udc}
for the treatment of long-distance dependencies in HPSG). Specifically,
CCG analyzes extraction dependencies via a chain of Function
Composition, as illustrated by the derivation for (\ref{ldd-EX}) in (\ref{ldd-CCG-DV}).

\ea\label{ldd-EX}
  This is the book that John thought that Mary read \trace.  
\z
\begin{figure}
\begin{exe}
\ex\label{ldd-CCG-DV}
\hspace{-2em}\attop{\resizebox{\linewidth+1.8em}{!}{\begin{prooftree}[separation=.25em,label separation=.25em]
\hypo{\MultiLine{\mathsf{ \ptfont that   }; \\ \sem{ \lambda P \lambda Q \lambda x. \\ \hspace{2ex} Q(x) \ensuremath{ \wedge\xspace } P(x) }; \\\syncat{(N\ensuremath{\backslash}{}N)\ensuremath{/}(S\ensuremath{/}NP)}}}
\hypo{\MultiLine{\mathsf{ \ptfont john  }; \\ \sem{ \trns{j} }; \syncat{NP}}}
\infer1[TR]{\MultiLine{\mathsf{ \ptfont john  }; \\ \sem{ \lambda P. P(\trns{j}) }; \\\syncat{S\ensuremath{/}(NP\ensuremath{\backslash}{}S)}}}
\hypo{\MultiLine{\mathsf{ \ptfont thought   }; \\ \sem{ \trns{think}   }; \\\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}S\ensuremath{'}}}}
\hypo{\MultiLine{\mathsf{ \ptfont that  }; \\ \sem{ \lambda p. p  }; \\\syncat{S\ensuremath{'}\hskip-.3ex\ensuremath{/}S}}}
\hypo{\MultiLine{\mathsf{ \ptfont read }; \\ \sem{ \trns{read} }; \\\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \trns{m} }; \syncat{NP}}}
\infer1[TR]{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \lambda P. P(\trns{m}) }; \\\syncat{S\ensuremath{/}(NP\ensuremath{\backslash}{}S)}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{read}(x)(\trns{m}) }; \syncat{S\ensuremath{/}NP}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{read}(x)(\trns{m}) }; \syncat{S\ensuremath{'}\hskip-.3ex\ensuremath{/}NP}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{think}(\trns{read}(x)(\trns{m})) }; \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{think}(\trns{read}(x)(\trns{m}))(\trns{j}) }; \syncat{S\ensuremath{/}NP}}}
\infer2[FA]{\MultiLine{\mathsf{ \ptfont that \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda Q \lambda x. Q(x) \ensuremath{ \wedge\xspace } \trns{think}(\trns{read}(x)(\trns{m}))(\trns{j})}; \syncat{N\ensuremath{\backslash}{}N}}}
\end{prooftree}}}
\end{exe}
\vspace{-\baselineskip}
\end{figure}
% 
Like (many versions of) HPSG, CCG does not assume any empty expression
at the gap site. Instead, the information that the subexpressions
(constituting the extraction pathway) such
as \textit{Mary read} and \textit{thought that Mary read} are missing an NP on the
right edge is encoded in the syntactic category of the linguistic
expression. \textit{Mary read} is assigned the type \syncat{S\ensuremath{/}NP}, since it is a
sentence missing an NP on its right edge. \textit{thought that Mary read} is
of type \syncat{VP\ensuremath{/}NP} since it is a VP missing an NP on its right edge, etc.
Expressions that are not originally functions (such as the subject NPs
in the higher and lower clauses inside the relative clause in
(\ref{ldd-EX})) are first type raised. Then, Function Composition
effectively ``delays'' the saturation of the object NP argument of the
embedded verb, until the whole relative clause meets the relative
pronoun, which itself is a higher-order function that takes a sentence
missing an NP (of type \syncat{S\ensuremath{/}NP}) as an argument.

%\largerpage
The successive passing of the \ensuremath{/}NP specification to larger structures
is essentially analogous to the treatment of extraction via the \slasch
feature in HPSG. However, unlike HPSG, which has a dedicated feature
that handles this information passing, CCG achieves the effect via the
ordinary slash that is also used for local syntactic
composition.

% % \footnote{There are several complications that arise in
% %   the CCG analysis because of this architectural difference, which I
% %   discuss immediately below. But one interesting consequence of the CCG
% %   approach is that the quite complex set of specifications of \slasch
% %   feature percolation (including the \emph{termination} of percolation)
% %   are all unnecessary in CCG.}

This difference immediately raises some issues for the CCG analysis of
extraction. First, in (\ref{ldd-EX}), the NP \isi{gap} happens to be on the right
edge of the sentence, but this is not always the  case. Harmonic
Function Composition alone cannot  handle non-peripheral
extraction of the sort found in examples such as the following:

\begin{exe}
 \ex\label{nonP}
  This is the book that John thought that [Mary read {\trace}\xspace at school].
\end{exe}
Assuming that  \textit{at school} is a VP modifier of type \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)},
what is needed here is a mechanism that assigns the type \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}
to the string \textit{read {\trace}\xspace at school}, despite the
fact that  the missing NP is not  
on the right edge. CCG employs a special  rule of ``Crossed'' Function
Composition for this purpose, defined as follows:

\begin{exe}
 \ex\label{mc}
Crossed Function Composition:\\[.5\baselineskip]
\attop{\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sG}}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF}}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{C}}}}
\infer2[xFC]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{C}\ensuremath{/}\kern-.07em\textit{B}}}}
\end{prooftree}}
\end{exe}
Unlike its harmonic counterpart (in which \ptv{a} has the type
\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}), in (\ref{mc})
the directionality of the slash is different 
in the two premises, and the resultant category inherits the
slash originally associated with the inherited argument 
(i.e.\ \syncat{\ensuremath{/}{}\textit{B}}).

Once this non-order-preserving version of Function Composition is 
introduced in the grammar, the derivation for (\ref{nonP}) is
straightforward, as in (\ref{nonP-DV}):

\begin{exe}
 \ex\label{nonP-DV}
\attop{\oneline{\begin{prooftree}
%\DerivSize
\hypo{\LexEnt{\pt{mary}}{\sem{ \trns{m} }}{\syncat{NP}}}
\infer1[TR]{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \lambda P. P(\trns{m}) }; \\\syncat{S\ensuremath{/}(NP\ensuremath{\backslash}{}S)}}}
\hypo{\MultiLine{\mathsf{ \ptfont read }; \\ \sem{ \trns{read} }; \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont at \ensuremath{\circ}\xspace school  }; \\ \sem{ \trns{at-school} }; \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)}}}
\infer2[xFC]{\LexEnt{\pt{read \ensuremath{\circ}\xspace at \ensuremath{\circ}\xspace school}}{\sem{ \lambda x. \trns{at-school}(\trns{read}(x)) }}{\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\infer2[FC]{\LexEnt{\pt{mary \ensuremath{\circ}\xspace read \ensuremath{\circ}\xspace at \ensuremath{\circ}\xspace school}}{\sem{ \lambda x. \trns{at-school}(\trns{read}(x))(\trns{m}) }}{\syncat{S\ensuremath{/}NP}}}
\end{prooftree}}}
\end{exe}

%\largerpage[2]
\noindent
Unless appropriately constrained, the addition of the crossed
composition rule leads to potential overgeneration, since
non-extracted expressions cannot change word order so freely in
\ili{English}. For example, without additional restrictions, the simple CCG
fragment above overgenerates examples such as the following (see, for
example, \citealt[188]{kuhlmann-etal-2015-lexicalization}):

\ea[*]{\label{overg}
a\sub{NP/N} [\sub{N/N} powerful\sub{N/N} by Rivaldo\sub{N$\backslash${}N}] shot\sub{N} 
}
\z
Here, I will not go into the technical details of how this issue is
addressed in the CCG literature.
In contemporary versions of CCG, the application of 
special rules such as crossed composition in (\ref{mc}) is 
regulated by the 
notion of ``structural control'' borrowed into CCG from  the
``multi"=modal'' variant of TLCG (see \citet{Baldridge2002a-u} and
\citet{steedman2011ccg}).

Another issue that arises in connection with extraction is how to treat
multiple gaps\is{gap} corresponding to a single filler. The simple fragment
developed above cannot license examples involving parasitic\is{gap!parasitic} gaps such
as the following:\footnote{Multiple gaps in coordination (i.e.\  ATB
  extraction) is not an
  issue, since these cases can be handled straightforwardly via the
  polymorphic definition of generalized conjunction in CCG, in just
  the same way that 
  unsaturated shared arguments in each conjunct are identified with one another.}

\begin{exe}
 \ex\label{PG}
  \begin{xlist}
 \ex\label{PG-1}
    This is the article that I filed {\trace}\xspace without reading {\trace}\xspace . 
 \ex\label{PG-2}
    Peter is a guy who even the best friends of {\trace}\xspace think  {\trace}\xspace should be
    closely watched.
  \end{xlist}
\end{exe}
Since neither Type Raising nor Function Composition changes the number
of ``gaps'' passed on to a larger expression, a new mechanism
is needed here. \citet[427]{Steedman87a-u} proposes the following rule to deal with this
issue:

\begin{exe}
 \ex\label{subst}
  Substitution:\\[.5\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sG}}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF}}{\syncat{(\textit{A}\ensuremath{\backslash}{}\textit{C})\ensuremath{/}\kern-.07em\textit{B}}}}
\infer2[S]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(x)(\sG(x))}}{\syncat{\textit{C}\ensuremath{/}\kern-.07em\textit{B}}}}
\end{prooftree}
\end{exe}
\noindent
This rule has the effect of
``collapsing'' the  arguments of the
two inputs into one, to be saturated by a single filler.
The derivation for  the adjunct parasitic gap example in
(\ref{PG-1}) then goes as follows (where VP is an abbreviation for \syncat{NP\ensuremath{\backslash}{}S}):

\begin{exe}
 \ex
\attop{\begin{prooftree}
%\DerivSize
\hypo{\MultiLine{\mathsf{ \ptfont filed  }; \\ \sem{ \trns{file} }; \syncat{VP\ensuremath{/}NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont without  }; \\ \sem{ \trns{wo} }; \syncat{(VP\ensuremath{\backslash}\kern-.12em VP)\ensuremath{/}VP}}}
\hypo{\MultiLine{\mathsf{ \ptfont reading  }; \\ \sem{ \trns{read} }; \syncat{VP\ensuremath{/}NP}}}
\infer2[FC]{\MultiLine{\mathsf{ \ptfont without \ensuremath{\circ}\xspace reading  }; \\ \sem{ \lambda x. \trns{wo}(\trns{read}(x)) }; \syncat{(VP\ensuremath{\backslash}\kern-.12em VP)\ensuremath{/}NP}}}
\infer2[S]{\MultiLine{\mathsf{ \ptfont filed \ensuremath{\circ}\xspace without \ensuremath{\circ}\xspace reading  }; \\ \sem{ \lambda x. \trns{wo}(\trns{read}(x))(\trns{file}(x)) }; \syncat{VP\ensuremath{/}NP}}}
\end{prooftree}}
\end{exe}

Like the crossed composition rule, the availability of the
substitution rule should be restricted to extraction environments. In
earlier versions of CCG, this was done by a stipulation on
the rule itself. \citet{Baldridge2002a-u} proposed an improvement of the
organization of the CCG rule system in which the applicability of
particular rules is governed by lexically specified ``modality''
encodings. See \citet{steedman2011ccg} for this relatively recent
development in CCG.\indexccgend

\subsection{Type-Logical Categorial Grammar \label{sectlg}}

The rule-based nature of CCG should be clear from the above
exposition. Though superficially similar in many respects,
TLCG takes a distinctly different perspective on the
underlying architecture of the grammar of natural language.
Specifically, in TLCG, the rule system of grammar is literally taken
to be a kind of logic. Consequently, all (or almost all) grammar rules are logical
inference rules reflecting the properties of (typically a small number
of) logical connectives such as \ensuremath{/} and
\ensuremath{\backslash}\ (which are, as noted in Section~\ref{ab}, viewed as
directional variants of implication).
It is important to keep in mind that this leads to
an inherently much more abstract view on the
organization of the grammar of natural language than the
surface-oriented perspective that HPSG and CCG share
at a broad level. This conceptual shift can be
best illustrated by first replacing the ABC Grammar introduced in
Section~\ref{preCCG} by the \term{Lambek calculus}, where all the rules
posited as primitive rules in the former are derived as \emph{theorems} (in
the technical sense of the term) in the latter.

Before moving on, I should hasten to note that the TLCG literature is
more varied than the CCG literature, consisting of several related but
distinct lines of research. I choose to present one particular variant
called Hybrid Type-Logical Categorial Grammar \citep{KubotaLevineBook}
in what follows, in line with the present chapter's linguistic
emphasis (for a more in-depth discussion on the linguistic application
of TLCG, see \citealt{Carpenter98a-u} and \citealt{KubotaLevineBook}).
A brief comparison with major alternatives
can be found in Chapter~12 of \citet{KubotaLevineBook}. 
Other variants of TLCG, most notably, the
\term{Categorial Type Logics} \citep{Moortgat2011a-u} and
\term{Displacement Calculus} \citep{morrill2011} emphasize logical
and computational aspects. \citet{mootretore2012} is a good
introduction to TLCG with emphasis on these latter aspects.


\subsubsection{The Lambek calculus \label{lambek}}

In addition to the \term{Slash Elimination} rules (reproduced here as
(\ref{SE})), which are identical to the two rules in the AB Grammar
from Section~\ref{ab}, the Lambek calculus posits the \term{Slash
Introduction} rules, which can be written in the current labeled
deduction format as in (\ref{si}) (the vertical dots around the
hypothesis abbreviate an arbitrarily complex proof
structure).\footnote{\citet[Chapter~4]{Morrill94a-u} was
the first to recast the Lambek calculus in this labelled deduction
format.}


\ea
\label{SE} %\\[-15pt]
%\label{rsethree}% a to be added manually
%\label{lsethree}% b to be added manually
\twomulticolexamples{
\mbox{\isi{Forward Slash Elimination}}:\\[.5\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}
}{
\mbox{\isi{Backward Slash Elimination}}:\\[.5\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \sF(\sG) }}{\syncat{\textit{A}}}}
\end{prooftree}% 
}
\z
\ea
\label{si} %\\[-15pt]
\label{rsi}% a to be added manually
%\label{lsi}% b to be added manually
\twomulticolexamples{
\mbox{\isi{Forward Slash Introduction}}:\\[.5\baselineskip]
\begin{prooftree}
\hypo{$\vdots$}
\infer[no rule]1{$\vdots$}
\hypo{[\LexEnt{\pt{ \ensuremath{\greekp}}}{\sem{ x}}{\syncat{\textit{A}}}]\ensuremath{^n}}
\infer1{$\vdots$}
\hypo{$\vdots$}
\infer[no rule]1{$\vdots$}
\infer3{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \p}}{\sem{ \sF}}{\syncat{\textit{B}}}}
\infer1[/I$^n$]{\LexEnt{\pt{\ptv{b}}}{\sem{ \lambda x. \sF}}{\syncat{\textit{B}\ensuremath{/\!}\textit{A}}}}
\end{prooftree}
}{%
\mbox{\isi{Backward  Slash Introduction}}:\\[.5\baselineskip]
\begin{prooftree}
\hypo{$\vdots$}
\infer[no rule]1{$\vdots$}
\hypo{[\LexEnt{\pt{\p}}{\sem{ x}}{\syncat{\textit{A}}}]\ensuremath{^n}}
\infer1{$\vdots$}
\hypo{$\vdots$}
\infer[no rule]1{$\vdots$}
\infer3{\LexEnt{\pt{\p \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF}}{\syncat{\textit{B}}}}
\infer1[\ensuremath{\backslash}I$^n$]{\LexEnt{\pt{\ptv{b}}}{\sem{ \lambda x. \sF}}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{B}}}}
\end{prooftree}
}
\z
 
\noindent The key idea behind the Slash Introduction rules in (\ref{si}) is that
they allow one  to derive linguistic expressions by \emph{hypothetically}
assuming the existence of words and phrases that are not
(necessarily) overtly present. For example,
(\ref{si}a) can be understood as consisting of two steps of inference:
one first draws a (tentative) conclusion that  the string of words
$\ptv{b} \circ \p$ is of type \textit{B},  by
hypothetically assuming the existence of an expression
\p\ of type \textit{A} (where a hypothesis 
is enclosed in square brackets to indicate its status as such).
At that point,  one can draw the (real)  conclusion that
\ptv{b}  alone is of type \textit{B}\ensuremath{/}\kern-0.14em\textit{A} since it was just shown to be
an expression that yields \textit{B} \emph{if} there is  an 
\textit{A} (namely, \p)  to its right. Note that the final conclusion
no longer depends on the hypothesis that
there is  an expression \p\ of type 
\textit{A}. More technically, the hypothesis is \emph{withdrawn} at the final step.

One consequence that immediately follows
in this system is that Type Raising and
Function Composition (as well as other theorems; see, for example, 
\citealt[46--49]{jaeger05}) are now derivable as theorems. As an
illustration, the proofs for (\ref{TR}a) and (\ref{FC}a) are shown in (\ref{TRproof})
and (\ref{FCproof}), respectively.

\begin{exe}
 \ex\label{TRproof}
\attop{\begin{prooftree}
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ v }}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{B}}}]^1\,\,}}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ v(\sF)}}{\syncat{\textit{B}}}}
\infer1[\ensuremath{/}I\ensuremath{^1}]{\LexEnt{\pt{\ptv{a}}}{\sem{ \lambda v. v(\sF) }}{\syncat{\textit{B}\ensuremath{/}(\textit{A}\ensuremath{\backslash}{}\textit{B})}}}
\end{prooftree}}
% \columnbreak
\end{exe}
%
\begin{exe}
 \ex\label{FCproof}
\attop{\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}}}
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ x }}{\syncat{\textit{C}}}]^1\,\,}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}\ensuremath{/}\kern-.1em\textit{C}}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ \sG(x) }}{\syncat{\textit{B}}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ \sF(\sG(x)) }}{\syncat{\textit{A}}}}
\infer1[\ensuremath{/}I\ensuremath{^1}]{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x)) }}{\syncat{\textit{A}\ensuremath{/}\kern-.1em\textit{C}}}}
\end{prooftree}}
\end{exe}


\noindent
These are formal theorems, but they intuitively make sense.
For example, what's going on in  (\ref{FCproof}) is simple. 
Some expression of type  \textit{C} is 
hypothetically assumed first, which is then combined with
\syncat{\textit{B}\ensuremath{/}\kern-.1em\textit{C}}. This produces a larger expression
of type \textit{B}, which can then be fed as an argument to
\syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}}. 
At that point, the initial hypothesis is withdrawn and it is concluded
that what one really had was just something that would become an \textit{A} \emph{if}
there is a \textit{C} to its right, namely, an expression of type \textit{A}\ensuremath{/}\kern-.1em\textit{C}.
Thus, a sequence of expression of types \syncat{\textit{A}\ensuremath{/}\kern-.07em\textit{B}} and \syncat{\textit{B}\ensuremath{/}\kern-.1em\textit{C}}
is proven to be of type \syncat{\textit{A}\ensuremath{/}\kern-.1em\textit{C}}.  This type of proof
is known as \term{hypothetical reasoning}, since it involves a step of
positing a hypothesis initially and withdrawing that hypothesis
at a later point.

Getting back to some notational issues, there are two crucial things
to keep in mind about the notational 
convention adopted here (which I implicitly assumed above). First, the
connective \ensuremath{\circ}\xspace in the prosodic component designates string
concatenation and is associative in both directions
(i.e.\  \pt{ (\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}) \ensuremath{\circ}\xspace \ensuremath{\greekp_3}} \ensuremath{ \equiv\xspace } \pt{ \ensuremath{\greekp_1} \ensuremath{\circ}\xspace (\ensuremath{\greekp_2} \ensuremath{\circ}\xspace \ensuremath{\greekp_3}) }). In other words, hierarchical
structure is irrelevant for the prosodic representation. Thus,
the applicability condition on the Forward Slash
Introduction rule (\ref{rsi}a) is simply that
the prosodic variable \pt{ \ensuremath{\greekp} }
of the hypothesis appears as the rightmost element of the
string prosody of the input expression (i.e.\
\pt{\ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp} }).
Since the penultimate step in (\ref{FCproof}) satisfies this 
condition, the rule is applicable here. Second, note in this
connection that the application of the
Introduction rules is conditioned on the position of the prosodic
variable, and \emph{not} on the position of the hypothesis itself in the
proof tree (this latter convention is more standardly adopted when the
Lambek calculus is presented in Prawitz-style natural deduction,
though the two presentations are
equivalent \emdashUS see, for example, \citealt[Chapter~5]{Carpenter98a-u} and
\citealt[Chapter~1]{jaeger05}).

Hypothetical reasoning with Slash Introduction makes it possible to
recast the CCG analysis of nonconstituent coordination from
Section~\ref{preCCG} within the logic of \ensuremath{/} and $\backslash$. This
reformulation fully retains the essential analytic ideas of the
original CCG analysis but makes the underlying logic of syntactic
composition more transparent.

The following derivation illustrates how the ``reanalysis'' of the
string \textit{Bill the book} as a derived constituent of type
\syncat{(VP\ensuremath{/}NP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}
(the same type as in (\ref{ACC-CCG-DV})) can be obtained in the Lambek
calculus:

\begin{exe}
 \ex\label{NCC-arg-deriv}
\attop{\begin{prooftree}
\small
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ f}}{\syncat{VP\ensuremath{/}NP\ensuremath{/}NP}}]^1\,\,}}
\hypo{\LexEnt{\pt{bill }}{\sem{ \trns{b} }}{\syncat{NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp} \ensuremath{\circ}\xspace bill }}{\sem{ f(\trns{b}) }}{\syncat{VP\ensuremath{/}NP}}}
\hypo{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \I(\trns{bk}) }}{\syncat{NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp} \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ f(\trns{b})(\I(\trns{bk})) }}{\syncat{VP}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^1}]{\LexEnt{\pt{bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ \lambda f. f(\trns{b})(\I(\trns{bk})) }}{\syncat{(VP\ensuremath{/}NP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\end{prooftree}}
\end{exe}


At this point, one may wonder what the relationship is between the
analysis of nonconstituent coordination via Type Raising and Function
Composition in the ABC Grammar in Section~\ref{preCCG} and the hypothetical
reasoning-based analysis in the Lambek calculus just presented. Intuitively, they
seem to achieve the same effect in slightly different ways. The
logic-based perspective of TLCG allows us to obtain a deeper
understanding of the relationship between them. To facilitate
comparison, I first recast the Type Raising + Function Composition
analysis from Section~\ref{preCCG} in the Lambek calculus. The relevant 
part is the part that derives the ``noncanonical constituent'' \textit{Bill the
book}:

\begin{exe}
 \ex\label{redundant-DV}
\hspace{-2em}\attop{\resizebox{\linewidth+1.8em}{!}{\begin{prooftree}[separation=.5em, label separation=.25em]
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_3} }}{\sem{ R }}{\syncat{DTV}}]^3\,\,}}
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_2} }}{\sem{ P }}{\syncat{DTV}}]^2\,\,}}
\hypo{\LexEnt{\pt{bill }}{\sem{ \trns{b}  }}{\syncat{NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp_2} \ensuremath{\circ}\xspace bill }}{\sem{ P(\trns{b})  }}{\syncat{TV}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^2}]{\LexEnt{\pt{bill }}{\sem{ \lambda P. P(\trns{b})  }}{\syncat{DTV\ensuremath{\backslash}{}TV}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ensuremath{\greekp_3} \ensuremath{\circ}\xspace bill }}{\sem{ R(\trns{b})  }}{\syncat{TV}}}
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_1} }}{\sem{ Q }}{\syncat{TV}}]^1\,\,}}
\hypo{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \I(\trns{bk})}}{\syncat{NP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ Q(\I(\trns{bk}))}}{\syncat{VP}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^1}]{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \lambda Q. Q(\I(\trns{bk}))}}{\syncat{TV\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ensuremath{\greekp_3} \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ R(\trns{b})(\I(\trns{bk})) }}{\syncat{VP}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^3}]{\LexEnt{\pt{bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ \lambda R. R(\trns{b})(\I(\trns{bk})) }}{\syncat{DTV\ensuremath{\backslash}\kern-.12em VP}}}
\end{prooftree}}}
\end{exe}

\noindent
By comparing (\ref{redundant-DV}) and (\ref{NCC-arg-deriv}), one can
see that
(\ref{redundant-DV}) contains some redundant steps. First, hypothesis
2 ($\greekp_2$) is
introduced only to be replaced by hypothesis 3 ($\greekp_3$). This is completely
redundant, since one could have obtained exactly the same result by
directly combining hypothesis 3 with the NP \textit{Bill}. Similarly,
hypothesis 1 can be eliminated by replacing it with the
TV \pt{ \ensuremath{\greekp_3} \ensuremath{\circ}\xspace bill} on the left-hand side of the third line from the bottom. By
making these two simplifications, the derivation in
(\ref{NCC-arg-deriv}) is obtained.

The relationship between the more complex proof in (\ref{redundant-DV}) and
the simpler one in (\ref{NCC-arg-deriv}) is parallel to the relationship
between an unreduced lambda term (such as
\sem{\lambda R [\lambda Q [Q(\I(\trns{bk}))](\lambda P [P(\trns{b})](R))] }\,)
and its
$\beta$-normal form (i.e.\  \sem{\lambda R. R(\trns{b})(\I(\trns{bk}))   }\,).
In fact, there is a formally precise
one-to-one relationship between linear logic (of which the Lambek calculus is known
to be a straightforward extension) and the typed lambda calculus known
as the \term{Curry-Howard Isomorphism} \citep{howard80}, according to which
the lambda term that represents the proof 
(\ref{redundant-DV}) $\beta$-reduces  to the term that represents the proof
(\ref{NCC-arg-deriv}).\footnote{There is a close relationship between these
lambda terms representing proofs (i.e.\ syntactic derivations) and the
lambda  terms that one writes 
to notate semantic translations, especially if the latter is written 
at each step of derivation \emph{without} performing $\beta$-reduction. But
it is important to keep in mind that
lambda terms representing syntactic proofs
and lambda terms notating semantic translations are distinct things.}
Technically, this is known as \term{proof normalization}
(\citealt[36--42, 137--144]{jaeger05} contains a particularly useful discussion on this notion). 

Thus, the logic-based architecture of the Lambek calculus (and various versions of
TLCG, which are all extensions of the Lambek calculus) enables us to say, in a
technically precise way, how (\ref{NCC-arg-deriv}) and (\ref{redundant-DV}) are
the ``same'' (or, more precisely, equivalent), by building on
independently established results in mathematical logic and computer
science. This is one big advantage of taking seriously the view,
advocated by the TLCG research, that ``language \emph{is} logic''.


\subsubsection{Extending the Lambek calculus \label{sec:extending}}

Hypothetical reasoning is a very powerful (yet systematic) tool, but
with forward and backward slashes, it is only good for analyzing
expressions missing some material at the (right or left) periphery.
This is problematic in the analyses of many linguistic phenomena, such
as \textit{wh}-extraction (where the ``\isi{gap}'' can be in a sentence-medial
position \emdashUS recall the discussion about crossed composition rules in
CCG in Section~\ref{abc2ccg}) and quantifier scope (where the
quantifier needs to covertly move from a sentence-medial position),
as well as various kinds of discontinuous constituency phenomena (see, 
for example,  \citealt{morrill-ea11}, which contains analyses of various
types of discontinuous constituency phenomena in a recent version of
TLCG known as ``Displacement Calculus''). In what follows, I sketch one
particular, relatively recent approach to this problem, known as
\emph{Hybrid Type-Logical Categorial Grammar}\is{Categorial Grammar (CG)!Hybrid Type-Logical
  \textasciitilde{} (Hybrid TLCG)} (Hybrid TLCG;
\citealt{kubota-diss,kubota-NCC,kubota-levine-coord,KubotaLevineBook}).
This approach combines the Lambek calculus with \citeposs{oehrle1994}
term-labeled calculus, which deals with discontinuity by employing
$\lambda$-binding in the prosodic component.

Hybrid TLCG extends the Lambek calculus with the Elimination and Introduction rules for the
\term{vertical slash}:

\ea
\label{scoping}
\label{upI} % a to be added manually
%\label{upE} % b to be added manually
\twomulticolexamples{
\mbox{\isi{Vertical  Slash Introduction}}:\\[.33\baselineskip]
  \begin{prooftree}
  \hypo{$\vdots$}
  \infer[no rule]1{$\vdots$}
  \hypo{[\LexEnt{\pt{\p}}{\sem{ x}}{\syncat{\textit{A}}}]\ensuremath{^n}}
  \infer1{$\vdots$}
  \hypo{$\vdots$}
  \infer[no rule]1{$\vdots$}
  \infer3{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF }}{\syncat{\textit{B}}}}
  \infer1[\vs I$^n$]{\LexEnt{\pt{\ensuremath{\lambda} \p.  \ptv{b}}}{\sem{ \lambda x. \sF }}{\syncat{\textit{B}\vs \textit{A}}}}
  \end{prooftree}
}{
 \mbox{\isi{Vertical  Slash Elimination}}:\\[.5\baselineskip]
\begin{prooftree}
\hypo{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}\vs \textit{B}}}}
\hypo{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\infer2[\ensuremath{\vs}E]{\LexEnt{\pt{\ptv{a}(\ptv{b})}}{\sem{  \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}
}
\z

\largerpage
\noindent 
These rules make it possible to model what (roughly) corresponds to
syntactic movement operations in
mainstream generative grammar. This is illustrated
in (\ref{inv-scope00}) for the \sem{   \forall > \exists   } %
{}%
reading for the sentence \textit{Someone talked to everyone today}.

% \begin{figure}
\begin{exe}
\ex\label{inv-scope00}
\hspace{-2em}\attop{\resizebox{\linewidth+1.8em}{!}{\begin{prooftree}[separation=.5em,label separation=.25em]
\hypo{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.\ensuremath{\greeks}(everyone) }; \\ \sem{ \GQU{A}{person}}; \\\syncat{S\vs (S\vs NP)}}}
\hypo{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.\ensuremath{\greeks}(someone) }; \\ \sem{ \GQU{E}{person} }; \\\syncat{S\vs (S\vs NP)}}}
\hypo{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} }; \\ \sem{ x_2}; \\\syncat{NP}}}^2}}
\hypo{\MultiLine{\mathsf{ \ptfont talked \ensuremath{\circ}\xspace to }; \\ \sem{ \trns{talked-to} }; \\\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}}}
\hypo{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_1} }; \\ \sem{ x_1}; \\\syncat{NP}}}^1}}
\infer2[\ensuremath{/}E]{\MultiLine{\mathsf{ \ptfont talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} }; \\ \sem{ \trns{talked-to}(x_1) }; \syncat{NP\ensuremath{\backslash}{}S}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} }; \\ \sem{ \trns{talked-to}(x_1)(x_2) }; \syncat{S}}}
\hypo{\MultiLine{\mathsf{ \ptfont today }; \\ \sem{ \trns{tdy} }; \\\syncat{S\ensuremath{\backslash}{}S}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \trns{tdy}(\trns{talked-to}(x_1)(x_2)) }; \syncat{S}}}
\infer[left label=â ]1[\ensuremath{\vs}I\ensuremath{^2}]{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp_2}. \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \lambda x_2. \trns{tdy}(\trns{talked-to}(x_1)(x_2)) }; \syncat{S\vs NP}}}
\infer[left label=â¡]2[\ensuremath{\vs}E]{\MultiLine{\mathsf{ \ptfont someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \GQU{E}{person}(\lambda x_2. \trns{tdy}(\trns{talked-to}(x_1)(x_2))) }; \syncat{S}}}
\infer1[\ensuremath{\vs}I\ensuremath{^1}]{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp_1}. someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \lambda x_1. \GQU{E}{person}(\lambda x_2. \trns{tdy}(\trns{talked-to}(x_1)(x_2))) }; \syncat{S\vs NP}}}
\infer2[\ensuremath{\vs}E]{\MultiLine{\mathsf{ \ptfont someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace everyone \ensuremath{\circ}\xspace today }; \\ \sem{ \GQU{A}{person}(\lambda x_1. \GQU{E}{person}(\lambda x_2. \trns{tdy}(\trns{talked-to}(x_1)(x_2)))) }; \syncat{S}}}
\end{prooftree}}}
\end{exe}
% \end{figure}

A quantifier has the ordinary G{}Q meaning  (\sem{  \GQU{E}{person}  } %
{}%
and \sem{  \GQU{A}{person}  } 
{}%
abbreviate the terms \sem{   \lambda P. \exists x [ \trns{person}(x) \ensuremath{ \wedge\xspace } P(x)]   } %
{}%
and
 \sem{   \lambda P. \forall x [ \trns{person}(x) \ensuremath{ \rightarrow }  P(x)]   }, %
{}%
respectively), but its phonology is a
function of type (\yusukest\hskip.4em\shortarrow\hskip.1em\yusukest)\hskip.4em\shortarrow
\yusukest\xspace (where \yusukest\xspace is the type of string).
By abstracting over the position in which the quantifier
``lowers into'' in an S via the Vertical Slash Introduction rule (\ref{upI}a),
an expression of
type \syncat{S\vs NP} (phonologically
\yusukest\hskip.4em\shortarrow\hskip.1em\yusukest\xspace)  is obtained (â ),
which is then given as an argument to the quantifier. Then, by function
application via $\vs$E (â¡), the subject quantifier \textit{someone}
semantically scopes over the sentence and lowers its phonology to the
``\isi{gap}'' position kept track of by $\lambda$-binding in phonology (note
that this result obtains by function application and beta-reduction of
the prosodic term). The same
process takes place for the object quantifier \textit{everyone} to complete
the derivation.  The scopal
relation between multiple quantifiers depends on the order of
application of this hypothetical reasoning.
The surface scope reading is obtained by switching the order of the 
hypothetical reasoning for the two quantifiers (which results in the
same string of words, but with the opposite scope relation).

This formalization of quantifying-in by \citet{oehrle1994} has later
been extended by \citet{barker07} for more complex types of
scope-taking phenomena known as \term{parasitic scope} in the analysis
of symmetrical predicates (such as \textit{same} and
\textit{different}).\footnote{``Parasitic scope'' is a notion coined by
\citet{barker07} where, in transformational terms, some expression
takes scope at LF by parasitizing on the scope created by a different
scopal operator's LF movement. In versions of (TL)CG of the sort
discussed here, this corresponds to double lambda-abstraction via the
vertical slash.}
Empirical application of parasitic scope includes ``respective'' readings
\citep{kubota-levine-resp},  
``split scope'' of negative quantifiers
\citep{kubota-levine-gapping} and modified numerals such as \textit{exactly N}
\citep{pollard-numerical}.

\largerpage
Hypothetical reasoning with prosodic $\lambda$-binding enables a simple
analysis of \textit{wh}-extraction too, as originally noted by \citet[39--40]{muskens03}.
The key idea is that sentences with medial gaps can be analyzed
as expressions  of type \syncat{S\vs NP}, as in the derivation for
(\ref{bagels}) in (\ref{bagels-derivation}).
% 
\begin{exe}
 \ex\label{bagels}
  Bagels\ensuremath{_i}, Kim gave  \trace$_i$ to Chris.
\end{exe}
\begin{exe}
 \ex\label{bagels-derivation}
\hspace{-2em}\attop{\resizebox{\linewidth+1.8em}{!}{\begin{prooftree}[label separation=.5em,separation=.25em]
\hypo{\MultiLine{\mathsf{ \ptfont bagels  }; \\ \sem{ \trns{b} }; \syncat{NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks} \lambda \ensuremath{\greekp}. \ensuremath{\greekp} \ensuremath{\circ}\xspace \ensuremath{\greeks}(\E)  }; \\ \sem{ \lambda \ensuremath{\mathcal{F}}. \ensuremath{\mathcal{F}} }; \\\syncat{(S\vs \textit{X})\vs (S\vs \textit{X})}}}
\hypo{\MultiLine{\mathsf{ \ptfont kim  }; \\ \sem{ \trns{k} }; \syncat{NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont gave }; \\ \sem{ \trns{gave} }; \\\syncat{VP\ensuremath{/}PP\ensuremath{/}NP}}}
\hypo{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp}  }; \\ \sem{ x }; \\\syncat{NP}}}^1}}
\infer2[/E]{\LexEnt{\pt{gave \ensuremath{\circ}\xspace \ensuremath{\greekp}  }}{\sem{ \trns{gave} (x) }}{\syncat{VP\ensuremath{/}PP}}}
\hypo{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace chris }; \\ \sem{ \trns{c} }; \syncat{PP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c}) }}{\syncat{VP}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S}}}
\infer[left label=â ]1[\ensuremath{\vs}I\ensuremath{^1}]{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp}. kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris  }; \\ \sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }; \syncat{S\vs NP}}}
\infer[left label=â¡]2[\ensuremath{\vs}E]{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. \ensuremath{\greekp} \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S\vs NP}}}
\infer2[\ensuremath{\vs}E]{\LexEnt{\pt{bagels \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(\trns{b})(\trns{c})(\trns{k}) }}{\syncat{S}}}
\end{prooftree}}}
\end{exe}


\noindent Here, after deriving an \syncat{S\vs NP}, which keeps track of the \isi{gap}
position via the $\lambda$-bound variable \pt{ \ensuremath{\greekp} }, the topicalization
operator fills in the gap with an empty string and concatenates the
topicalized NP to the left of the string thus obtained. This way, the
difference between ``overt'' and ``covert'' movement reduces to a lexical
difference in the prosodic specifications of the operators that induce
them. A covert movement operator throws  in some material in the gap
position, whereas an overt movement operator ``closes off'' the \isi{gap}
with an empty string.

As illustrated above, hypothetical reasoning for the Lambek slashes \ensuremath{/}
and \ensuremath{\backslash}\ and for the vertical slash
\ensuremath{\vs} have important empirical 
motivations, but the real strength of a ``hybrid'' system like Hybrid TLCG which
recognizes both types of slashes is that it extends automatically to cases
in which ``directional'' and ``non-directional'' phenomena interact. A
case in point comes from the interaction of nonconstituent
coordination and quantifier scope. Examples such as those in (\ref{NCCq})
allow for at least a reading in which the shared quantifier outscopes
conjunction.\footnote{Whether the other scopal relation (one in which the
quantifier meaning is ``distributed'' to each conjunct, as in the
paraphrase ``I gave a couple of books to Pat on Monday and I gave a
couple of books to Sandy on
Tuesday'' for (\ref{NCCq})) is possible seems to depend on various factors.
With downward-entailing quantifiers such as (\ref{nothingEG}), this reading
seems difficult to obtain without heavy contextualization and
appropriate intonational cues. See \citet[Section~2.2]{kubota-levine-coord} for some discussion.}

\begin{exe}
 \ex\label{NCCq}
  \begin{xlist}
 \ex
    I gave a couple of books to Pat on Monday and to Sandy on Tuesday.
 \ex\label{nothingEG}
    Terry said nothing  to Robin on Thursday or to Leslie on Friday. 
  \end{xlist}
\end{exe}

\noindent
I now illustrate how this wide scope reading for the quantifier in NCC
sentences like (\ref{NCCq}) is immediately predicted to be available in the
fragment developed so far (Hybrid TLCG actually predicts both scopal relations
for all NCC sentences; see \citealt[Section~4.3]{kubota-levine-coord} for how the
distributive scope is licensed). The derivation for (\ref{nothingEG}) is
given in (\ref{RT-derivation}) on the next page.
%
\begin{figure}
\begin{exe}
\ex
\label{RT-derivation}
\attop{\scalebox{.93}{\begin{prooftree}[separation=.5em,label separation=.25em]
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_1}}}{\sem{ P}}{\syncat{VP\ensuremath{/}PP\ensuremath{/}NP}}]^1\,\,}}
\hypo{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_2}}}{\sem{ x}}{\syncat{NP}}]^2\,\,}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}}}{\sem{ P(x) }}{\syncat{VP\ensuremath{/}PP}}}
\hypo{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace robin }; \\ \sem{ \trns{r} }; \syncat{PP}}}
\infer2[\ensuremath{/}E]{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin}}{\sem{ P(x)(\trns{r}) }}{\syncat{VP}}}
\hypo{\MultiLine{\mathsf{ \ptfont on \ensuremath{\circ}\xspace thursday }; \\ \sem{ \trns{onTh} }; \syncat{VP\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[\ensuremath{\backslash}E]{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \trns{onTh}(P(x)(\trns{r}))}}{\syncat{VP}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^1}]{\LexEnt{\pt{\ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \lambda P. \trns{onTh}(P(x)(\trns{r}))}}{\syncat{(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\infer1[\ensuremath{\backslash}I\ensuremath{^2}]{\LexEnt{\pt{to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))}}{\syncat{NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\end{prooftree}}}\\[2ex]
\scalebox{.93}{\begin{prooftree}[separation=.5em,label separation=.25em]
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday }; \\ \sem{ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))}; \\\syncat{NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\hypo{\MultiLine{\mathsf{ \ptfont or }; \\ \sem{ \lambda \ensuremath{\mathcal{V}} \lambda \ensuremath{\mathcal{W}}. \ensuremath{\mathcal{W}} \ensuremath{ \sqcup\xspace } \ensuremath{\mathcal{V}} }; \\\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})\ensuremath{/}\textit{X}}}}
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday }; \\ \sem{ \lambda x \lambda P. \trns{onFr}(P(x)(\trns{l}))}; \\\syncat{NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[\ensuremath{/}E]{\MultiLine{\mathsf{ \ptfont or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\ \sem{ \lambda \ensuremath{\mathcal{W}}. \ensuremath{\mathcal{W}} \ensuremath{ \sqcup\xspace } [ \lambda x \lambda P. \trns{onFr}(P(x)(\trns{l}))]}; \\\syncat{(NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP)}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{ \ptfont to
      \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on
      \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace or
      \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie
      \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday };
    \\ \sem{ [ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))] \sqcup\xspace [ \lambda
      x \lambda P. \trns{onFr}(P(x)(\trns{l}))] }; \syncat{NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\end{prooftree}}\\[2ex]
%
%
\hspace{-1em}\scalebox{.93}{\begin{prooftree}[separation=.5em,label separation=.25em]
\hypo{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.  \ensuremath{\greeks}(nothing) }; \\ \sem{ \ensuremath{ \neg\xspace} \GQU{E}{thing} }; \\\syncat{S\vs (S\vs NP)}}}
\hypo{\MultiLine{\mathsf{ \ptfont terry }; \\ \sem{ \trns{t} }; \syncat{NP}}}
\hypo{\MultiLine{\mathsf{ \ptfont said }; \\ \sem{ \trns{said} }; \\\syncat{VP\ensuremath{/}NP\ensuremath{/}PP}}}
\hypo{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_3}  }; \\ \sem{ x }; \\ \syncat{NP}}}^3}}
\hypo{$\vdots$}
\infer[no rule]1{\MultiLine{\mathsf{  \ptfont to
      \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on
      \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace
      {} }\\\mathsf{  \ptfont   \hspace{2ex} or
      \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie
      \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday
    }; \\   \sem{ [ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))]
      \sqcup } \\ \hspace{2ex} \sem{ [ \lambda 
      x \lambda P. \trns{onFr}(P(x)(\trns{l}))] }; \\\syncat{NP\ensuremath{\backslash}{}(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{  \ptfont \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace     {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ [ \lambda P. \trns{onTh}(P(x)(\trns{r}))]} \\ \hspace{2ex} \sem{\sqcup\xspace [ 
      \lambda P. \trns{onFr}(P(x)(\trns{l}))] }; \\\syncat{(VP\ensuremath{/}PP\ensuremath{/}NP)\ensuremath{\backslash}\kern-.12em VP}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{  \ptfont said \ensuremath{\circ}\xspace \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \trns{onTh}(\trns{said}(x)(\trns{r})) \sqcup\xspace  
      \trns{onFr}(\trns{said}(x)(\trns{l})) }; \syncat{VP}}}
\infer2[\ensuremath{\backslash}E]{\MultiLine{\mathsf{  \ptfont terry \ensuremath{\circ}\xspace said \ensuremath{\circ}\xspace \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t}) \vee\xspace
      \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t}) }; \syncat{S}}}
\infer1[\ensuremath{\vs}I\ensuremath{^3}]{\MultiLine{\mathsf{  \ptfont
      \lambda \ensuremath{\greekp_3}. terry \ensuremath{\circ}\xspace
      said \ensuremath{\circ}\xspace \ensuremath{\greekp_3}
      \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin
      \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday
      \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont
      \hspace{2ex} or \ensuremath{\circ}\xspace to
      \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on
      \ensuremath{\circ}\xspace friday  }; \\   \sem{ \lambda x. \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t}) \vee\xspace
      \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t}) }; \syncat{S\vs NP}}}
\infer2[\ensuremath{\vs}E]{\MultiLine{\mathsf{ \ptfont terry \ensuremath{\circ}\xspace said \ensuremath{\circ}\xspace nothing \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace  or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday }; \\ \sem{ \ensuremath{ \neg\xspace} \GQU{E}{thing} ( \lambda x. \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t}) \ensuremath{ \vee\xspace } \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t})) }; \syncat{S}}}
\end{prooftree}}
\z
\vspace{-\baselineskip}
\end{figure}
%\noindent 
The key point in this derivation is that, via hypothetical
reasoning, the string \textit{to Robin on Thursday or to Leslie on Friday}
forms a syntactic constituent with a full-fledged meaning assigned to
it in the usual way. Then the quantifier takes scope above this whole
coordinate structure, yielding the non-distributive, quantifier
wide-scope reading.

Licensing the correct scopal relation between the quantifier and
conjunction in the analysis of NCC remains a challenging problem in
the HPSG literature. See Section~\ref{NCCcomparison} for some
discussion.

\section{Architectural similarities and differences \label{architecture}}

\subsection{Broad architecture}

One important property common to HPSG and CG is that they are both
lexicalist theories of syntax in the broader sense.\footnote{I say
``broader sense'' here since not all variants of either HPSG or CG subscribe to the
so-called \isi{Lexical Integrity} Hypothesis\is{lexicalism} (see 
\crossrefchapteralt[Section~\ref{sec:lex}]{lexicon}), which says that syntax and 
morphology are distinct components of grammar. For example, in the CG literature, 
the treatments of verb clustering in \ili{Dutch} by \citet{moortgatoehrle94} and
in \ili{Japanese} by \citet{Kubota2014a-u} seem to go against the tenet of
the Lexical Integrity Hypothesis. In HPSG,
\citet{Gunji99a-u} formulates an analysis of \ili{Japanese}
causatives that does not adhere to the Lexical Integrity Hypothesis and which 
contrasts sharply with the strictly lexicalist analysis by 
\citet{MSI99a}. 
See also \crossrefchaptert{lexicon}, 
\citet{Bruening2018a, Bruening2018c-u}, \citet{MuellerLexicalism} and \citet{MWArgSt}
for some discussion on lexicalism.} 
This is partly due to an explicit choice made at an early
stage of the development of HPSG to encode valence information in the
syntactic categories of linguistic expressions, following CG
(see \crossrefchapteralt[\pageref{page-subcategorization-start}--\pageref{page-subcategorization-start}]{evolution} and \crossrefchapteralt[Section~\ref{lexicon:sec-what-information-is-in-lexical-entries}]{lexicon}).\footnote{This
  point is explicitly noted by the founders of HPSG
  in the following passage in \citet{ps}:

\begin{quote}
A third principle of universal grammar posited by HPSG, the
\textit{Subcategorization Principle}\is{principle!Subcategorization}, is essentially a generalization of the
``argument cancellation'' employed in categorial grammar. \hfill \citep[11]{ps}
\end{quote}

}
The two theories share many similarities in the
analyses of specific linguistic phenomena due to this basic
architectural similarity. For example, many phenomena that are treated
by means of local movement operations (or via empty categories) in
mainstream generative syntax, such as passivization, raising/control in \ili{English}
and complex predicate phenomena in a typologically broad range of
languages are generally treated by the sharing of valence information
in the lexicon in these theories. For HPSG
analyses of these phenomena, see
\crossrefchaptert{arg-st}, \crossrefchaptert{complex-predicates} 
and \crossrefchaptert{control-raising}.
\citet{steedman2011ccg} contains a good
summary of CG analyses of local dependencies 
(passivization, raising/control). \citet[Section~4.2]{Kubota2014a-u} contains
a comparison of HPSG and CG analyses of complex predicates.
The heavy reliance on lexicalist analyses of
local dependencies is perhaps the most important property that is shared in
HPSG and various versions of CG.

\largerpage
But emphasizing this commonality too much may be a bit misleading,
since the valence features of HPSG and the slash connectives in CG
have very different ontological statuses in the respective theories.
The valence features in HPSG are primarily specifications, closely
tied to the specific phrase structure rules, that dictate the ways in
which hierarchical representations are built. To be sure, the lexical
specifications of the valence information play a key role in the
movement-free analyses of local dependencies
% \itdopt{non-local
%   dependencies?\\ Yusuke: 'local dependencies' is correct here, since
%   here I'm talking about analyses of passive, complex predicates etc.\\
% Stefan: Ah, since this is a comparison to HPSG I got onto the wrong track.}
along the lines noted
above, but still, there is a rather tight connection between these
valence specifications originating in the lexicon and the ways in
which they are ``canceled'' in specific phrase structure rules.

Things are quite different in CG, especially in TLCG. As discussed in
Section~\ref{cgexposition}, TLCG views the grammar of natural language \emph{not} as a
structure-building system, but as a logical deductive system. The two
slashes \ensuremath{/} and \ensuremath{\backslash}\ are thus not ``features'' that encode the
subcategorization properties of words in the lexicon, but have a much
more general and fundamental role within the basic architecture of
grammar in TLCG. These connectives are literally 
implicational connectives within a logical calculus. Thus, in TLCG,
``derived'' rules such as
Type Raising and Function Composition are \emph{theorems}, in just the same
way that the 
transitivity inference is a theorem in classical propositional logic.
Note that this is not just a matter of high-level conceptual
organization of the theory, since, as discussed in Section~\ref{cgexposition}, the
ability to assign ``constituent'' statuses to non-canonical constituents
in the CG analyses of NCC directly exploits this property of the
underlying calculus. The straightforward mapping from syntax to
semantics discussed in Section~\ref{interface} is also a direct
consequence of adopting this ``derivation as proof'' perspective on
syntax, building on the results of the Curry-Howard correspondence
\citep{howard80} in
setting up the syntax-semantics interface.\footnote{Although CCG does
not embody the idea of ``derivation as proof'' as explicitly as TLCG
does, it remains true to a large extent that the role of the slash
connective within the overall theory is largely similar in CCG and TLCG
in that CCG and TLCG share many key 
ideas in the analyses of actual empirical phenomena.}

\enlargethispage{3pt}
Another notable difference between (especially a recent variant of)
HPSG and CG is that CG currently lacks a detailed theory of (phrasal)
``constructions'', that is, patterns and (sub)regularities that are
exhibited by linguistic expressions that cannot (at least according to
the proponents of ``constructionist'' approaches) be lexicalized easily.
As discussed in \crossrefchaptert{cxg} (see also
\citealt{Sag97a}, \citealt{Fillmore99a} and \citealt{GSag2000a-u}),  recent constructional variants
of HPSG (\eg \citegen{Sag97a} Constructional HPSG as assumed in this volume and Sign-Based Construction
Grammar\indexsbcg (SBCG, \citealp*{SBK2012a}) incorporate ideas from Construction Grammar\indexcxg 
\citep{FKoC88a} and capture such
generalizations via a set of constructional templates (or schemata),
which are essentially a family of related phrase structure rules that
are organized in a type inheritance hierarchy.

Such an architecture seems nearly impossible to implement literally in
CG, except via empty operators or lexical operations corresponding to
each such constructional schema. In particular, in TLCG, syntactic
rules are logical inference rules, so, if one strictly adheres to its
slogan ``language is logic'', there is no option to freely add
syntactic rules in the deductive system. The general consensus in the
literature seems to be that while many of the phenomena initially
adduced as evidence for a constructional approach can be lexicalized
(see, for example, \citew{MWArgSt} and \crossrefchapterw{cxg}; see also
\citew[202]{steedman2011ccg}, which discusses ways in which some of the
empirical generalizations that \citew{Goldberg95a} adduces to the
notion of constructions can be lexicalized within CCG), there remain
some real challenges for a strictly lexicalist approach
(\crossrefchapteralt[Section~\ref{sec-npn}]{cxg} identifies the \textit{N after N}
construction as an instance of this latter type of phenomenon). It
then seems undeniable that the grammar of natural language is equipped with
mechanisms for dealing with ``peripheral'' patterns, but whether such
mechanisms should be given a central role in the architecture of
grammar is still a highly controversial issue. Whatever position one
takes, it is important to keep in mind that this is ultimately an
empirical question (a very complex and tough one indeed) that
should be settled on the basis of (various types of) evidence.



\subsection{Syntax--semantics interface }

%\largerpage
As should be clear from the exposition in Section~\ref{cgexposition},
both CCG and TLCG (at 
least in the simplest form) adopt a very rigid, one-to-one
correspondence between syntax and semantics. Steedman's work on CCG
has demonstrated that 
this simple and systematic mapping between syntax and semantics
enables attractive analyses of
a number of empirical phenomena at the syntax-semantics interface,
including some notorious problems such as the scope parallelism issue
in right-node raising known as
the Geach paradigm (\textit{Every boy loves, and
every girl detests, some saxophonist}; cf.~\citealt[8]{Geach70a}). Other important work on issues
at the syntax-semantics interface includes Jacobson's
(\citeyear{Jacobson1999a,Jacobson2000a}) work on pronominal anaphora
in Variable-Free Semantics (covering a wide range of phenomena
including the paycheck/Bach-Peters paradigms and binding parallelism
in right-node raising), \citeposs{barkershan2015} work on ``continuation-based''
semantics (weak crossover, superiority effects and ``parasitic scope''
treatments of symmetrical predicates and sluicing) and Kubota and
Levine's
(\citeyear{kubota-levine-coord,kubota-levine-pseudo,KubotaLevineBook})
Hybrid TLCG, dealing with interactions between coordination, ellipsis and scopal
phenomena.

%Things are actually not so simple. As long as the meanings of
%individual lexical items are specified as lambda terms, it is trivial
%to add a layer of underspecification to both CCG and TLCG.
%-steedman2012/s approach to quantification adopts this technique and
%treats indefinites as underspecified Skolem terms. There is another
%approach that makes use of underspecification in CG. Dependent Type
%Semantics \citep{bekki14,TanakaEA15} B\&M, a version of proof theoretic
%semantics that crucially makes use of underspecification for the
%treatment of anaphora, has been applied to both CCG B\&M and TLCG
%(Kubota \& Levine-LENLS).


%While this is definitely an interesting and promising body of work, it
%is probably fair to say that the rigid correspondence between syntax
%adn semantics is both a blessing and a curse. For example, how to
%compactly represent ambiguous scopal relations between multiple
%quantifiers would be a major headache for this type of
%architecture.\footnote{In this respect, it is interesting to see that
%Steedman's version of CCG, which is the most practically-oriented
%version of CG among different variants, partially incorporates
%the idea of underspecification in the treatment of quantification, as
%noted above.}

As discussed in \crossrefchaptert{semantics}, recent HPSG work on
complex empirical phenomena at the syntax-semantics interface makes
heavy use of underspecification. For example, major analyses of
nonconstituent coordination in recent HPSG use some version of an
underspecification framework to deal with complex interactions between
coordination and scopal operators.
\citep{Yatabe2001a,BS2004a,parkea18gapping,parkDiss,yatabe-tam2017}.
In a sense, HPSG retains a rigid phrase structure-based syntax (modulo
the flexibility entertained with the use of the linearization-based
architecture) and deals with the complex mapping to semantics via the
use of underspecification languages in the semantic component (such as
Minimal Recursion Semantics by \citealt{CFPS2005a} and Lexical Resources
Semantics by \citealt{RS2004a-u}; see also \crossrefchapteralt{semantics}). CG, on the other hand, tends to adhere
more closely to a tight mapping from syntax to semantics, but makes
the syntactic component itself flexible. But it is important to keep
in mind that, even within the CG research community,
there is no clear consensus about 
how strictly one should adhere to the
Montagovian notion of compositionality \emdashUS a glimpse of the recent 
literature reveals that the issue is very much an open-ended one: 
many contemporary variants of CG 
make use of underspecification for certain purposes (see, for example, 
\citealt[Chapter~7]{steedman2012}, \citealt{bekki14},
\citealt{bekkimineshima17} and \citealt{kubotaEA2019Geach}), 
while at the same time Jacobson's 
\citeyearpar{Jacobson1999a,Jacobson2000a} program of Variable-Free
Semantics is distinct in explicitly taking the classical notion of compositionality
as a driving principle.


% These differences may again be a reflection of a
% somewhat broader ``philosophical'' difference between the two theories
% alluded to above (HPSG's surface-oriented syntax vs.\  CG's abstract view
% on the nature of the combinatoric component).

%These proposals contrast sharply with the CG analyses of the same
%phenomena (see Kubota \& Levine and references cited therein) in that they reconcile
%the apparent mismatch between the syntactic representations and final
%semantic interpretations by making the compositional mechanism of
%grammar to manipulate underspecified semantic representations. The CG
%approach instead maintains a direct mapping from the combinatoric
%structure of syntax to semantic interpretation, but makes the
%combinatoric component itself radically flexible. These two competing
%views are interestingly different, and this may again be another
%reflection of a somewhat broader ``philosophical'' difference between
%the two general approaches alluded to at some other points in this
%chapter.



\subsection{Morpho-syntax and word order \label{sec:morphology}}

\largerpage\enlargethispage{3pt}
While there is relatively less detailed work on morphology and the
morpho-syntax interface in CG as compared to HPSG, there are several
ideas originating in the CG literature that have either
influenced some HPSG work or which are closely related to a certain
line of work in HPSG. I review some of these in this
section.\footnote{An important omission in the ensuing discussion is a
comparison of recent work in HPSG on morphology by Olivier Bonami and
Berthold Crysmann (see \crossrefchapteralt[Section~\ref{sec:IbM}]{morphology}), which builds on
and extends Greg Stump's \term{Paradigm Function Morphology} (PFM;
\citealt{Stump2001a}), and early CG work on morphology
\citep{hoeksema-diss,moortgat84,hoeksemajanda88,raffelsiefen92} which
could be viewed as precursors of PFM.}

% Bonami and Crysmann's work is still relatively new, and it exploits
% some aspects of HPSG not shared by other grammatical theories such as
% extensive use of underspecification and type inheritance hierarchies
% with default specifications. While this new approach to morphology in
% HPSG deserves a serious attention, there is no space for a careful
% critique here, and it seems premature to attempt one anyway at this
% stage, so I leave this task for whoever is more qualified to undertake
% it.


\subsubsection{Linearization-based HPSG and the phenogrammar/tectogrammar distinction in CG }

\is{linearization-based HPSG|(}
The idea of separating surface word order and the underlying
combinatorics, embodied in the so-called \emph{linearization-based}
version of HPSG (\citealt{Reape94a,Mueller95c,Kathol2000a};
cf.\ \crossrefchapteralt[Section~\ref{sec-domains}]{order}), has its origin 
in the work by the logician Haskell Curry \citeyearpar{curry61}, in which he
proposed the distinction between \term{phenogrammar} (the component
pertaining to surface word order) and \term{tectogrammar} (underlying
combinatorics). This same idea has influenced a certain line of work
in the CG literature too. Important early work was done by
\citeauthor{Dowty82a-u} \citeyearpar{Dowty82a-u,Dowty90a-Eng} in a variant of CG which is essentially an
AB Grammar with ``syncategorematic'' rules that directly manipulate
string representations, of the sort utilized in Montague Grammar, for
dealing with various sorts of discontinuous constituency.\footnote{See
also \crossrefchaptert[Section~\ref{evolution:sec-head-grammar}]{evolution} for a discussion of the influence
that early forms of CG \citep{Bach79a,Bach80a,Dowty82a-u,Dowty82b} had
on Head Grammar \citep{Pollard84a-u}, a precursor of HPSG.}

Dowty's early work has influenced two separate lines of work in the
later development of CG. First, a more formally sophisticated
implementation of an enriched theory of phenogrammatical component of
the sort sketched in \citet{Dowty90a-Eng} was developed in the
literature on Multi"=Modal Categorial Type Logics 
in the 90s, by exploiting the notion of ``modal control'' (as
already noted, this technique was  
later incorporated into CCG by \citealt[Chapter~5]{Baldridge2002a-u}). Some empirical
work in this line of research includes \citet{moortgatoehrle94} (on \ili{Dutch}
cross-serial dependencies; see also \citealt[Section~4]{Dowty97a-u} for an accessible
exposition of this analysis), \citet{kraak98} (\ili{French} clitic climbing),
\citet{Whitman2009} (``right-node wrapping'' in \ili{English}) and
\citet{kubota-diss,Kubota2014a-u} (complex predicates in \ili{Japanese}). Second,
the Curry/Dowty idea of the pheno/tecto distinction has also been the core motivation for the
underlying architecture of
a family of approaches called \emph{Linear Categorial Grammar}\is{Categorial Grammar (CG)!Linear
  \textasciitilde{} (LCG)} (LCG;
\citealt{oehrle1994,degroote01,muskens03,mihalicek-pollard10,pollard13}),
in which, following the work of \citet{oehrle1994},
the prosodic component is modeled as a lambda calculus
(cf.\ Section~\ref{sec:extending}) for
dealing with complex operations pertaining to word order
(the more standard approach in the TLCG tradition is to model the
prosodic component as some sort of algebra of structured strings as in
\citealt{morrill-ea11} (and at least implicitly in \citealt[Section~4]{moortgat1997})). 
In fact, among different variants of  
CG, LCG can be thought of as an extremist approach in
relegating word order completely from the combinatorics, by
doing away with the distinction between the Lambek forward and backward slashes.

One issue that arises for approaches that distinguish between the
levels of phenogrammar and tectogrammar, across the HPSG/CG divide, is
how closely these two components interact with one another.
\citet[Section~2.3]{Kubota2014a-u} discusses some data in the morpho-syntax of
complex predicates in \ili{Japanese} which (according to him) would call for
an architecture of grammar in which the pheno and tecto components
interact with one another closely, and which would thus be problematic
for the simpler LCG-type architecture. It would be interesting to see
whether/to what extent this same criticism would carry over to
linearization-based HPSG, which is similar (at least in its simplest
form) to LCG in maintaining a clear separation of the pheno/tecto
components.\footnote{But note also in this connection that
linearization-based HPSG is by no means monolithic; for example,
\citet{yatabe-tam2017} (discussed below in
Section~\ref{NCCcomparison}) propose a somewhat radical extension of
the linearization-based approach in which semantic composition is done at
the level of word order domains.}
\is{linearization-based HPSG|)}


\subsubsection{Syntactic features and feature neutralization}

As compared to HPSG, the status of syntactic features in CG is
somewhat unclear, despite the fact that such ``features'' are often used
in linguistic analyses in the CG literature. One reason that a
full-blown theory of syntactic features has not been developed in CG
research to date seems to be that as compared to HPSG, syntactic
features play a far less major role in linguistic analysis in CG.
Another possible reason is that empirical work on complex linguistic
phenomena (especially on languages other than \ili{English}) are still very
few in number in CG. 

It is certainly conceivable to develop a theory of syntactic features
and feature underspecification within CG by borrowing ideas from HPSG,
for which there is already a rich tradition of foundational work on this
issue. In fact, the work on Unification-based Categorial Grammar
\citep*{CalderEA1988} explored at the end of the 80s seems to have had
precisely such a goal. Unfortunately, this approach remains largely
isolated from other developments in the literature (of either CG or
other grammatical theories/formalisms). Another possibility would be to pursue a
more logic-based approach. For some ideas, see \citet{BJ95},
\citet{Bayer96}
and \citet{Morrill94a-u}. \citet[Chapter~6]{Morrill94a-u} in particular briefly
explores the idea of implementing syntactic features via the notion of
\emph{dependent types}\is{dependent type}. There is some renewed interest in
the linguistic application of ideas from \isi{Dependent
Type Theory} \citep{MartinLofIntuitionistic} in the recent literature
of CG and formal semantics (see, for example,
\citealt{ChatzikyriakidisLuo2017}), so pursuing this latter type of
approach in connection with this new line of work may lead to some
interesting developments.

One issue that is worth noting in connection to syntactic features is
the treatment of case syncretism and feature neutralization
(cf.~\crossrefchapteralt[Section~\ref{sec:case:syn}]{case}). The work by
\citet[Chapter~6]{Morrill94a-u}, \citet{Bayer96} and \citet{BJ95} mentioned
above proposed an approach
to feature neutralization by positing meet and join connectives (which
are like conjunction and disjunction in propositional logic) in CG.
The key idea of this approach was recast in HPSG by means of
inheritance hierarchies by \citet{levy:01}, \citet{lev:pol:01} and
\citet{dani:01}.\footnote{As noted by \citet{levy:01}, the
  type hierarchy-based rendering of ``meet'' and ``join''
  was first introduced in HPSG by \citet*[Section~6.3.2]{LHC2001a-u}.} See
\crossrefchaptert[Section~\ref{sec:case:syn}]{case} for an 
exposition of this HPSG work on feature neutralization.


\section{Specific empirical phenomena \label{phenomena}}

Part II of the present handbook contains an excellent introduction to
recent developments of HPSG research on major linguistic phenomena. I
will therefore presuppose familiarity with such recent analyses, and my discussion below aims to highlight the
differences between HPSG and CG in the analyses of selected
empirical phenomena. In order to make the ensuing discussion maximally
informative, I focus on phenomena over which there is some ongoing
major cross-theoretical debate, and those for which I believe one or
the other theory would benefit from recent developments/rich research
tradition in the other.


\subsection{Long-distance dependencies}

\enlargethispage{-6pt}
As noted in Section~\ref{sec:CCG}, CCG treats long-distance
dependencies via a sequence of Function Composition, which is similar
to the \slasch percolation analysis in HPSG. CCG offers a treatment of
major aspects of long-distance dependencies, including island effects
\citep[Section~4.2]{Steedman2000a-u} and parasitic gaps \citep{Steedman87a-u}.
Earlier versions of CCG involved a somewhat ad-hoc stipulation on the
use of crossed composition rules \citep{Steedman97a}. This was
overcome in the more recent, multi"=modal variant of CCG
\citep{Baldridge2002a-u}, which controls the application of such
non-order-preserving rules via a fine-grained system of
lexicalized modality. The modality specifications in this new version of CCG
enable one to relocate language-specific idiosyncrasies to the
lexicon, in line with the general spirit of lexicalist theories of
grammar.

The situation is somewhat different in TLCG. TLCG typically makes use
of a movement-like operation for the treatment of extraction
phenomena (via hypothetical reasoning), but the specific implementations differ considerably in
different variants of TLCG. Major alternatives include the approach in
terms of   ``structural control'' in Multi"=Modal Categorial Type
Logics (cf.~\citealt[Chapter~1]{BernardiPhD}; \citealt[Section~2.4]{Moortgat2011a-u};
see also \citealt[Chapter~7]{Morrill94a-u}), and the one involving prosodic
$\lambda$-binding in LCG and related approaches (see
Section~\ref{sec:extending}). In either approach, extraction phenomena
are treated by means of some form of hypothetical reasoning, and this
raises a major technical issue in the treatment of multiple gap
phenomena. The underlying calculus of TLCG is a version of linear
logic, and this means that the implication connective is resource
sensitive. This is problematic in situations in which a single filler
corresponds to multiple gaps, as in parasitic gaps and related
phenomena. These cases of extraction require some sort of extension of
the underlying logic or some special operator that is responsible for
resource duplication. Currently, the most detailed treatment of
extraction phenomena in the TLCG literature is \citet{morrilllp},
which lays out in detail an analysis of long-distance dependencies
capturing both major island constraints and parasitic gaps\is{gap!parasitic} within the
most recent version of Morrill's Displacement Calculus.

There are several complex issues that arise in relation to the
linguistic analysis of extraction phenomena. One major open question
is whether island constraints should be accounted for within narrow
grammar. Both Steedman and Morrill follow the standard practice in
Generative Grammar research in taking island effects to be syntactic,
but this consensus has been challenged by a new body of research in
the recent literature proposing various alternative explanations on
different types of island constraints (some important work in this
tradition includes \citew{deaneBook}, \citew{kluender98},
\citew{HS2010a-u} and \citew{CP2020a-u}; see
\crossrefchapterw{islands}, \citew{levine2017} and
\citew{newmeyer2016} for an overview of this line of work and pointers
to the relevant literature). Recent syntactic analyses of
long-distance dependencies in the HPSG literature explicitly avoid
directly encoding major island constraints within the grammar
\citep{Sag2010b,chaves12b}. Unlike CCG and Displacement Calculus,
Kubota \& Levine's Hybrid TLCG opts for this latter type of view (that
is, the one that is generally in line with recent HPSG work; see
\citealt[Chapter~10]{KubotaLevineBook}).

Another major empirical problem related to the analysis of
long"=distance dependencies is the so-called \term{extraction pathway
marking} phenomenon \citep{mccloskey79,Zaenen83a-u}. While this issue
received considerable attention in the HPSG literature, through a
series of work by Levine and Hukari (see \citealt{LH2006a}), there is
currently no explicit treatment of this phenomenon in the CG
literature. CCG can probably incorporate the HPSG analysis relatively
easily, given the close similarity between the \slasch percolation
mechanism and the step-by-step inheritance of the $/$NP specification
in the Function Composition-based approach in CCG. Extraction pathway
marking poses a much trickier challenge to TLCG, in which extraction
is typically handled by a single-chain movement-like process by means
of hypothetical reasoning (but see \citew[Chapter~7]{KubotaLevineBook}
for a sketch of a possible approach which mimics successive cyclic
movement in the type-logical setup).

Finally,\label{cg:page-pied-piping-start} \isi{pied-piping} poses a somewhat tricky issue for the
analysis of relativization in CG (see, for example,
\citealt{Pollard88a,Morrill94a-u};
\citealt[Section~8.6]{MuellerGT-Eng3}; see also
\crossrefchaptert[footnote~\ref{rc-fn-relative-word-cg}]{relative-clauses}). To see this point,
note that the analysis of (simple cases of) relative clauses in CCG
outlined in Section~\ref{abc2ccg} above does not straightforwardly
extend to pied-piping examples such as the following:

\ealnoraggedright
\label{piedP}
\ex\label{piedP-2}
    John is the only person to whom Mary told the truth.
\ex\label{piedP-1}
    Reports the height of the lettering on the covers of which the
    government prescribes should be abolished. \mbox{}\hfill{\mbox{\citep[109]{Ross67a}}}
\zl
In these examples, the relative pronoun is embedded inside
the fronted relative phrase, so, a simple \syncat{(N\ensuremath{\backslash}{}N)/(S/NP)}
assignment doesn't work.
\citet[Chapter~4, Section~3.3]{Morrill94a-u} proposes 
a more sophisticated treatment in TLCG (see \citealt[Section~9.7]{Carpenter98a-u}
for a lucid exposition of this analysis), which can be thought of as
a translation of the HPSG analysis \citep[Chapter~5]{ps2} involving two types
of long-distance dependency (handled by the
\REL and \SLASH features
in HPSG, see also \crossrefchaptert{relative-clauses}).

In Hybrid TLCG, Morrill's analysis of pied-piping
can be implemented by positing the following lexical entry for the
relative pronoun \textit{whom} (for examples such as (\ref{piedP-2}) where 
the fronted relative
phrase is an argument PP; the entry needs to be generalized to cover
other cases involving fronted elements with different syntactic categories): 

\begin{exe}
 \ex\label{piedPlex}
  \oneline{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greeks_1} \ensuremath{\lambda} \ensuremath{\greeks_2}. \ensuremath{\greeks_1}(whom) \ensuremath{\circ}\xspace \ensuremath{\greeks_2}(\E)}}{\sem{ \lambda F \lambda P \lambda Q \lambda x. P(F(x)) \ensuremath{ \wedge\xspace } Q(x)}}{\syncat{(N\ensuremath{\backslash}{}N)\vs (S\vs PP)\vs (PP\vs NP)}}}
\end{exe}
The entry in (\ref{piedPlex}) says that the relative pronoun takes two
arguments, a PP missing an NP inside itself and an S missing a PP, and
then becomes a nominal modifier. Note that the two types of
long-distance dependency mediated by \REL and \SLASH in HPSG are both handled
by the vertical slash in this analysis. The relative pronoun itself is
embedded inside the PP in the prosodic representation to form a relative phrase
which appears as a fronted expression in the surface string.

Since the vertical slash mediates long-distance dependencies, this
analysis avoids the problem of ad-hoc proliferation of lexical entries 
for pied-piped relative pronouns corresponding to different levels of
embedding (which was the main point of criticism in
\citeposs{Pollard88a} critique of an earlier CG analysis). In this
sense, this CG analysis is a fairly straightforward reimplementation
of the \citet{ps2} analysis. One possible difference between
the HPSG analysis 
and the CG analysis of the sort sketched above is that the latter
requires positing different lexical entries for relative pronouns
corresponding to different syntactic types of the relative phrase. If
it turns out that the constraints on what can be preposed are largely
orthogonal to narrow syntax,\footnote{The question of which syntactic
category can be pied-piped is actually a rather thorny issue. See
\crossrefchaptert[Section~\ref{rc:sec-internal-structure}]{relative-clauses} for some
discussion.} there may be an advantage for an analysis in HPSG that
posits a general PS rule or constructional schema for licensing pied-piping
relative clauses.\label{cg:page-pied-piping-end}


%At the end of section 3, I noted briefly that the reconceptualization
%of the Steedman/Dowty CCG analysis of NCC within TLCG by
%-Morrill94a-u/ and \citet{Carpenter98a-u} offers a more transparent view on
%the underlying patterns of the empirical phenomenon, and that the
%notion of proof normalization is at the heart of this
%reconceptualization. However, the relative degrees of difficulty that
%the extraction pathway marking phenomenon poses for CCG and TLCG may
%perhaps suggest that things are not so simple and that there are after
%all different empirical consequences between function
%composition-based analyses and hypothetical reasoning-based analyses
%of linguistic phenomena, which in a way is similar to the relationship
%between non-movement-based analyses and movement-based analyses of
%linguistic phenomena in the debate between HPSG and mainstream syntax.
%Here, I just note this point as an issue worth exloring further in
%future work.




\subsection{Coordination and ellipsis}

Coordination and ellipsis are both major issues in contemporary
syntactic theory. There are moreover some phenomena, such as Gapping
and Stripping, which seem to lie at the boundary of the two
empirical domains (see,  for example,  the recent overview
by \citealt{johnson-ellipsis-handbook}). There are some important similarities and
differences between analytic ideas entertained in the HPSG and CG
literature for problems in these empirical domains.


\subsubsection{Analyses of nonconstituent coordination  \label{NCCcomparison}}

CG is perhaps best known in the linguistics literature for its
analysis of nonconstituent coordination. Steedman's work on CCG
\citep{Steedman97a,Steedman2000a-u,steedman2012} in particular has
shown how this analysis of coordination interacts smoothly with
analyses of other major linguistic phenomena (such as long"=distance
dependencies, control and raising and quantification) to achieve a
surface"=oriented grammar that has wide empirical coverage and at the
same time has attractive computational properties.
\citet{kubota-levine-coord,KubotaLevineBook} offer an up-to-date TLCG
analysis of coordination, and compare it with major alternatives in
both the CCG and HPSG literature.

%A more recent line of work by Kubota \& Levine \cmt{ref} propose a version of TLG called
%Hybrid TLCG, which aims to offer comparable empirical coverage but with
%somewhat different perspective on various issues (such as the
%relationship between grammar proper and extra-grammatical factors
%pertaining to island effects).

As compared to long-distance dependencies, coordination (in particular
NCC) has received considerably less attention in the (H)PSG literature
initially (\citealt{sgww} is an important exception in the early
literature). Things started to change somewhat around 2000, with a
series of related proposals appearing one after another, including
\citet{Yatabe2001a}, \citet{BS2004a}, \citet{chaves07} and
\citet{Crysmann2003c} (see
\crossrefchapteralt[Section~\ref{sec-non-constituent-coordination}]{coordination} and \crossrefchapteralt[Section~\ref{ellipsis:sec-analyses-of-noncon}]{ellipsis}). Here, I take up
\citet{BS2004a} and \citet{Yatabe2001a} (updated in
\citealt{yatabe-tam2017}) as two representative proposals in this line
of work. The two proposals share some common assumptions and ideas,
but they also differ in important respects.

Both \citet{BS2004a} and \citet{Yatabe2001a} adopt linearization-based HPSG, together
with (a version of) Minimal Recursion Semantics for
semantics. Of the two, Beavers \& Sag's
analysis is more in line with standard assumptions in HPSG. The basic
idea of Beavers \& Sag's analysis is indeed very simple: by exploiting the flexible
mapping between the combinatoric component and the surface word order
realization in linearization-based HPSG, they essentially propose a
surface deletion-based analysis of NCC according to which NCC examples
are analyzed as follows:
%
\begin{exe}
 \ex\label{BeaversSag}\relax
  [\textsubscript{S} Terry gave no man a book on Friday] or
  [\textsubscript{S} \sout{Terry gave no man} a record on Saturday]. 
\end{exe}
where the material in strike-out is underlyingly present but undergoes
deletion in the prosodic representation.

In its simplest form, this analysis gets the scopal relation between
the quantifier and coordination wrong in examples like (\ref{BeaversSag}) (a
well-known problem for the conjunction reduction analysis from the
70s; cf.\ \citealt{partee70}).
% whole paper 
Beavers \& Sag address this issue by introducing a constraint called
\term{Optional Quantifier Merger}:
%
\begin{exe}
 \ex\label{OQM}
  \term{Optional Quantifier Merger}: For any elided phrase 
  denoting a generalized quantifier in the 
  domain of either conjunct, the semantics of that phrase may
  optionally be identified with the semantics of its non-elided
  counterpart.
\end{exe}
As noted by \citet{levine11}
% whole paper
and \citet[Section~3.2.1]{kubota-levine-coord}, this condition
does not follow from any general principle and is merely stipulated in
\citeauthor{BS2004a}'s account.

%\largerpage[-1]
\citet{Yatabe2001a} and \citet{yatabe-tam2017} (the latter of which contains a
much more accessible exposition of essentially the same proposal as
the former) propose a somewhat different analysis. Unlike \citeauthor{BS2004a}, who
assume that semantic composition is carried out on the basis of the
meanings of \emph{signs} on each node (which is the standard assumption
about semantic composition in HPSG), Yatabe shifts the locus of
semantic composition to the list of domain objects, that is, the
component that directly gets affected by the deletion operation that
yields the surface string.
\pagebreak

This crucially changes the default meaning predicted for examples such
as (\ref{BeaversSag}). Specifically, on Yatabe's analysis, the surface
string for (\ref{BeaversSag}) is obtained by the ``compaction''
operation on word order domains that collapses two quantifiers
originally contained in the two conjuncts into one. The semantics of
the whole sentence is computed on the basis of this resultant word
order domain representation, which contains only \emph{one} instance
of a domain object corresponding to the quantifier. The quantifier is
then required to scope over the whole coordinate structure due to
independently motivated principles of underspecification resolution.
While this approach successfully yields the wide-scope reading for
quantifiers, the distributive, narrow scope reading for quantifiers
(which was trivial for \citeauthor{BS2004a}) now becomes a challenge.
\citeauthor{yatabe-tam2017} simply stipulate a complex disjunctive constraint on semantic
interpretation tied to the ``compaction'' operation that takes place in
coordination so as to generate the two scopal readings.

%\largerpage[-1]
\citet[Section~3.2.2]{kubota-levine-coord} note that, in addition to the quantifier
scope issue noted above, \citeauthor{BS2004a}'s approach suffers from similar problems
in the interpretations of symmetrical predicates (\textit{same},
\textit{different}, etc.), summative predicates (\textit{a total of
X}, \textit{X in total}, etc.) and the so-called ``respective'' readings
of plural and conjoined expressions (see \citealt{chaves12} for a lucid
discussion of the empirical parallels between the three phenomena and
how the basic cases can receive a uniform analysis within HPSG).
\citet{yatabe-tam2017} offer a response to \citeauthor{kubota-levine-coord}, working out explicit
analyses of these more complex phenomena in linearization-based HPSG.
A major point of disagreement between \citeauthor{kubota-levine-coord} on the other
hand and \citeauthor{yatabe-tam2017} on the other seems to be
whether/to what extent an analysis of a linguistic phenomenon
should aim to explain (as opposed to merely account for)
linguistic generalizations. There is no easy answer to this
question, and it is understandable that different theories put
different degrees of emphasis on this goal.
Whatever conclusion one draws from this recent HPSG/CG debate on
the treatment of nonconstituent coordination, one point seems 
relatively uncontroversial: coordination continues to constitute a challenging
empirical domain for any grammatical theory,
consisting of both highly regular patterns such as systematic
interactions with scopal operators
\citep{kubota-levine-coord,KubotaLevineBook} and puzzling
idiosyncrasies, the latter of which includes the summative agreement
facts \citep{postalextraction,yatabe-tam2017} and extraposed relative
clauses with split antecedents
\citep{PerlmutterRoss70,Link84a-u,Kiss2005a,yatabe-tam2017}.


\subsubsection{Gapping and Stripping}
\label{sec:gapping}

Descriptively, Gapping\is{gapping|(} is a type of ellipsis phenomenon that occurs in
coordination and which deletes some material including the main
verb:\footnote{There is some disagreement as to whether Gapping is
restricted to coordination. \citet{kubota-levine-gapping}, following
authors such as \citet{johnson2009}, take Gapping to be restricted to
coordination. \citet{parkea18gapping} and \citet{parkDiss} take a
different view, and argue that Gapping should be viewed as a type of
ellipsis phenomenon that is not restricted to coordination
environments. See \citet[46--47]{KubotaLevineBook} for a response to
\citet{parkea18gapping}.}

\begin{exe}
 \ex\label{gapping}
  \begin{xlist}
 \ex\label{ex-leslie-bought-a-cd}
    Leslie \emph{bought} a CD, and Robin   \ensuremath{\varnothing}  a book.
 \ex
    Terry \emph{can go} with me, and Pat  \ensuremath{\varnothing}  with you.
 \ex
    John \emph{wants to try to begin to write} a novel, and Mary  \ensuremath{\varnothing}  a play.
  \end{xlist}
\end{exe}
Gapping has invoked some theoretical controversy in the recent
HPSG/CG literature for the ``scope anomaly'' issue that it exhibits.
The relevant data involving auxiliary verbs such as (\ref{mrj1}) and (\ref{ex2b})
have long been known in the literature since
\citet{oehrle71,oehrle1987} and 
\citet{siegel87}. \citet[247]{mccawley1993} later pointed out similar examples
involving downward-entailing quantifiers of the sort exemplified by (\ref{mccawley}).

\begin{exe}
 \ex\label{scope}
  \begin{xlist}
 \ex\label{mrj1}
    Mrs.\ J can't live in Boston and Mr.\ J  \ensuremath{\varnothing}  in LA.
 \ex\label{ex2b}
    Kim didn't play bingo or Sandy  \ensuremath{\varnothing}  sit at home all evening.
 \ex\label{mccawley}
    No dog eats Whiskas or  \ensuremath{\varnothing}  cat  \ensuremath{\varnothing}  Alpo.
  \end{xlist}
\end{exe}
The issue here is that (\ref{mrj1}), for example, has a reading in
which the modal \textit{can't} scopes over the conjunction (`it's not
possible for Mrs.~J to live in NY and Mr.~J to live in LA at the same
time'). This is puzzling, since such a reading wouldn't be predicted
on the (initially plausible) assumption that Gapping sentences are interpreted by simply supplying the meaning of the missing material
in the right conjunct.

\largerpage
\citet{kubota-levine-gapping} and 
\citet[Section~3.1]{KubotaLevineBook} note some
difficulties for earlier accounts of Gapping in the (H)PSG literature
\citep{sgww,abeille-ea} and argue for a constituent coordination
analysis of Gapping in TLCG, building on earlier analyses of Gapping
in CG \citep{Steedman90a-u,hendriks95,morrillsolias93}. The key idea
of Kubota \& Levine's analysis involves taking Gapping as coordination of clauses
missing a verb in the middle, which can be transparently represented
as a function from strings to strings of category {\syncat{S\vs
    ((NP\ensuremath{\backslash}{}S)\ensuremath{/}NP)}} (for (\ref{ex-leslie-bought-a-cd}), for example):

\begin{exe}
 \ex
  \LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. leslie \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace a \ensuremath{\circ}\xspace cd }}{\sem{ \lambda R. \exists x. \trns{cd}(x) \ensuremath{ \wedge\xspace } R(x)(\trns{l}) }}{\syncat{S\vs ((NP\ensuremath{\backslash}{}S)/NP)}}
\end{exe}
A special type of conjunction entry (prosodically of type
(\yusukest\xspace \shortarrow \yusukest\xspace) \shortarrow (\yusukest\xspace
\shortarrow \yusukest\xspace) \shortarrow (\yusukest\xspace \shortarrow \yusukest\xspace))
then conjoins two such expressions
and returns a conjoined sentence missing the verb only in the first
conjunct (on the prosodic representation). By feeding the verb to
this resultant expression,  a proper form-meaning  pair is obtained
for Gapping sentences like those in (\ref{gapping}).

The apparently unexpected wide scope readings for auxiliaries and
quantifiers in (\ref{scope}) turn out to be straightforward on this
analysis. I refer the interested reader to
\citet{kubota-levine-gapping} (and \citet[Chapter~3]{KubotaLevineBook}) for
details, but the key idea is that the apparently anomalous scope in
such examples isn't really anomalous on this approach, since the
auxiliary (which prosodically lowers into the first conjunct) takes
the whole conjoined gapped clause as its argument in the combinatoric
component underlying semantic interpretation.\footnote{This is
  essentially a formalization of an idea that goes back to
  \citeposs{siegel87} work.} Thus, the existence
of the wide scope reading is automatically predicted.
\citet{puthawala2018} extends this approach to a similar ``scope anomaly''
data found in Stripping, in examples such as the following:

\begin{exe}
 \ex   John didn't sleep, or Mary (either).
\end{exe}
\largerpage%\enlargethispage{4pt}
Just like the Gapping examples in (\ref{scope}), this sentence has
both wide scope (`neither John nor Mary slept') and narrow scope
(`John was the one who didn't sleep, or maybe that was Mary')
interpretations for negation.

The determiner gapping example in (\ref{mccawley}) requires a somewhat more
elaborate treatment. \citet{kubota-levine-gapping} analyze determiner
gapping via higher-order functions. \citet{morrillvalentin16} criticize this
approach for a certain type of overgeneration problem regarding word
order and propose an alternative analysis in Displacement Calculus.

\citet{parkea18gapping} and \citet{parkDiss} propose an analysis of Gapping in HPSG that overcomes
the limitations of previous (H)PSG analyses (\citealt[Section~4.3]{sgww};
\citealt{chaves05,abeille-ea}), couched in Lexical Resources Semantics.  In Park et al.'s analysis,
the lexical entries of the clause-level conjunction words \textit{and} and \textit{or} are
underspecified as to the relative scope between the propositional operator contributed by the modal
auxiliary in the first conjunct and the Boolean conjunction or disjunction connective that is
contributed by the conjunction word itself. Park et al.\ argue that this is sufficient for capturing
the scope anomaly in the Oehrle/Siegel data such as (\ref{mrj1}) and (\ref{ex2b}). Extension to the
determiner gapping case (\ref{mccawley}) is left for future work.

Here again, instead of trying to settle the debate, I'd like to draw
the reader's attention to the different perspectives on grammar that
seem to be behind the HPSG and (Hybrid) TLCG approaches. Kubota \&
Levine's approach attains \pagebreak{}theoretical elegance at the cost of
employing abstract higher-order operators (both in semantics and
prosody). This makes the relationship between the competence grammar
and the on-line human sentence processing model indirect, and
relatedly, it is likely to make efficient computational implementation
less straightforward (for a discussion on the relationship between
competence grammar and a model of sentence processing, see
\crossrefchapteralt{processing} and \crossrefchapteralt[Section~\ref{sec-minimalism-processing}]{minimalism}).
\citeposs{parkea18gapping} approach, on the 
other hand, is more in line with the usual practice (and the shared
spirit) of HPSG research, where the main emphasis is on writing an
explicit grammar fragment that is constraint-based and
surface-oriented. This type of tension is perhaps not easy to
overcome, but it seems useful (for researchers working in different
grammatical theories) to at least recognize (and appreciate) the
existence of these different theoretical orientations tied to
different approaches.%
\is{gapping|)}



\subsubsection{Ellipsis}

Analyses of major ellipsis phenomena in HPSG and CG share the same
essential idea that ellipsis is a form of anaphora, without any
invisible hierarchically structured representations corresponding to
the ``elided'' expression. See \crossrefchaptert{ellipsis} and
\citet{ginzburg-miller-ellipsis-handbook} for an
overview of approaches to ellipsis in HPSG.

\largerpage
Recent analyses of ellipsis in HPSG (\citealt[Chapter~8]{GSag2000a-u}; \citealt{millereisspseudo})
make heavy use of the notion of ``construction'' adopted from
Construction Grammar (this idea is even borrowed into some of the CG
analyses of ellipsis such as \citealt{Jacobson2016}). Many ellipsis phenomena
are known to exhibit some form of syntactic sensitivity
\citep{kennedy2003,chung13,yoshida-ea-pg}, and this fact has long been
taken to provide strong evidence for the ``covert structure'' analyses
of ellipsis popular in Mainstream Generative Grammar \citep{merchant13}.

Some of the early works on ellipsis in CG include
\citet{hendriks-diss} and \citet{morrillmerenciano1996}.
\citet{morrillmerenciano1996} in particular show how hypothetical
reasoning in TLCG allows treatments of important properties of
ellipsis phenomena such as strict/sloppy ambiguity and scope ambiguity
of elided quantifiers in VP ellipsis. \citet{jaeger05} integrates
these earlier works with a general theory of anaphora in TLCG,
incorporating the key empirical analyses of pronominal anaphora by
\citet{Jacobson1999a,Jacobson2000a}. Jacobson's
\citeyearpar{Jacobson_p1998a,Jacobson2008} analysis of
Antecedent-Contained Ellipsis is also important. Antecedent-Contained
Ellipsis is often taken to provide a strong piece of evidence for the
representational analysis of ellipsis in Mainstream Generative Syntax.
Jacobson offers a counterproposal to this standard analysis that
completely dispenses with covert structural representations. While the
above works from the 90s have mostly focused on VP ellipsis, recent
developments in the CG literature, including \citet{barker-sluicing}
on sluicing, \citet{Jacobson2016} on fragment answers and
\citet{kubota-levine-pseudo} on pseudogapping, considerably extended
the empirical coverage of the same line of analysis.

The relationship between recent CG analyses of ellipsis and
HPSG counterparts seems to be similar to the situation with competing
analyses on coordination. Both \citet{barker-sluicing} and
\citet{kubota-levine-pseudo} exploit hypothetical reasoning to treat the
antecedent of an elided material as a ``constituent'' with full-fledged
semantic interpretation at an abstract combinatoric component of
syntax. The anaphoric mechanism can then refer to both the syntactic
and semantic information of the antecedent expression to capture
syntactic sensitivity observed in ellipsis phenomena, without the need
to posit hierarchical representations at the ellipsis site. Due to its
surface-oriented nature, HPSG is not equipped with an analogous
abstract combinatoric component that assigns ``constituent'' status to
expressions that do not (in any obvious sense) correspond to
constituents in the surface representation. In HPSG, the major work in
restricting the possible form of ellipsis is instead
taken over by constructional schemata, which can encode
syntactic information of the antecedent to capture connectivity
effects, as is done, for example, with the use of the 
\textsc{sal-utt} feature in \citeauthor{GSag2000a-u}'s 
(\citeyear[Chapter~8]{GSag2000a-u}) analysis of sluicing
(cf.~\crossrefchapteralt{ellipsis}).

\largerpage
\citet[Chapter~8]{KubotaLevineBook} extend 
\citeposs{kubota-levine-pseudo} approach further to the treatment of
interactions between  VP ellipsis and extraction, which has often been
invoked in the earlier literature (in particular, \citealt{kennedy2003})
as providing crucial evidence for  covert structure analysis of
ellipsis phenomena (see also \citealt{Jacobson14ellipsis} for a related
proposal, cast in a variant of CCG).
At least some of the counterproposals that Kubota \& Levine formulate
in their argument against the covert structure analysis seem to be
directly compatible with the HPSG approach to ellipsis,  but (so far
as I am aware) no concrete analysis of extraction/ellipsis
interaction currently exists in the HPSG
literature.


\subsubsection{Mismatches in right-node raising}

While right-node raising (RNR) has mostly been discussed in connection to coordination in
the literature, it is well-known that RNR is not necessarily
restricted to coordination environments (see, for example, \citealt{wilder-ellipsis-handbook}
for a recent overview). Moreover, it
has recently been pointed out by \citet{ACS2016a-u} and \citet{SAHM2019a-u} that RNR 
admits certain types of syntactic mismatch between the RNR'ed material
and the selecting head in a non-adjacent conjunct.
The current literature seems to agree that RNR is not a unitary
phenomenon, and that at least some type of RNR should be treated via a
mechanism of surface ellipsis, which could be modeled as deletion of
syntactic (or prosodic) objects or via some sort of anaphoric
mechanism (cf.\ \crossrefchapteralt[Section~\ref{ellipsis:sec-rnr}]{ellipsis},
\citealt{Chaves2014a-u}, 
\citealt{SAHM2019a-u}; see also \citealt[footnote 15]{kubota-levine-pseudo}).

One point that is worth emphasizing in this connection is that
while the ``NCC as constituent coordination'' analysis of RNR in CG
discussed in Section~\ref{preCCG} (major evidence for which comes from the
interactions between various sorts of scopal operators and RNR as
noted in Section~\ref{NCCcomparison}) is well-known, neither CCG nor TLCG is by
any means committed to the idea that \emph{all} instances of RNR should be
analyzed this way.
In fact, given the extensive evidence for the non-unitary nature of
RNR reviewed in \citet{Chaves2014a-u} and the syntactic mismatch data from
\ili{French} offered by \citet{ACS2016a-u} and \citet{SAHM2019a-u}, it seems
that a comprehensive account of RNR in CG (or, for that matter, in any
other theory) would need to recognize the
non-unitary nature of the phenomenon, along lines similar to
\citeposs{Chaves2014a-u} recent proposal in HPSG. While there is currently no
detailed comprehensive account of RNR along these lines in the
CG literature, there does not  seem to be any inherent 
obstacle for formulating such an account.


\subsection{Binding}
\label{cg:binding}

\is{Binding Theory|(}%
Empirical phenomena that have traditionally been analyzed by means of
Binding Theory (both in the transformational and the
non-transformational literature; cf.~\crossrefchapteralt{binding})
potentially pose a major challenge to the
``non-representational'' view of the syntax-semantics interface common
to most variants of CG. The HPSG Binding Theory in \textcites{PS92a}[Chapter~6]{ps2}
captures Principles A\is{principle!Binding!Principle A} and B\is{principle!Binding!Principle B} at the level of argument structure, while
Principle C makes reference to the configurational structure (i.e.\
the feature-structure encoding of the constituent geometry). The
status of Principle C\is{principle!Binding!Principle C} itself is controversial to begin with, but if
this condition needs to be stated in the syntax, it would possibly
constitute one of the greatest challenges to CG-based theories of
syntax, since, unlike phrase structure trees, the proof trees in CG
are not objects that a principle of grammar can directly refer to.

While there seems to be no consensus in the current CG literature on
how the standard
facts about binding theory are to be accounted for, there are
some important ideas and proposals in the wider literature of CG-based
syntax (broadly construed to include work in the Montague Grammar
tradition). First, as for Principle A\is{principle!Binding!Principle A}, there is a recurrent suggestion
in the literature that these effects can (and should) be captured
simply via strictly lexical properties of reflexive pronouns
(e.g.\ \citealt{keenan88binding,szabolcsi1992}; see \citealt[43--44]{buringbinding} for a concise summary).
For example, for a reflexive in the direct object position
of a transitive verb bound by the subject NP,
the following type assignment (where the reflexive pronoun first takes
a transitive verb and then the subject NP as arguments) suffices to
capture its bound status:

\begin{exe}
 \ex\label{SELF}
 \LexEnt{\pt{himself }}
        {\sem{ \lambda R \lambda x. R(x)(x)}}
        {\syncat{((NP\ensuremath{\backslash}S)\ensuremath{/}NP)\ensuremath{\backslash}NP\ensuremath{\backslash}{}S}}
\end{exe}

\noindent
This approach is attractively simple, but there are at least two
things to keep in mind, in order to make it a complete analysis of
Principle A\is{principle!Binding!Principle A} in CG. First, while this 
lexical treatment of reflexive binding may at first sight 
appear to capture the locality of binding quite nicely, CG's flexible
syntax potentially overgenerates unacceptable long-distance binding
readings for (\ili{English}) reflexives. Since RNR can take
place across clause boundaries,  it seems necessary to
assume that hypothetical reasoning for the Lambek-slash
(or a chain of Function Composition that has the same effect in CCG)
can generally take place across clause boundaries. But then,
expressions such as \textit{thinks Bill hates} can be assigned
the same syntactic type (i.e.\  \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/}NP}) as lexical transitive verbs,
overgenerating non-local binding of a reflexive from a subject NP in
the upstairs clause (*~\textit{John\ensuremath{_i} thinks Bill hates {himself\ensuremath{_i}}}).

In order to prevent this situation while still retaining the lexical
analysis of reflexivization sketched above, some kind of restriction
needs to be imposed as to the way in which reflexives combine with
other linguistic expressions. One possibility would be to distinguish
between lexical transitive verbs and derived transitive verb-like
expressions by positing different ``modes of composition'' in the two
cases in a ``multi"=modal'' version of CG.

The other issue is that the lexical entry in (\ref{SELF}) needs to be
generalized to cover all cases in which a reflexive is bound by an
argument that is higher in the \isi{obliqueness} hierarchy. This amounts to
positing a polymorphic lexical entry for the reflexive. The use of
polymorphism is not itself a problem, since it is needed in other
places in the grammar (such as coordination) anyway. But this account
would amount to capturing the Principle A\is{principle!Binding!Principle A} effects purely in terms of the 
specific lexical encoding for reflexive pronouns (unlike the treatment
in HPSG which explicitly refers to the \isi{obliqueness} hierarchy).

While Principle A effects are in essence amenable to a 
relatively simple lexical treatment along lines sketched above, Principle B turns out to be
considerably more challenging for CG. To see this point, note that the
lexical analysis of reflexives sketched above crucially relies on the
fact that the constraint associated with reflexives corresponds to a
straightforward semantic effect of variable binding. Pronouns instead
require \emph{disjointness} of reference from less oblique co-arguments, but
such an effect cannot be captured by simply specifying some
appropriate lambda term as the semantic translation for the pronoun.

To date, the most detailed treatment of Principle B\is{principle!Binding!Principle B} effects in CG that
explicitly addresses this difficulty is the proposal by
\citet{Jacobson07}, formulated in a version of CCG
(\citealt{Steedman97a} proposes a different approach to binding,
which will be briefly discussed at the end of this section). The key
idea of Jacobson's account of Principle B effects is that NPs are
divided by a binary-valued feature $\pm$p, with pronouns marked NP[+p]
and all other NPs NP[$-$p]. In all lexical entries of the form in
(\ref{le}), all NP (and PP) arguments in any realization of $/$\$ are
specified as [$-$p].\footnote{Here, $/$\$ is an abbreviation of a
  sequence of argument categories sought via $/$. Thus, VP$/$\$ can be
  instantiated as VP/NP, VP/NP/NP, VP/PP/NP, etc.}

\begin{exe}
 \ex\label{le}
  \LexEnt{\pt{\ptv{k}}}{\sem{ P}}{\syncat{VP\ensuremath{/}\$}}
\end{exe}
%\largerpage
The effect of this restriction is to rule out pronouns from argument
positions of verbs with ordinary semantic denotations. On this
approach, the only way a lexically specified functional category can
take [+p] arguments is via the application of the following
irreflexive operator:\footnote{For expository purposes, I state the
operator in (\ref{def}) in its most restricted form, dealing with only the
case where there is a single syntactic argument apart from the
subject. A much broader coverage is of course necessary in order to
handle cases like the following:

\begin{exe}
 \exi{(i)}
  \begin{xlist}
 \ex[*]{
    John\ensuremath{_i} warned Mary about him\ensuremath{_i}.
 }
 \ex[*]{
   John talked to Mary\ensuremath{_i} about her\ensuremath{_i}.
 }
 \ex[*]{
    John explained himself\ensuremath{_i} to him\ensuremath{_i}.
 }
 \end{xlist}
\end{exe}
What is needed in effect is a schematic type specification that applies
to a pronoun in any or all argument positions, i.e., stated on an
input of the form VP\ensuremath{/}\$\ensuremath{/}XP[$-$p]\ensuremath{/}\$ to yield an output of 
the form VP\ensuremath{/}\$\ensuremath{/}XP[+p]\ensuremath{/}\$. To ensure the correct implementation
of this extension, some version of the ``wrapping'' analysis needs to be
assumed (cf.\ \citealt[194]{Jacobson07}), so that the order of the arguments in 
verbs' lexical entries is isomorphic to the obliqueness hierarchy (of
the sort discussed by \citealt{PS92a}).

Cases such as the following also call for an extension (also a
relatively straightforward one):

\begin{exe}
 \exi{(ii)}[*]{
  John\ensuremath{_i} is proud of him\ensuremath{_i}.
}
\end{exe}
By assuming (following \citealt{Jacobson07}) that the [$\pm$p] feature
percolates from NPs to PPs and by generalizing the irreflexive
operator still further so that it applies not just to VP\ensuremath{/}XP[$-$p] but to
AP\ensuremath{/}XP[$-$p] as well, the ungrammaticality of (ii) follows
straightforwardly.}

\begin{exe}
 \ex\label{def}
  \LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. \ensuremath{\greekp}}}{\sem{  \lambda f \lambda u \lambda v. f(u)(v), \mbox{\grey{\sem{ u \neq v }}} }}{\syncat{(VP\ensuremath{/}NP[+p])\vs (VP\ensuremath{/}NP[\ensuremath{-}p])}}
\end{exe}
The greyed-in part \sem{  u \neq v  } %
{}%
separated from the truth
conditional meaning by a comma is a presupposition introduced by the
pronoun-seeking variant of the predicate. It says that the subject and
object arguments are forced to pick out different objects in the
model. For the semantics of pronouns themselves, one can assume,
following the  standard practice, that free (i.e.\  unbound)
pronouns are simply translated as arbitrary variables (cf.\ \citealt{cooper79}).

Crucially, the operator in (\ref{def}) is restricted in its domain of
application to the set of signs which are specified in the lexicon. I 
notate this restriction by using the dashed line notion in 
what follows. Then (\ref{condBex}) will be derived as in (\ref{proof}).

\ea
\label{condBex}
John praises him.
\z

\begin{exe}\ex\label{proof}%
\attop{\oneline{\begin{prooftree}[separation=.5em, label separation=.25em]%\footnotesize
\hypo{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp}. \ensuremath{\greekp} }; \\ \sem{  \lambda f \lambda u \lambda v. f(u)(v), u \neq v}; \\\syncat{(VP\ensuremath{/}NP[+p])\vs (VP\ensuremath{/}NP[\ensuremath{-}p])}}}
\hypo{\MultiLine{\mathsf{ \ptfont praises }; \\ \sem{ \trns{praise} }; \syncat{VP\ensuremath{/}NP[\ensuremath{-}p]}}}
\infer[dashed]2{\LexEnt{\pt{praises}}{\sem{ \lambda u \lambda v. \trns{praise}(u)(v), u \neq v}}{\syncat{VP\ensuremath{/}NP[+p]}}}
\hypo{\LexEnt{\pt{him}}{\sem{ z}}{\syncat{NP[+p]}}}
\infer2{\LexEnt{\pt{praises \ensuremath{\circ}\xspace him}}{\sem{ \lambda v. \trns{praise}(z)(v), z \neq v}}{\syncat{VP}}}
\hypo{\LexEnt{\pt{john}}{\sem{ \trns{j} }}{\syncat{NP[\ensuremath{-}p]}}}
\infer2{\LexEnt{\pt{john \ensuremath{\circ}\xspace praises \ensuremath{\circ}\xspace him}}{\sem{  \trns{praise}(z)(\trns{j}), z \neq  \trns{j} }}{\syncat{S}}}
\end{prooftree}}}
\end{exe}

\noindent
The presupposition \sem{   z \neq \trns{j}   } %
{}%
ensures that the
referent of the pronoun is different from John.

Thus, Jacobson's approach captures the relevant conditions on the
interpretation of pronouns essentially as a type of lexical
presupposition tied to the denotation of the pronoun-taking verb,
and the syntactic feature [$\pm$p] mediates the
distributional correlation between the pronoun and the verb that
subcategorizes for it. The idea is
essentially the same as in the HPSG Binding Theory, except that the
relevant condition is directly encoded as a restriction on the
denotation itself, since the standard CG syntax-semantics interface
does not admit of syntactic indices of the sort assumed in HPSG.

Unlike Jacobson's proposal outlined above, \citeauthor{Steedman97a}'s 
(\citeyear[Chapter~2]{Steedman97a})
analysis of binding conditions in CCG recognizes the syntactic forms
of the logical language that is used to write the denotations of
linguistic expressions as the ``level'' at which binding conditions are
stated. This approach can be thought of as a ``compromise'' which enables
a straightforward encoding of the HPSG-style Binding Conditions by
(slightly) deviating from the CG doctrine of not admitting any
representational object at the syntax-semantics interface (see
\citealt{Dowty97a-u} for a critique of
the approach to binding by \citealt{Steedman97a}
discussing this issue clearly).

Steedman's approach can be best illustrated by taking a look at the analysis of
(\ref{condBex2}).\footnote{At the same time that
he formulates an essentially syntactic account of Principle B via the
term \trns{pro} in the translation language, \citet[29]{Steedman97a} briefly
speculates on the (somewhat radical) possibility of relegating Principle B entirely to
the pragmatic component of pronominal anaphora resolution. However,
the relevant discussion is rather sketchy, and the details of such a
pragmatic alternative are not entirely clear.}

\ea[*]{\label{condBex2}
Every student\ensuremath{_i} praised him\ensuremath{_i}. 
}
\z
According to Steedman, pronouns receive translations of the form
\sem{  \trns{pro}(x)  }, %
{}%
where \trns{pro} is effectively a term that marks the
presence of (the translation of) a pronoun at some particular
syntactic position in the logical formula that represents the meaning
of the sentence.

With this assumption, the translation for (\ref{condBex2}) that needs to be
ruled out (via Principle B\is{principle!Binding!Principle B}) is as follows:

\begin{exe}
 \ex\label{lf}
  \sem{   \forall x [ \trns{student}(x) \ensuremath{ \rightarrow } \trns{praise}(\trns{pro}(x))(x)]   } 
{}%
\end{exe}
And this is where the CCG Binding Theory kicks in. The relevant part
of the structure of the logical formula in (\ref{lf}) can be more
perspicuously written as a tree as in Figure~\ref{fig-lf-tree}, which makes clear the
hierarchical relation between sub-terms.\footnote{Since binding
  conditions are stated at the level of the translation language, this
  approach raises the issue of whether it can correctly capture the
  binding relations in constructions in which there is a mismatch between the surface argument
  structure and the underlying semantics, such as in 
  subject-to-object raising constructions
  (\textit{John$_i$ believes himself$_i$ to be a descendant of
    Beethoven}). \citet{Steedman97a} does not  contain an explicit
  discussion on this type of data, but it seems likely that one will
  need to assume a particular syntax for the translation language in
  order to  accommodate this type of data in his approach.}
\begin{figure}
\begin{forest}
nice empty nodes
[{} [{} [\trns{praise}] 
    [{} 
      [\trns{pro}]
      [ $x$ ] ]] 
  [$x$] ]
\end{forest}
\caption{Logical formula as a tree}\label{fig-lf-tree}
\end{figure}
Principle B states that pronouns need to be locally free. Figure~\ref{fig-lf-tree}
violates this condition since there is a locally c-commanding term 
$x$ that binds \sem{  \trns{pro}(x)  } %
{}%
(where a term $\alpha$ binds 
term $\beta$ when they are semantically bound by the same operator).

Principles A\is{principle!Binding!Principle A} and C\is{principle!Binding!Principle C} are formulated similarly by making crucial
reference to the structures of the terms that represent the semantic
translations of sentences.

% A couple of comments are in order regarding this treatment of Binding
% Conditions by \citet{Steedman97a}. First, as discussed very clearly by
% \citet{Dowty97a-u}, and as should already be clear from the comparison with
% \citeposs{Jacobson07} account of Principle B, Steedman's proposal effectively
% treats logical terms denoting semantic translations as a level of
% representation that grammatical constraints make reference to, at
% least for the purpose of binding principles. This is at odds with the
% standard assumption in CG research, which takes such terms merely as
% notational devices for writing the denotations of linguistic
% expression (which are standardly taken to be model theoretic objects
% in truth-conditional semantics) in a form that is easy to read for the
% linguist. 

% Second, though it may deviate from what one might call the `CG
% orthodoxy' for the reason noted  above, Steedman's approach 
% bears a close resemblance to the HPSG Binding Theory 
% which is stated in terms of  lexically encoded argument structure. 
% In fact, the only substantial difference is that instead of 
% formulating the condition at the level of syntactically encoded
% argument structure, Steedman's approach states the relevant conditions
% via the ``predicate-argument structure'' of the logical predicates 
% (such as the two-place predicate \trns{praise})  directly.  

What one can see from the comparison of different approaches to binding
in CG and the treatment of binding in HPSG is that although HPSG and
CG are both lexicalist theories of syntax, and there is a general
consensus that binding conditions are to be formulated lexically
rather than configurationally, there are important differences in the
actual implementations of the conditions between approaches that stick to
the classical Montagovian tradition (embodying the tenet of
``direct compositionality'' in Jacobson's terms) and those that make use of
(analogues of) representational devices more liberally.

% This seems to be related to
% the tendency of HPSG and (Steedman's) CCG to make more liberal use of
% underspecified semantic representations as opposed to other variants
% of CG that generally eschew such intermediate representations.

% This may be ultimately related to different perspectives on the
% architecture of the syntax-semantics interface and the notion of
% compositionality in the semantics of natural language.

Finally, some comments are in order regarding the status of
Principle C, the part of Binding Theory that is
supposed to rule out examples such as the following:

\eal
\ex[*]{He\ensuremath{_i} talked to John\ensuremath{_i}.}
\ex[*]{He\ensuremath{_i} talked to John's\ensuremath{_i} brother.}
\zl

The formulation of Principle C\is{principle!Binding!Principle C} has always been a problem in lexicalist theories of syntax. While
Principles A and B can be stated by just making reference to the local argument structure of a
predicate in the lexicon, the global nature of Principle C seems to require looking at the whole
configurational structure of the sentence in which the referring term appears (but see
\citet{Branco2002a} for an alternative view; see also \crossrefchaptert{binding}). In fact,
\textcites{PS92a}[Chapter~6]{ps2} opt for this solution, and their definition of the Principle C has
a somewhat exceptional status within the whole theory (which otherwise adheres to strict locality
conditions) in directly referring to the configurational structure.

Essentially the same problem arises in CG. \citeposs{Steedman97a} formulation
of Principle C\is{principle!Binding!Principle C} can be thought of as an analog of Pollard \&
Sag's \citeyearpar{PS92a,ps2} proposal,
where global reference to hierarchical structure is made not at the
level of phrase structure, but instead at the level of
``logical structure'', that is, in the syntactic structure of the logical
language used for writing the meanings of natural language
expressions. As already noted above, if one takes the Montagovian, or
``direct compositional'', view of the syntax-semantics interface that is
more traditional/standard in CG research, this option is unavailable.

% What this means is that if it can be firmly established that the
% formulation of Principle C as a syntactic condition (as is standard in
% syntactic literature, and as implemented in non-derivational theories
% by \citet{ps2} and \citet{Steedman97a}) is correct, it would pose a serious
% challenge to the non-representational approach to natural language
% semantics that certain variants of CG adhere to (a position most
% vigorously argued for by Polly Jacobson in her work on direct
% compositionality).

Thus, Principle C\is{principle!Binding!Principle C} has a somewhat cumbersome place within lexicalist
theories in general. However, unlike Principles A and B, the status of
Principle C in the grammar is still considerably unclear and
controversial to begin with (see \citealt[122--124]{buringbinding} for
some discussion on this point). In particular, it has been noted in
the literature \citep{lasnik86} that there are languages such as \ili{Thai}
and Vietnamese that do not show Principle C effects. If, as suggested
by some authors (cf., e.g., \citealt{levinson87,levinson91}), the
effects of Principle C can be accounted for by pragmatic principles,
that would remove one major sticking point in both HPSG and CG
formulations of the Binding Theory.
\is{Binding Theory|)}

\section{A brief note on processing and implementation \label{implementation}}

The discussion above has mostly focused on linguistic analysis. In
this final section, I will briefly comment on implications for
psycholinguistics and computational linguistics research.

As should already be clear from the above discussion, different
variants of both HPSG and CG make different assumptions
about the relationship between the competence grammar and theories of
performance. To make things even more complicated, such assumptions
are often implicit. As a first approximation, it is probably fair to
say that HPSG (at least the ``bare-bones'' version of it) and CCG are
more similar to each other than they are to TLCG in being
surface-oriented. TLCG makes heavy use of hypothetical reasoning in
the analyses of certain linguistic phenomena, and, as should already
be clear at this point, the role it plays in the grammar is much like
the role of movement operations in Mainstream Generative Grammar.

As repeatedly emphasized by practitioners of HPSG and CCG (see, for
example, \citealt{SW2011a}, \citealt[Section~13.7]{steedman2012} and
\crossrefchapteralt{processing}), all other things being equal, it is more
preferable to make the relationship between the competence grammar and
the model of performance as transparent as possible. It is
unlikely that any reasonable
researcher would deny such a claim, but it begs one big question:
how exactly are we to understand the qualification ``all other things
being equal''? Practitioners of TLCG in general seem to have a somewhat more
detached take on the relationship between competence and performance, 
and I believe the consensus there is more in line with (what seems to be)
the spirit of Mainstream Generative grammar: the goal is to clarify
the most fundamental principles of grammar and state them in the
simplest form possible. TLCG subscribes to the thesis that (a certain
variety of) logic is indeed the underlying principle of grammar of
natural language. This is an attractive view, but at the same time
language exhibits phenomena that suggest that pushing this perspective
to the limit is unlikely to be the most fruitful research strategy. The
right approach is probably one that combines the insights of both 
surface-oriented approaches (such as HPSG and CCG) and more abstract
approaches (such as TLCG and Mainstream Generative Grammar).

At a more specific level, one attractive feature of CCG (but not CG in
general), when viewed as an integrated model of the competence grammar
and human sentence processing, is that it enables surface-oriented,
incremental\is{incremental processing} analyses of strings from left to right. This aspect was
emphasized in the early literature of CCG \citep{AS82a,CS85a}, but it
does not seem to have had much impact on psycholinguistic research in
general since then. A notable exception is the work by
\citet{pickering-barry91,PB93a} in the early 90s. There is also some work
on the relationship between processing and TLCG (see \citealt[Chapters 9 and 10]{morrill2011},
and references therein). In any event, a serious
investigation of the relationship between competence grammar and human
sentence processing from a CG perspective (either CCG or TLCG) is a
research topic that is waiting to be explored, much like the situation
with HPSG (see \crossrefchapteralt{processing}).

As for connections to computational linguistics (CL)/natural language
processing (NLP) research, like
HPSG (cf.\ \crossrefchapteralt{cl}), large"=scale computational
implementation has been an important 
research agenda for CCG (see, for example, \citealt{WhiteBaldridge2003,CC2007a-u}). I refer
the reader to \citew[Chapter~13]{steedman2012} for an excellent
summary on this subject (this chapter contains a discussion of human
sentence processing as well). Together with work on linguistically
informed parsing in HPSG, CCG parsers seem to be attracting
some renewed interest in CL/NLP research recently, due to the new trend of
combining the insights of statistical approaches and
linguistically-informed approaches. In particular, the straightforward
syntax-semantics interface of (C)CG is an attractive feature
in building CL/NLP systems that have an explicit logical
representation of meaning. See, for example, \citet{steedmanlewis13}
and \citet{mineshima-etal:2016:emnlp} 
for this type of work. TLCG research has traditionally been less
directly related to CL/NLP research. But there are recent attempts at
constructing large-scale treebanks \citep{moot2015} and combining TLCG
frameworks with more mainstream approaches in NLP research such as
distributional semantics \citep{moot2018}.


\section{Conclusion}

As should be clear from the above discussion, HPSG and CG share many
important similarities, mainly due to the fact that they are both
variants of lexicalist syntactic theories. This is particularly clear
in the analyses of local dependencies in terms of lexically encoded
argument structure information. Important differences emerge once one
turns one's attention to less canonical types of phenomena, such as
atypical types of coordination (nonconstituent coordination,
Gapping) and the treatment of ``constructional'' patterns that are not
easily lexicalizable. In general, HPSG has a richer and more
comprehensive treatment of various empirical phenomena, whereas CG has
a lot to offer to grammatical theory (perhaps somewhat paradoxically)
due to the very fact that the potentials of the logic-based
perspective it embodies has not yet been explored in full detail. It
is more likely than not that the two will continue to develop as
distinct theories of natural language syntax (and semantics). I hope
that the discussion in the present chapter has made it clear that
there are still many occasions for fruitful interactions between the
two approaches both at the level of analytic ideas for specific
empirical phenomena and at the more general, foundational level
pertaining to the overall architecture of grammatical theory.

\section*{\acknowledgmentsUS}

I'd like to thank Jean-Pierre Koenig, Bob Levine and Stefan MÃ¼ller
for comments. This work is supported by the NINJAL collaborative
research project ``Cross-linguistic Studies of Japanese Prosody and Grammar''.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}



%      <!-- Local IspellDict: en_US-w_accents -->
