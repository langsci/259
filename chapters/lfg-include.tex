%% Time-stamp: <2021-05-12 19:23:42 admin>
%% -*- coding:utf-8 -*-

% This is the body of the paper. It should compile with latex and xelatex

% xelatex lfg.tex; biber lfg
% xelatex   lfg-fast.tex; bibtex lfg-fast


%\mainmatter % Inserted by Ash 2020-05-10 to get arabic page numbers; delete/comment out before sending to Stefan
% \newcommand{\ednote}[1]{#1}
\newcommand{\ednote}[1]{}

\begin{document}
\maketitle
\label{chap-lfg}




%\if0
\section{Introduction} 
\label{intro-sec}
Head-Driven Phrase Structure Grammar is similar in many respects to its sister framework, Lexical
Functional Grammar or LFG \citep{BATW2016a,dalrymple;ea19}.  Both HPSG and LFG are lexicalist
frameworks in the sense that they distinguish between the morphological system that creates words
and the syntax proper that combines those fully inflected words into phrases and sentences.  Both
frameworks assume a lexical theory of argument structure (\citealp{MWArgSt}; compare also
\crossrefchapteralp{arg-st}) in which verbs and other predicators come equipped with valence
structures indicating the kinds of complements that the word is to be combined with.  Both theories
treat certain instances of control (equi) and raising as lexical properties of control or raising
predicates (on control and raising in HPSG see \crossrefchapteralt{control-raising}). Both theories
allow phonologically empty nodes in the constituent structure, although researchers in both theories
tend to avoid them unless they are well-motivated
\parencites{SF94a}{Berman97a}[734--742]{dalrymple;ea19}.
% Berman97a complete article
Both frameworks use recursively embedded attribute-value matrices (AVMs). These structures directly model linguistic expressions in LFG, but are understood in HPSG as grammatical descriptions satisfied by the directed acyclic graphs that model utterances.\footnote{Both frameworks are historically related to unification grammar \citep{Kay84a-u}.  Unification is defined as an operation on feature structures:  the result of unifying two mutually consistent feature structures is a feature structure containing all and only the information in the original two feature structures.  Neither framework actually employs a unification operation, but unification can produce structures resembling the ones in use in LFG and HPSG.} 
%which
%yields a feature structure that contains all and only the information in the unified feature structures. 
%Neither HPSG nor LFG employs unification in this strict sense. 
%HPSG captures mutual consistency of information through the conjunction of descriptions of the relevant structures, where the descriptions are typically captured in an AVM representation of a feature logic (see \crossrefchapterw{evolution} on the evolution of HPSG and \crossrefchapterw{formal-background} on underlying formal assumptions).  LFG captures the same intuition with equations that state that two feature values are equal. It is nevertheless not uncommon for practitioners of HPSG and LFG to informally use the term \qterm{unification} when talking about the resolution of two mutually consistent descriptions of a single structure.

%PREVIOUS VERSION:
%these are themselves the relevant structures in LFG, but are understood as representations of directed acyclic graphs in HPSG.
%
%\footnote{For the purpose of this paper, given its goal of exposition of LFG with reference to HPSG, this characterization will do, but it's not strictly true that LFG is a unification-based framework, because unification is really an operation on structures, as in HPSG, whereas the effects of unification in LFG are rather captured by mutually consistent \emph{descriptions} of structures.}   

There are also some differences in representational resources, especially in the original formulations of the two frameworks.
But each framework now exists in many variants, and features that were originally exclusive to one framework can often now be found in some variant of the other.   HPSG's valence lists are ordered, while those of LFG usually are not, but \citet{andrews1999complex} use an ordered list of terms (subject and objects) in LFG.  
LFG represents grammatical relations in a \textit{functional structure} or f-structure that is autonomous from the constituent structure, while HPSG usually lacks anything like a functional structure. 
%But \citet{Bender2008a} proposes changes to HPSG's \LFGfeat{arg-st} that make it nearly equivalent to f-structure (see Section~\ref{nonconfig-sec}).  
%But \citet{Bender2008a} proposes that \LFGfeat{arg-st} be projected from lexical daughters to mothers,
%which results in something similar to f-structure, since information about arguments is no longer
%strictly local to the predicate that selects for them. 
% St. MÃ¼. reformulation:
% But \citet{Bender2008a} proposes that elements of the \compsl are not cancelled-off and hence are projected from daughters to mothers, which results in something similar to f-structure, since information about arguments is no longer strictly local to the predicate that selects for them.
But in \citegen{Bender2008a} 
version of HPSG, the \compsl functions very much like LFG's f-structure (see Section~\ref{nonconfig-sec} below).  This chapter explores the utility of various formal devices for the description of natural language grammars, but since those devices are not exclusively intrinsic to one framework, this discussion does not bear on the comparative utility of the frameworks themselves. For a comparison of the representational architectures and formal assumptions of the two theories, see \citet{Przepiorkowski2021a}, which complements this chapter. 


\largerpage 
We start with a look at the design considerations guiding the development of the two theories,
followed by point by point comparisons of syntactic issues organized by grammatical topic.  Then we
turn to the semantic system of LFG, beginning with a brief history by way of explaining its
motivations, and continuing with a presentation of the semantic theory itself.  A comparison with
HPSG semantics is impractical and will not be attempted here, but see \crossrefchaptert{semantics}
for an overview of HPSG semantics.

\section{Design principles in the origins of HPSG and LFG} 
\label{design-sec}

The HPSG and LFG frameworks were originally motivated by rather different design considerations.  In
order to make a meaningful comparison between the frameworks, it is helpful to understand those
differences.

HPSG grew out of the tradition, established by \citet{Chomsky57a}, of studying the computational
properties of natural language syntax as a window onto the capabilities of human cognitive
processes.  The advent of Generalized Phrase Structure Grammar\indexgpsg \citep{GKPS85a} was an
important milestone in that tradition, bringing as it did the surprising prospect that the entire
range of syntactic phenomena known to exist at the time could be described with a context-free
grammar (CFG).  If this could be maintained, it would mean that natural language is context-free in
the sense of the Chomsky Hierarchy \citep{Chomsky57a}, which is an answer to the question raised by
Chomsky that was very different from, and more interesting than, Chomsky's own answer.  Then
\citet{Shieber85a} and \citet{Culy85a} showed that certain phenomena such as recursive cross-serial
dependencies in Swiss German\il{German!Swiss} and \ili{Bambara} exceeded the generative capacity of
a context-free grammar.

\largerpage\enlargethispage{3pt}
Despite that development, it nonetheless appeared that languages for the most part hewed rather
closely to the context-free design.  Thus began the search for more powerful grammatical formalisms
that would preserve the insight of a context-free grammar while allowing for certain phenomena that
exceed that generative capacity.\footnote{There were also some problems with the GPSG theory of the
  lexicon, in which complement selection was assimilated to the phrase structure grammar.  For
  discussion see \citet[Section~4.1]{MWArgSt}.}  HPSG grew out of this search. As
\citet[83]{SWB2003a} observe, HPSG ``is still closely related to standard CFG''.  In fact, an HPSG
grammar largely consists of constraints on local sub-trees (i.e., trees consisting of a node and its
immediate daughters), which would make it a context-free grammar were it not that the nodes
themselves are complex, recursively-defined feature structures.  This CFG-like character of HPSG
means that the framework itself has the potential to embody an interesting theory of locality.  At
the same time, the original theory also allowed for the description of non-local relations, and new
non-local devices were added in versions of the theory developed later (a proposal to add functional
structure was mentioned in Section~\ref{intro-sec}, and others will be described below).  The
flexibility of HPSG thus provides for the study of locality and non-locality, while also allowing
for grammatical description and theory construction by syntacticians with no interest in locality.

The architecture of LFG was originally motivated by rather different concerns:  an interest in typological variation from a broadly functionalist perspective, from which one studies cross-linguistic variation in the expression of functionally equivalent elements of grammar.
%, the search for linguistic universals and a desire to explain the prevailing grammatical patterns found across languages.  
For this reason two levels of representation are discerned: a functional structure or \textit{f-structure} representing the internal grammatical relations in a sentence that are largely invariant across languages, and a categorial constituent structure or \textit{c-structure} representing the external morphological and syntactic expression of those relations, which vary, often rather dramatically, across different typological varieties of language.  For example, probably all (or nearly all) languages have subjects and objects, hence those relations are represented in f-structure.  But languages vary as to the mechanisms for signaling subjecthood and objecthood,  the three main mechanisms being word order, head-marking, and dependent-marking \citep{Nichols86a-u}, hence those mechanisms are distinguished at c-structure.    The word \textit{functional} in the name of the LFG framework  is a three-way pun, referring to the grammatical \textit{functions} that play such an important role in the framework, the mathematical \textit{functions} that are the basis for the representational formalism, and the generally \textit{functionalist}-friendly nature of the LFG approach.  

%Evidence for these different underlying motivations can be found in the later proposals for further
%theory development by two of the principal architects of the respective theories.  In some of his
%last work before his death, Ivan Sag worked on the development of \isi{Sign-Based Construction
%  Grammar}, a variant of HPSG in which those \isi{locality} conditions are even more deeply built
%into the representational system (\citealp{Sag2012a}, see also
%\crossrefchapteralp[Section~\ref{sec-sbcg}]{cxg} for discussion).  Meanwhile, Joan Bresnan had
%developed Optimality-Theoretic LFG\is{Optimality Theory!Optimality-Theoretic LFG} \citep{Bresnan00-OT}, a framework in which the OT \textit{input} is defined as an underspecified f-structure representing the universal aspects of a syntactic structure, the \textit{candidate set} is the set of external structures mapping to that input, and cross-linguistic variation is modeled with constraint re-rankings.  So while Sag sought to strengthen the locality conditions of the framework, Bresnan sought a more explicit account of universality and variation.  

Despite these differing design motivations, there is no dichotomy between the frameworks with respect to the actual research undertaken within the two research communities.  Typological variation within almost every area of grammar has been studied in HPSG, and locality is studied within LFG by developing theories of the mapping between c-structure and f-structure  \citep[see][88--128]{BATW2016a}.   In the remainder of this chapter we will survey various phenomena and compare HPSG and LFG approaches.


\section{Phrases and endocentricity} 

\largerpage[2]
A phrasal node shares certain grammatical features with specific daughters. %such as the \textsc{head} features that it shares with the head daughter.  
In HPSG, this is accomplished
by means of structure-sharing (reentrancies) in the immediate dominance schemata and other 
constraints on local sub-trees such as the Head Feature Principle\is{principle!Head Feature}.  LFG employs essentially the same mechanism for feature sharing in a local sub-tree but implements it slightly differently, so as to better address the design motivations of the theory.  Each node in a phrase structure is paired with an f-structure, which is formally a set of attribute-value pairs.  It is through the f-structure that the nodes of the phrase structure share features.   The phrase structure is referred to as \textit{c-structure}, for categorial or constituent structure, in order to distinguish it from f-structure. 
%Context-free phrase structure rules  license c-structures, but the c-structure elements are annotated with \term{functional descriptions} (\term{f-descriptions}), which describe the corresponding f-structure. 
Context-free phrase structure rules  license c-structures, and the c-structure elements are annotated with \term{functional equations} which describe the corresponding f-structure.  
The correspondence function from c-structures to f-structures, $\phi$, defines and constrains the f-structure on the basis of the  equations collected from the c-structure annotations and from lexical entries of the terminal nodes.\footnote{Taken together, the set of equations in a c-structure is called the functional description or \term{f-description}.}  For example, the phrase structure grammar in (\ref{psg}) and lexicon in (\ref{lex}) license the tree in Figure~\ref{fig-tree1}.\footnote{In (\ref{lex}c) the verb is broken down into the root \textit{roar} and third person singular suffix \textit{-s} to show the functional equations contributed by each morpheme.  However, LFG does not require that words be analyzed into morphemes.  It is compatible with morpheme-based \citep[e.g.,][384--385, 395--396]{ishikawa85,BATW2016a} or realizational morphology   \citep[Chapter 12]{dalrymple;ea19} or any other theory of morphology that associates word forms with grammatical features.}$^,$\footnote{For simplicity's sake, in this basic example we assume an NP analysis rather than a DP one \citep{Brame82a}. However, much recent LFG work assumes DP; see \citet{BATW2016a} and \citet{dalrymple;ea19} for further discussion.}

% This works now, with tikzexternalize commented out in lfg.tex; Ash 20200511
% \begin{forest}
% sm edges 
% [S 
%  [$f_1$
%    [Det [those]]
%    [N   [lions]]]
%  [VP
%    [V   [roar]]]]
% \end{forest}

\eal  \label{psg}
\ex
{
\phraserule{S}{\rulenode{NP\\(\up \LFGfeat{subj}) $=$ \down}
               \rulenode{VP\\\Updown}}}

\ex 

{
\phraserule{NP}{\optrulenode{Det\\ \Updown}
                \rulenode{N\\ \Updown}}}

\ex 

{
\phraserule{VP}{\rulenode{V\\ \Updown}
                \optrulenode{NP\\(\up \textsc{obj}) $=$ \Down}}}

\zl

\eal \label{lex} 
\ex 

{
\makebox[4em][l]{{\it this}\/: Det}\qquad\feqs{(\up \LFGfeat{prox}) $=$ $+$\\
(\up \LFGfeat{num}) $=$ \textsc{sg}}}


\ex 

{
\makebox[4em][l]{{\it lion}\/: N}\qquad\feqs{(\up \LFGfeat{pred}) $=$ `lion'\\
(\up \LFGfeat{num}) $=$ \textsc{sg}}}

\ex 

{\label{indent}
  \begin{tabular}[t]{@{}l@{}}
\makebox[4em][l]{{\it roar}\/: V}\qquad\feqs{(\up \LFGfeat{pred}) $=$ \predvall{roar$\leftangle
(\up \LFGfeat{subj})\rightangle$}}
\\
\makebox[4em][l]{-{\it s}\/: {\it infl}\/\pslabel{v}}\qquad\feqs{(\up \LFGfeat{tense}) = \LFGfeat{pres}\\
                  (\up \LFGfeat{subj}) $=$ \down\\
                  \qquad(\down \LFGfeat{pers}) $=$ 3\\
                  \qquad(\down \LFGfeat{num}) $=$ \LFGfeat{sg}}
  \end{tabular}
  }
\zl



\begin{figure}
  % \ednote{WE NEED THE TREE BELOW THIS POINT IN THE .TEX FILE, BUT I HAD TO COMMENT IT OUT TO GET THE THING TO COMPILE!}
  \begin{forest}
sm edges without translation
[S 
 [\csn{(\up\LFGfeat{subj}) $=$ \down}{NP}
  [\csn{\Updown}{Det} [this\\
                         {(\up \LFGfeat{num}) $=$ \LFGfeat{sg}}\\
                         {(\up \LFGfeat{prox}) $=$ \LFGfeat{+}}]]
  [\csn{\Updown}{N}   [lion\\
                         {(\up \LFGfeat{pred}) $=$ `lion'}\\
                         {(\up \LFGfeat{num}) $=$ \LFGfeat{sg}}]]]
  [\csn{\Updown}{VP}
            [\csn{\Updown}{V} [roars\\ 
                              {(\up \LFGfeat{pred}) $=$ `roar$\langle(\up \LFGfeat{subj})\rangle$'}\\
                              {(\up \LFGfeat{tense}) $=$ \LFGfeat{pres}}\\
                              {(\up \LFGfeat{subj}) $=$ \down}\\
                              {(\down \LFGfeat{pers}) $=$ 3}\\
                              {(\down \LFGfeat{num}) $=$ \LFGfeat{sg}} ]]]]
\end{forest}
\caption{C-structure in LFG}\label{fig-tree1}
\end{figure}

\largerpage[2]
\noindent
Each node in the c-structure maps to an f-structure, that is, to a set of attribute-value pairs.  Within the equations, the up and down arrows are metavariables over f-structure labels, interpreted as follows:  the up arrow refers to the f"=structure to which the c-structure mother node maps, and the down arrow refers to the f-structure that its own c-structure node maps to.  To derive the f-structure from Figure~\ref{fig-tree1}, we instantiate the metavariables to specific function names and solve for the f-structure associated with the root node (here, S).  In Figure~\ref{fig-tree2}, the f-structure labels \ensuremath{f_1},  \ensuremath{f_2}, etc. are subscripted to the node labels.  The arrows have been replaced with those labels.  

\begin{figure}
  % \ednote{WE NEED THE TREE BELOW THIS POINT IN THE .TEX FILE, BUT I HAD TO COMMENT IT OUT TO GET THE THING TO COMPILE!}
\begin{forest}
sm edges without translation
[S$_{f_1}$ 
  [\csn{(\ensuremath{f_1} \LFGfeat{subj}) $=$ \ensuremath{f_2}}{NP\rlap{$_{f_2}$}}
    [\csn{\ensuremath{f_2\ =\ f_4}}{Det\rlap{$_{f_4}$}} [this$_{f_7}$\\
                         {(\ensuremath{f_4} \LFGfeat{num}) $=$ \LFGfeat{sg}}\\
                         {(\ensuremath{f_4} \LFGfeat{prox}) $=$ \LFGfeat{+}}]]
    [\csn{\ensuremath{f_2\ =\ f_5}}{N\rlap{$_{f_5}$}}   [lion$_{f_8}$\\
                         {(\ensuremath{f_5} \LFGfeat{pred}) $=$ \predvall{lion}}\\
                         {(\ensuremath{f_5} \LFGfeat{num}) $=$ \LFGfeat{sg}}]]]
  [\csn{\ensuremath{f_1 =\ f_3}}{VP\rlap{$_{f_3}$}}
    [\csn{\ensuremath{f_3 =\ f_6}}{V\!\rlap{$_{f_6}$}}   [roars$_{f_9}$\\
                         {(\ensuremath{f_6} \LFGfeat{pred}) $=$ `roar$\langle (\up \LFGfeat{subj})  \rangle $'}
                         \\ {(\ensuremath{f_6}\ \LFGfeat{tense}) $=$ \LFGfeat{pres}}\\
                  {(\ensuremath{f_6}\ \LFGfeat{subj}) $=$ \ensuremath{f_9}}\\
                 {(\ensuremath{f_9}\ \LFGfeat{pers}) $=$ 3}\\
                  {(\ensuremath{f_9}\ \LFGfeat{num}) $=$ \LFGfeat{sg}} ]]]]
\end{forest}
\caption{Deriving the f-description}\label{fig-tree2}
\end{figure}

Collecting all the equations from this tree and solving for \ensuremath{f_1}, we arrive at the f-structure in (\ref{fs1}):
\ea		
\label{fs1} 
% \avm{
% \ensuremath{f_1} [ subj & [ pred & \predvall{lion} \\ 
%                  num  & \vall{sg} \\ 
%                  pers & \vall{3} \\ 
%                  prox & $+$ ] \\
%         pred  & \predvall{roar\ensuremath{\langle (f_1\, \LFGfeat{subj}) \rangle}} \\
%         tense & \vall{pres} ]}
\avm{
\lfgfst{f_1}[ subj & [ pred & \predvall{lion} \\ 
                 num  & \vall{sg} \\ 
                 pers & \vall{3} \\ 
                 prox & $+$ ] \\
        pred  & \predvall{roar $\langle$(\lfgfst{f_1} \LFGfeat{subj})$\rangle$} \\
        tense & \vall{pres} ]}
\z

\largerpage
\noindent
F-structures are subject to three general well-formedness conditions/principles:\footnote{Here we state them informally, just to capture the key intuitions, but see \citet[37 (reprint pagination)]{KB82a-u} or \citet[52--53]{dalrymple;ea19} for precise definitions.}%\todostefan{add page numbers for KB82a}
%
\begin{exe}
  \ex \high{Uniqueness}: 
%  No attribute may have more than one value.
Every attribute has a unique value.
\ex \high{Completeness}: All grammatical functions governed by an f-structure's \LFGfeat{pred} feature must occur in the f-structure. 
  \ex \high{Coherence}: Only grammatical functions governed by an f-structure's \LFGfeat{pred} feature may occur in the f-structure.
\end{exe}
%
Completeness and Coherence together play a similar role to the Valence Principle\is{principle!Valence} in HPSG \citep[348]{ps2} in that they guarantee that only exactly the right number of syntactic dependents are in the structure.  Uniqueness means that an f-structure has to be consistent in its feature values\footnote{In fact, another name for the uniqueness principle is \term{Consistency} \citep[53]{dalrymple;ea19}.} and also means that the f-structure is a function in the mathematical sense, a set of ordered pairs such that no two pairs have the same first member. Moreover, each \LFGfeat{pred} value is assumed to bear a unique index (normally suppressed, as we do here), so that even two instances of apparently the \scare{same} \LFGfeat{pred} value cannot be identical; this plays an important role in LFG's analysis of pronominal affixes and agreement (see Section~\ref{sec:agre-affix-pron}).

\largerpage
Since the up and down arrows refer to nodes of the local sub-tree, LFG annotated phrase structure rules like those in (\ref{psg}) can often be directly translated into HPSG immediate dominance schemata and principles constraining local sub-trees.  
By way of illustration, let \textsc{fs} (for \textit{f-structure}) be an HPSG attribute corresponding to the f-structure projection function.  Then the LFG rule in (\ref{psg2}a) (repeated from (\ref{psg}a) above) is equivalent to the  HPSG rule\is{schema} in (\ref{psg2}b):

\eal 
 \label{psg2}
\ex
\label{psg2a}
{
\phraserule{S}{\rulenode{NP\\(\up \LFGfeat{subj}) $=$ \down}
               \rulenode{VP\\ \Updown}}}
               
\ex
\label{psg2b} 
{
\phraserule{S[\textsc{fs} \ibox{1}]\hspace*{1em}}{\rulenode{NP[\textsc{fs} \ibox{2}]}  \rulenode{VP[\textsc{fs}  \ibox{1}[\textsc{subj} \ibox{2}]]}}}
\zl

\noindent
Let us compare the two representations with respect to heads and dependents.

Taking heads first, the VP node annotated with \Updown\ is an \textit{f-structure head}, meaning
that the features of the VP are identified with those of the mother S.  This effect is equivalent to
the tag\ \ibox{1}\ in (\ref{psg2b}).    Hence  \Updown\ has an effect similar to HPSG's Head Feature
Principle\is{principle!Head Feature}.  However, in LFG the part of speech categories and their projections such as N, V, Det, NP, VP, DP, etc. belong to the c-structure and not the f-structure.  As a consequence, those features are not subject to sharing, and any principled correlations between such categories, such as the fact that N is the head of NP, V the head of VP, C the head of CP, and so on, are instead captured in an explicit version of (extended) X-bar theory applying exclusively to the c-structure \citep{grimshaw98}.  The version of extended X-bar theory in \citet[Chapter 6]{BATW2016a} assumes that all nodes on the right side of the arrow of the phrase structure rule are optional, with many unacceptable partial structures ruled out in the f-structure instead.  Also, not all structures need to be endocentric (i.e., not all structures have a head daughter in c-structure).  The LFG category S shown in (\ref{psg2a}) is inherently exocentric, lacking a  c-structure head whose c-structure category could influence its external syntax (the f-structure head of S is the daughter with the \Updown\ annotation, here the VP).   (\ili{English} is also assumed to have endocentric clauses of category IP, where an auxiliary verb of category I (for Inflection) serves as the c-structure head.)  S is used for copulaless clauses and also for the flat structures of nonconfigurational clauses in languages such as \ili{Warlpiri} (see Section~\ref{nonconfig-sec} below).   

Functional projections like DP, IP, and CP are typically assumed to form a ``shell'' over the lexical projections NP, VP, AP, and PP (plus CP can appear over S).  While this assumption is widespread in transformational approaches, its origins can be found in non-transformational  research, including early LFG:  CP was proposed in \citegen[141]{fassifehri81} LFG treatment of \ili{Arabic}, and IP (the idea of the sentence as functional projection) is found in \citegen{Falk83a-u} LFG analysis of the \ili{English} auxiliary system (Falk called it ``MP'' instead of ``IP'').\footnote{For more on the origins of extended projections see \citet[124--125]{BATW2016a}.}  

\enlargethispage{4pt}
%\largerpage[2]
Extended projections are formally implemented in LFG by having the functional head (such as D) and its lexical complement (such as NP) be f-structure co-heads.  See for example the DP \textit{this lion} in Figure~\ref{dp-tree}, where D, NP, and N are all annotated with \Updown, hence the DP, D, NP, and N nodes all map to the same f-structure.  What makes this identity possible is that function words lack a \textsc{pred} feature that would otherwise indicate a semantic form.\footnote{The attribute \LFGfeat{pred} ostensibly stands for \qterm{predicate}, but really it means something more like \scare{has semantic content}, as there are lexical items, such as proper names, which have \LFGfeat{pred} features but are not predicates under standard assumptions.}  Content words such as \textit{lion} have such a feature ([\LFGfeat{pred} \predvall{lion}]), and so if the D had one as well, then they would clash in the f-structure.  Note more generally that the f-structure flattens out much of the hierarchical structure of the corresponding c-structure.\pagebreak

\begin{figure}
\centering
  \begin{forest}
sm edges without translation
[DP 
 [\csn{\Updown}{D}
   [this\\
                         {(\up \LFGfeat{num}) $=$ \LFGfeat{sg}}\\
                         {(\up \LFGfeat{prox}) $=$ \LFGfeat{+}} ] ]
  [\csn{\Updown}{NP}   
    [\csn{\Updown}{N}  
       [lion\\
                         {(\up \LFGfeat{pred}) $=$ `lion'}\\
                         {(\up \LFGfeat{num}) $=$ \LFGfeat{sg}} ] ] ]  ]
\end{forest}
\caption{Functional heads in LFG}\label{dp-tree}
\end{figure}

Complementation works a little differently in LFG from HPSG.  Note that the LFG  rule (\ref{psg2a}) indicates the \LFGfeat{subj} grammatical function on the subject NP node, while the pseudo-HPSG rule (\ref{psg2b}) indicates the \LFGfeat{subj} function on the VP functor selecting the subject.   A consequence of the use of functional equations in LFG is that a grammatical relation such as \LFGfeat{subj} can be locally associated with its formal exponents, whether a configurational position in phrase structure (as in Figure~\ref{fig-tree1}), head-marking (agreement, see (\ref{indent})), or dependent marking (case).  A nominative case affix specialized for exclusively marking subjects can introduce a so-called \qterm{inside-out} functional designator, (\LFGfeat{subj} \Up),
 which requires that the f-structure of the NP or DP bearing that case ending be the value of a \LFGfeat{subj} attribute \citep{Nordlinger98a-u}.\footnote{Inside-out designators can be identified by their special syntax: the attribute symbol (here \LFGfeat{subj}) precedes instead of following the function symbol (here the metavariable $\uparrow$).  
They are defined as follows:  for function \textit{f}, attribute \textit{a}, and value \textit{v}, $(f \, a) = v$ iff $(a \, v) = f$. If \textit{f} is the f-structure representing a nominal in nominative case, then (\LFGfeat{subj} \textit{f}) refers to the f-structure of the clause whose subject is that nominal.}  Other argument cases effectively resolve an annotation on the NP or DP node to the appropriate grammatical function.  In all of these situations, the attribute encoding a grammatical function, such as \LFGfeat{subj} or \LFGfeat{obj}, is directly associated with an element filling that function.  This aspect of LFG representations makes it convenient for functionalist and typological work on grammatical relations.  
%\largerpage[2]

 
\section{Grammatical functions and valence} 
\label{valence-sec}
\emph{Grammatical functions}  (or \emph{grammatical relations})  like subject and object play an important role in LFG theory,  and it is worthwhile to compare the status of grammatical functions in LFG and HPSG.  Grammatical functions in LFG are best understood in the context of the break with Transformational Grammar that led to LFG and other alternative frameworks.  \citet[68--74]{Chomsky65a} argued that while grammatical functions clearly play a role in  grammar, they need not be explicitly incorporated into the grammar as such.  Instead he proposed to define them in phrase structural terms: the \qterm{subject} is the NP immediately dominated by S, the \qterm{object} is the NP immediately dominated by VP, and so on.   This theoretical
assumption necessitated the use of transformations and a
profusion of certain null elements: an affixal subject, for example,
would have to be inserted under an NP node that is the daughter of S, and then moved to its surface position as affix; a subject that is phonologically null but anaphorically active would have to be generated in that position as well, hence the need for \qterm{null \textit{pro}}.  Early alternatives to Transformational Grammar such as Relational Grammar \citep{Perlmutter83a-ed} and LFG also sought to  capture the equivalence
across different alternative expressions of a single argument, but
rejected the transformational model.   Instead the grammar licenses an abstract representation of the \qterm{subject}, for example, essentially as a set of features.  The language-specific grammar maps a predicator's semantic roles to these abstract representations (named \qterm{subject}, \qterm{object}, and so on), and also maps those representations onto the expressions in the language.  
%\citet[95]{BATW2016a} explain it this way: 

As a non-transformational theory, HPSG breaks down the relation between semantic roles and their grammatical expressions into two distinct mappings, in roughly the same way that LFG does.  The intermediate representations of arguments consists of sets of features, just like in LFG.  However, while LFG provides a consistent cross-linguistic representation of each grammatical function type, many variants of the HPSG framework allow the representation to vary from language to language.  For example, LFG identifies subjects in every language with the attribute \LFGfeat{subj}, while HPSG identifies the subject, if at all, as a specific element of the \LFGfeat{arg-st} list, but which element it is can vary with the language.  An \ili{English} subject is usually assumed to be the first element in the list, or \textsc{arg-st|first}, while the definition of a \ili{German} subject necessarily involves the \textsc{case} feature \citep{Reis82}.  Note that it would not work to represent \ili{German} subjects as \textsc{arg-st|first}, because in \ili{German}  subjectless sentences with arguments (such as datives),  the first item in the \textsc{arg-st} list is a non-subject.  Also, the HPSG valence feature \textsc{subj} (or \textsc{spr} \qterm{specifier}) 
does not represent the subject grammatical function in the LFG sense, but is instead closer to \citegen[68--74]{Chomsky65a} 
 definition of subject as an NP in a particular phrase structural position.  However, nothing precludes an HPSG practitioner from adopting LFG-style grammatical function features within HPSG, and many have done so, such as the \LFGfeat{qval} feature of \citet{KDHB2007a} or the \LFGfeat{erg} feature proposed by \citet{Pollard94a}.  Similarly, \citet{andrews1999complex} present a version of LFG that incorporates an HPSG-style valence list.  

This illustrates once again LFG's orientation towards cross-linguistic grammar comparison.  HPSG practitioners who do not adopt an LFG-style \LFGfeat{subj} feature can still formulate theories of \qterm{subjects}, comparing subjects in \ili{English}, \ili{German}, \ili{Icelandic}, and so on, but they lack a formal correlate of the notion \qterm{subject} comparable to LFG's \textsc{subj} feature.  Meanwhile from the perspective of a single grammar, the original motivation for adopting grammatical functions, in reaction to the pan-phrase-structural view of the transformational approach, informed both LFG and HPSG in the same way.  

In LFG, a lexical predicator such as a verb selects its complements via grammatical functions, which are native to f-structure, rather than using c-structure categories.  A transitive verb selects a \textsc{subj} and \textsc{obj}, which are features of f-structure, but it cannot select for the category DP because such part of speech categories belong only to c-structure.  For example, the verb stem \textit{roar} in (\ref{lex}c) has a \textsc{pred} feature whose value contains (\up \LFGfeat{subj}), which has the effect of requiring a \textsc{subj} function in the f-structure.    The f-structure (shown in (\ref{fs1})) is built using the defining equations, as described above.  Then that f-structure is checked against any \textit{existential constraints} such as  the expression (\up \LFGfeat{subj}), which requires that the f-structure contain a \LFGfeat{SUBJ} feature.  That constraint is satisfied, as shown in (\ref{fs1}).  Moreover, the fact that (\up \LFGfeat{subj}) appears in the angled brackets means that it expresses a semantic role of the \predvall{roar} relation, hence the \textsc{subj} value is required to contain a \textsc{pred} feature, which is satisfied by the feature [\textsc{pred} \predvall{lion}] in  (\ref{fs1}).  



Selection for grammatical relations instead of formal categories enables LFG to capture the  flexibility in the expression of a given grammatical relation described at the end of the previous section.  As noted there, in many languages the subject can be expressed either as an independent NP/DP phrase as in \ili{English}, or as a pronominal affix on the verb.  As long as the affix introduces a \LFGfeat{PRED} feature and is designated by the grammar as filling the \featt{SUBJ} relation, then it satisfies the subcategorization requirements imposed by a verb.  A more subtle example of flexible expression of grammatical functions  can be seen in \ili{English} constructions where an argument can in principle take the form of either a DP (as in (\ref{sick}a)) or a clause (as in (\ref{sick}b)) (the examples in (\ref{sick}) are from \citealt[11--12]{BATW2016a}).  

\eal 
 \label{sick}
\ex[]{That problem, we talked about for days.}

\ex[]{That he was sick, we talked about for days.}

\ex[]{We talked about that problem for days.}

\ex[*]{We talked about that he was sick for days.}

\zl
The preposition \textit{about} selects neither a DP nor a CP \textit{per se}, but rather selects the grammatical function  \featt{OBJ}.
%\footnote{Following \citet[300]{BATW2016a},  this analysis assumes that the preposition \textit{about} does not introduce a  \LFGfeat{pred} feature, so it lacks its own argument structure and instead serves as a marker of an argument of the verb, functioning much like an oblique case marker.}

\eal \label{talk-about}
{{\it about}\/: P}\qquad\feqs{(\up \LFGfeat{pred}) = `about$\leftangle
(\up \LFGfeat{obj})\rangle$'}    
\zl

%\eal \label{talk-about}
%{{\it talk}\/: V}\qquad\feqs{(\up \LFGfeat{pred}) = `talk-about$\leftangle
%(\up \LFGfeat{subj})(\up \LFGfeat{obl}_{about} \LFGfeat{obj})\rangle$'}    
%\zl

\noindent
It is not the preposition but the local c-structure environment that conditions the category of that argument: the canonical prepositional object position right-adjacent to \textit{about} can only house a DP (compare (\ref{sick}c) and (\ref{sick}d)), while the topic position allows either DP or CP (compare (\ref{sick}a) and (\ref{sick}b)).  In LFG, the grammatical functions such as \textsc{subj} and \textsc{obj} represent equivalence classes across various modes of c-structure expression.  

Some HPSG approaches to filler-gap mismatches are presented in \crossrefchaptert[Section~\ref{sec:UDC:FillerGapMismatches}]{udc}.  They are essentially similar to the LFG account just presented, in that they work by allowing the preposed clause and the gap to differ in certain features.  The main difference between research on this problem in the two frameworks is that in LFG the bifurcation between matching and non-matching features of filler/""gap  is built into the framework in the separation between f-structure and c-structure.\footnote{Note that c-structure and f-structure are autonomous but not independent.  To constrain the relation between the c-structure category and f-structure, one can use either metacategories \citep[691--698]{dalrymple;ea19} or the CAT function \citep[265]{dalrymple;ea19}.}  

%HPSG can capture this variability in the expression of arguments in roughly the same way as LFG, despite some terminological differences.  The preposition about 
%HPSG correspondent of the LFG \textsc{subj} specification is the \textsc{arg-st|first} value.  The same applies to complements.  The disjunction between DP and clausal expression of an argument is encoded in the \textsc{arg-st;} the restriction to DP (observed in examples (\ref{sick}c,d)) is encoded in the relevant \textsc{comps} list item.  


\section{Head mobility} 
\label{mobile-sec}
The lexical head of a phrase can sometimes appear in an alternative position apparently outside of what would normally be its phrasal projection.  Assuming that an \ili{English} finite auxiliary verb is the (category I) head of its (IP) clause, then that auxiliary appears outside its clause in a yes/no question:

\eal 
\label{mad}
\ex {} [\sub{IP} she is mad]
\ex Is [\sub{IP} she \trace{} mad]?
\zl
Transformational grammars capture the systematic relation between these two structures with a head-movement transformation that leaves the source IP structure intact, with a trace replacing the moved lexical head of the clause.  The landing site of the moved clausal head is often assumed to be C, the complementizer position, as motivated by complementarity between the fronted verb and a lexical complementizer.  This complementarity is observed most strikingly in \ili{German} verb-second versus verb-final alternations, but is also found in other languages, including some \ili{English} constructions such as the following:  

\eal 
\label{mad2}
\ex[]{I wonder whether [\sub{IP} she is mad].}  
\ex[]{I wonder, is [\sub{IP} she \trace{} mad]?}  
\ex[*]{I wonder whether is she mad.}  
\zl 
Non-transformational frameworks like HPSG and LFG offer two alternative approaches to head mobility, described in the HPSG context in \crossrefchaptert[Section~\ref{sec-head-movement-vs-flat}]{order}.  Let us consider these in turn.

In the constructional approach,  
 the sentences in (\ref{mad2}) have been treated as displaying two distinct structures licensed by the grammar (\citealt{Sag2020a} and other references in \crossrefchapteralt[Section~\ref{sec-head-movement-vs-flat}]{order}).  For example, assuming ternary branching for the sentence in (\ref{mad}b), then the subject DP \textit{she} and predicate AP \textit{mad} would normally be assumed to be sisters of the fronted auxiliary \textit{is}.  On that analysis, the phrase structure is flattened out so that \textit{she mad} is not a constituent.  In fact, for \ili{English} the fronting of \textit{is} can even be seen as a consequence of that flattening:  \ili{English} is a head-initial language, so the two dependents \textit{she} and \textit{mad} are expected to follow their selecting head \textit{is}.  This analysis is common in HPSG, and it could be cast within the LFG framework as well.  

The second approach is closer in spirit to the head movement posited in Transformational Grammar.
It is found in both HPSG and LFG, but the formal implementation is rather different in the two
frameworks.  The HPSG \scare{head movement}\is{head movement} account due to \citet{Borsley89} posits a phonologically empty element that can function as the verb in its canonical position (such as (\ref{mad}a)), and that empty element is structure-shared with the verb that is fronted.  This treats the variation in head position similarly to non-local dependencies involving phrases.  See \crossrefchaptert[Section~\ref{sec-head-movement}]{order}.

The LFG version of \scare{head movement}\is{head movement} takes advantage of  the separation of autonomous c- and f-structures.  Recall from the above discussion of the DP in Figure~\ref{dp-tree} that functional heads such as determiners, auxiliaries, and complementizers do not introduce new f-structures, but rather map to the same f-structure as their complement phrases.   The finite auxiliary can therefore appear in either the I or C position without this difference in position affecting the f-structure, as we will see presently.  Recall also that c-structure nodes are optional and can be omitted as long as a well-formed f-structure is generated.  Comparing the \pagebreak{}non-terminal structures of Figure~\ref{fig-tree3} and Figure~\ref{fig-tree4}, the I preterminal node is omitted from the latter structure, but otherwise they are identical.  (The lexical equations in Figure~\ref{fig-tree4} are the same as the ones in Figure~\ref{fig-tree3} but are omitted for clarity.)  Given the many \updown{} annotations, the C, I, and AP nodes (as well as IP and CP) all map to the same f-structure, namely the one shown in (\ref{fs2}).  

\begin{figure}
  \ednote{THE SAME UP ARROW SPACING ISSUES ARE PRESENT HERE, BUT AT LEAST IT COMPILES!} \begin{forest}
sm edges without translation
[CP
 [\csn{\Updown}{C} [whether\\
 {(\up \LFGfeat{fin}) $=$ +}]]
[\csn{\Updown}{IP} 
  [\csn{(\up\LFGfeat{subj}) $=$ \down}{DP} [she\\
      {(\up \LFGfeat{pred}) $=$ `{pro}'}\\ 
      {(\up \LFGfeat{pers}) $=$ 3}\\
      {(\up \LFGfeat{num}) $=$ \LFGfeat{sg}}\\
      {(\up \LFGfeat{gend}) $=$ \LFGfeat{fem}} ]]
[\csn{\Updown}{I\rlap{$'$}}
    [\csn{\Updown}{I} [is\\ 
    {(\up \LFGfeat{fin}) $=$ +}\\
    {(\up \LFGfeat{subj}) = \down}\\
                 {(\down \LFGfeat{pers}) $=$ 3}\\
                  {(\down \LFGfeat{num}) $=$ \LFGfeat{sg}}
                 ]]
    [\csn{\Updown}{AP} [mad\\
     {(\up \LFGfeat{pred}) $=$ `mad$\langle (\up\LFGfeat{subj}) \rangle $'}]]
     ]]]
   \end{forest}
\caption{Functional projections in LFG}\label{fig-tree3}
\end{figure}

\begin{figure}
  \ednote{THE SAME UP ARROW SPACING ISSUES ARE PRESENT HERE, BUT AT LEAST IT COMPILES!}
\begin{forest}
sm edges without translation
[CP
 [\csn{\Updown}{C} [is]]
[\csn{\Updown}{IP} 
  [\csn{(\up\LFGfeat{subj}) $=$ \down}{DP} [she]]
  [\csn{\Updown}{I\rlap{$'$}}
 %   [\csn{\updown}{I} [is]]
    [\csn{\Updown}{AP} [mad\\
    %{(\up \LFGfeat{pred}) $=$ `mad$\langle (\up\LFGfeat{subj}) \rangle $'}
    ]] ]]]
   \end{forest}
\caption{Head mobility in LFG}\label{fig-tree4}
\end{figure}



\ea		
\label{fs2} 
\avm{
\lfgfst{f}[ subj & [ pred & \predvall{{pro}} \\ 
                     num & \vall{sg} \\ 
                     pers & \vall{3} \\ 
                     gend & \vall{fem} ] \\
            pred & \predvall{mad$\langle$(\lfgfst{f} \LFGfeat{subj})$\rangle$} \\
            fin  & $+$ ]
}
\z
The C and I positions are appropriate for markers of clausal grammatical features such as finiteness ([\textsc{fin} \textpm]), encoded either by auxiliary verbs like finite \textit{is} or complementizers like finite \textit{that} and infinitival \textit{for}: \textit{I said that/*for she is present} vs. \textit{I asked for/*that her to be present}.  \ili{English} has a specialized class of  auxiliary verbs for marking finiteness from the C position, while in languages like \ili{German} all finite verbs, including main verbs, can appear in a C position that is unoccupied by a lexical complementizer.  
Summarizing, the LFG framework enables a theory of head mobility based on the intuition that a clause has multiple head positions where inflectional features of the clause are encoded.  

\section{Agreement, case, and constraining equations} 
The basic theory of agreement is the same in LFG and HPSG (see \crossrefchapteralt{agreement}):  agreement occurs when 
multiple feature sets
 arising from distinct elements of a sentence specify information about a single abstract object, so that the information must be mutually consistent \citep{Kay84a-u}.  
The two forms are said to agree when the values imposed by the two constraints are compatible, while ungrammaticality results when they are incompatible.  An LFG example is seen in Figure~\ref{fig-tree1} above, where the noun, determiner, and verbal suffix each specify person and/or number features of the same \subj{} value.   

The basic mechanism for case marking works in essentially the same way as agreement in both frameworks: in case marking, distinct elements of a sentence specify case information about a single abstract object, hence that information must be compatible.   To account for the contrast in (\ref{case}a), nominative \LFGfeat{case} equations are associated with the pronoun \textit{she} and added to the entry for the verbal agreement suffix \textit{-s}:

\eal 
 \label{case}
\ex She/*Her/*You rules.
\ex {\makebox[4em][l]{{\it she}\/: {D}}\qquad\feqs{
    (\up \LFGfeat{pred}) $=$ \predvall{pro}\\     
    (\up \LFGfeat{case}) $=$ \LFGfeat{nom}\\
    (\up \LFGfeat{pers}) $=$ 3\\
    (\up \LFGfeat{num}) $=$ \LFGfeat{sg}\\ 
    (\up \LFGfeat{gend}) $=$ \LFGfeat{fem}} }
\ex {\makebox[4em][l]{{\it her}\/: {D}}\qquad\feqs{
        (\up \LFGfeat{pred}) $=$ \predvall{pro}\\     
				(\up \LFGfeat{case}) $=$ \LFGfeat{acc}\\
                  (\up \LFGfeat{pers}) $=$ 3\\
                  (\up \LFGfeat{num}) $=$ \LFGfeat{sg} \\
                  (\up \LFGfeat{gend}) $=$ \LFGfeat{fem}}}
\ex {\makebox[4em][l]{{\it you}\/: {D}}\qquad\feqs{
        (\up \LFGfeat{pred}) $=$ \predvall{pro}\\     
                  (\up \LFGfeat{pers}) $=$ 2}}
\ex {\makebox[4em][l]{-{\it s}\/: {\it infl}}\qquad\feqs{
				(\up \LFGfeat{tense}) = \LFGfeat{pres}\\
                  (\up \LFGfeat{subj}) $=$ \down\\
                  \qquad(\down\ \LFGfeat{pers}) $=$ 3\\
                  \qquad(\down\ \LFGfeat{num}) $=$ \LFGfeat{sg}\\
                  \qquad(\down\ \case) $=$ \LFGfeat{nom} }}
 \zl
The variant of (\ref{case}a) with \textit{her} as subject is ruled out due to a clash of \LFGfeat{case}  within the \subj{}  f-structure.  The variant with \textit{you} as subject is ruled out due to a clash of \LFGfeat{pers} features.  This mechanism is essentially the same as in HPSG, where it operates via the valence features.  

This account allows for underspecification of both the case assigner and the case-bearing element,
and of both the trigger and target of agreement.   In  \ili{English}, for example, gender is marked on
some pronouns but not on a verbal affix, and (nominative) case is not marked on   nominals, with the
exception of  pronouns, but is governed by the finite verb.  But certain case and agreement
phenomena do not tolerate underspecification, and for those phenomena LFG offers an account using a
\textit{constraining equation}, a mechanism absent from HPSG and indeed ruled out by current
foundational assumptions of HPSG theory \crossrefchapterp{formal-background}.  (Some early precursors to HPSG included a special feature value called \LFGfeat{any} that functioned much like an LFG constraining equation, e.g.\ \citealt[36--37]{Shieber86a}, but that device has been eliminated from HPSG.)  The functional equations described so far in this chapter work by building the f-structure, as illustrated in Figure~\ref{fig-tree1} and (\ref{fs1}) above; such equations are called \textit{defining equations}.  A constraining equation has the same syntax as a defining equation, but it functions by checking the completed f-structure for the presence of a feature.  An f-structure lacking the feature designated by the constraining equation is ill-formed.  

The following lexical entry for \textit{she} is identical to the one in (\ref{case}b) above, except that the \LFGfeat{case} equation has been replaced with a constraining equation, notated with $=_c$.  

\ea
\label{constrain}
{\makebox[4em][l]{{\it she}\/: {D}}\qquad\feqs{
    (\up \LFGfeat{pred}) $=$ \predvall{pro}\\
				(\up \LFGfeat{case}) $=_c$ \LFGfeat{nom}\\
                  (\up \LFGfeat{pers}) $=$  3\\
                  (\up \LFGfeat{num}) $=$  \LFGfeat{sg}\\ 
                  (\up \LFGfeat{gend}) $=$  \LFGfeat{fem}} }
\z
The f-structure is built from the defining equations, after which the \subj{} field is checked for the presence of the [\LFGfeat{case} \LFGfeat{nom}] feature, as indicated by the constraining equation.  If this feature has been contributed by the finite verb, as in (\ref{case}), then the sentence is predicted to be grammatical; if there is no finite verb (and there is no other source of nominative case) then it is ruled out.  This predicts the following grammaticality pattern:

\begin{exe}
\ex  Who won the popular vote in the 2016 election? 
\label{she}
\begin{xlist}
\ex[]{She did!  / *Her did!}
\ex[*]{She! / Her!}
\end{xlist}
\end{exe}
\ili{English} nominative pronouns require the presence of a finite verb, here the finite auxiliary \textit{did}.  Constraining equations operate as output filters on f-structures and are the primary way to grammatically specify the obligatoriness of a form, especially under the assumption that all daughter nodes are optional in the phrase structure.  As described in Section~\ref{valence-sec} above, obligatory dependents are specified in the lexical form of a predicator using existential constraints like (\up \subj) or (\up \textsc{obj}).  These are equivalent to constraining equations in which the particular value is unspecified, but some value must appear in order for the f-structure to be well-formed.  

A constraining equation for case  introduced by the case-\textit{assigner}, rather than the case-bearing element, predicts that the appropriate case-bearing element must appear.  A striking example from \ili{Serbo-Croatian} is described by \citet[134]{WZ2003a}, who give this descriptive generalization:

\ea
{\label{dat-inst}
\ili{Serbo-Croatian} Dative/Instrumental Case Realization Condition:\\
If a verb or noun assigns dative or instrumental case to an NP, then that case must be morphologically realized by some element within the NP.}
\z

\noindent
In \ili{Serbo-Croatian} most common nouns, proper nouns, adjectives, and determiners are inflected for case.  An NP in a dative position must contain at least one such item morphologically inflected for dative case, and similarly for instrumental case.  The verb {\it pokloniti} `give' governs a dative object, such as \textit{ovom  studentu} in (\ref{pokloniti}a).  But a quantified NP like \textit{ovih pet studenata} `these five students' has invariant case, namely genitive on the determiner and noun, and an undeclinable numeral \textit{pet} `five'.  Such a quantified NP can appear in any case position, except when it fails to satisfy the condition in (\ref{dat-inst}), such as this dative position \citep[125]{WZ2003a}:

\begin{exe} 
\ex	\label{pokloniti} 
\begin{xlist}
\ex[ ]{ 		\gll  pokloniti  knjige  ovom  studentu \\ 
give.{\sc inf}  books.{\sc acc}  this.{\sc dat.sg}  student.{\sc dat.sg} \\ \hfill (\ili{Serbo-Croatian})
\glt `to give books to this student'}  
\ex[*] {  \gll {pokloniti}  knjige  [ovih  pet  studenata] \\
give.{\sc inf}  books.{\sc acc}  \spacebr{this.{\sc gen.pl}}  five  student.{\sc gen.pl} \\
\glt intended: `to give books to these five students'  } 
\end{xlist}
\end{exe}

\begin{sloppypar}
\noindent
  Similarly, certain foreign names such as \textit{Miki} and loanwords such as \emph{braon} `brown', `brunette' are undeclinable, and can appear in any case position, except those ruled out by (\ref{dat-inst}).  Thus  example (\ref{admireMiki}a) is unacceptable, while the inflected possessive
adjective \emph{mojoj} `my'  saves it, as shown in (\ref{admireMiki}b).   When the possessive adjective realizes the case
feature, it is acceptable.  In  (\ref{admireMiki}c) we contrast the undeclined loan word {\it
braon} `brown' with the inflected form \emph{lepoj} `beautiful'.  The example is acceptable only
with the inflected adjective \citep[134]{WZ2003a}.  
\end{sloppypar}

\begin{exe} 
\ex	\label{admireMiki} 
\begin{xlist}
\ex[*] {\gll  Divim  se   Miki. \\
admire.{\sc 1sg}  {\sc refl}  Miki \\\hfill (\ili{Serbo-Croatian})
\glt `I admire (my) Miki.'}
\ex[ ] {\gll  Divim  se  mojoj  Miki. \\
admire.{\sc 1sg}  {\sc refl}  my.{\sc dat.sg}  Miki \\
\glt `I admire (my) Miki.'}
\ex[]{
\gll Divim                se            *braon              /  lepoj  Miki. \\
     admire.\textsc{1sg}  \textsc{refl} \hphantom{*}brown {} beautiful.\textsc{dat.sg}  Miki \\
\glt intended: `I admire brunette / beautiful Miki.'}
\end{xlist}
\end{exe}

\noindent
This complex distribution is captured simply by positing that the dative (and instrumental) case assigning equations on verbs and nouns, such as the verbs \textit{pokloniti} and \textit{divim} in the above examples, are constraining equations:

\ea
(\up \LFGfeat{obl}$_{dat}$ \LFGfeat{case}) =$_c$ \LFGfeat{dat}
\z
Any item in dative form within the NP, such as \textit{ovom} or \textit{studentu} in (\ref{pokloniti}a) or \textit{mojoj} or \textit{lepoj} in (\ref{admireMiki}b,c), could introduce the [\LFGfeat{case} \LFGfeat{dat}] feature that satisfies this equation, but if none appears then the sentence fails.  In contrast, other case-assigning equations (e.g., for nominative, accusative, or genitive case, or for cases assigned by prepositions) are defining equations, which therefore allow the undeclined NPs to appear.  This sort of phenomenon is easy to capture using an output filter such as a constraining equation, but rather difficult otherwise.  See \citet{wechsler2001case} for further examples and discussion.  

\section{Agreement and affixal pronouns}
\label{sec:agre-affix-pron}

Agreement inflections that include the person feature derive historically from incorporated pronominal affixes.  Distinguishing between agreement markers and affixal pronouns can be a subtle and controversial matter.  LFG provides a particular formal device for representing this distinction within the f-structure:  a true pronoun, whether affixal or free, introduces a semantic form (formally a \textsc{pred} feature) with the value `{pro}', while an agreement inflection does not.  
For example, \citet{bresnan+mchombo:1987} argue that the \ili{ChichewÌa} (\ili{Bantu}) object marker ({OM}) is an incorporated pronoun, while the subject marker ({SM}) alternates between agreement and incorporated pronoun, as in this example: 


 \begin{exe} 
\ex	\label{bees}
{\gll Nj\^{u}chi   zi-n\'a-w\'{a}-lum-a  a-lenje. \\
 10.bee 10.\textsc{sm}-\textsc{pst}-2.\textsc{om}-bite-\textsc{fv} 2-hunter \\ \hfill (\ili{ChichewÌa})
\glt `The bees bit them, the hunters.' }
\end{exe}

\noindent
According to \citet[745]{bresnan+mchombo:1987},  the class 2 object marker \textit{w\'{a}-} is a pronoun, so the phrase \textit{alenje} `the hunters' is not the true object, but rather a postposed topic cataphorically linked to the object marker, with which it agrees in noun class (a case of \textit{anaphoric agreement}).  Meanwhile, the class 10 subject marker \textit{zi-} alternates: when an associated subject NP (\textit{njuÌchi} âbeesâ in (\ref{bees})) appears, then it is a grammatical agreement marker, but when no subject NP appears, then it functions as a pronoun.  This is captured in LFG with the simplified lexical entries in (\ref{affixes}):

\eal 
 \label{affixes}
\ex {\makebox[4em][l]{{\it lum}\/: V}\qquad\feqs{
(\up \LFGfeat{pred}) = `bite$\leftangle(\up \LFGfeat{subj})(\up \LFGfeat{obj})\rangle$'}  }  
\ex {\makebox[4em][l]{{\it w\'{a}-}\/: {\it Aff}}\qquad\feqs{
				(\up \LFGfeat{obj} \LFGfeat{gend}) = \LFGfeat{2}\\
                  (\up \LFGfeat{obj} \LFGfeat{pred}) = `{pro}'   } }
\ex {\makebox[4em][l]{{\it zi-}\/: {\it Aff}}\qquad\feqs{
				(\up \LFGfeat{subj} \LFGfeat{gend}) = \LFGfeat{10}\\
                  ((\up \LFGfeat{subj} \LFGfeat{pred}) = `{pro}') } }
 \zl 
 
\noindent
The \LFGfeat{pred} feature in (\ref{affixes}b) is obligatory while that of (\ref{affixes}c) is optional, as indicated by the parentheses around the latter.  These entries interact with the grammar in the following manner.  The two grammatical functions governed by the verb in (\ref{affixes}a) are \LFGfeat{subj} and \LFGfeat{obj} (the \textit{governed} functions are the ones designated in the predicate argument structure of a predicator).  According to the \isi{Principle of Completeness}, a \LFGfeat{pred} feature must appear in the f-structure for each governed grammatical function that appears within the angle brackets of a predicator (indicating assignment of a semantic role).  By the uniqueness condition it follows that there must be \textit{exactly one} \LFGfeat{pred} feature, since a second such feature would cause a clash of values.\footnote{Recall that each \LFGfeat{pred} value is assumed to be unique, so that two `{pro}' values cannot unify.}  

The OM {\it w\'{a}-} introduces the [\LFGfeat{pred} `{pro}'] into the object field of the f-structure of this sentence; the word \textit{alenje} `hunters' introduces its own \LFGfeat{pred} feature with value \predvall{hunter}, so it cannot be the true object, and instead is assumed to be in a \isi{topic} position.  \citet[744--745]{bresnan+mchombo:1987} note that the OM can be omitted from the sentence, in which case the phrasal object (here, \textit{alenje}) is fixed in the immediately post-verbal position, while that phrase can alternatively be preposed when the OM appears.  This is explained by assuming that the post-verbal position is an \LFGfeat{obj} position, while the adjoined \LFGfeat{topic} position is more flexible.  

The subject \textit{nj\^{u}chi} can be omitted from (\ref{bees}), yielding a grammatical sentence meaning `They (some class 10 plural entity) bit them, the hunters.'  The optional \LFGfeat{pred} feature equation in (\ref{affixes}c) captures this pro-drop property: when the equation appears, then a phrase such as \textit{nj\^{u}chi} cannot appear in the subject position, since this would lead to a clash of \LFGfeat{pred} values 
(`{pro}' versus `{bee}'); but when the equation is not selected, then  \textit{nj\^{u}chi} must appear in the subject position in order for the f-structure to be complete.  

\largerpage[2]
The diachronic process in which a pronominal affix is reanalyzed as agreement has been modeled in LFG as the historic loss of the \LFGfeat{pred} feature, along with the retention of the pronoun's person, number, and gender features \citep{coppock+wechsler:2010}.  The anaphoric agreement of the older pronoun with its antecedent then becomes reanalyzed as grammatical agreement of the inflected verb with an external nominal.  Finer transition states can also be modeled in terms of selective feature loss.  Clitic doubling can be modeled as optional loss of the \LFGfeat{pred} feature with retention of some semantic vestiges of the pronominal, such as specificity of reference.

The LFG analysis of  affixal pronouns and agreement inflections can be translated into HPSG by associating the former but not the latter with pronominal semantics in the \textsc{content} field.  
The complementarity between the appearance of a pronominal inflection and an analytic phrase filling the same grammatical relation would be modeled as an exclusive disjunction between those two options, which captures the effects of the uniqueness of \LFGfeat{pred} values in LFG.  
                  
                  
\section{Lexical mapping}

LFG and HPSG both adopt \textit{lexical approaches to argument structure} in the sense of \citet{MWArgSt}: a verb or other predicator is equipped with a valence structure indicating the grammatical expression of its semantic arguments as syntactic dependents.  Both frameworks have complex systems for mapping semantic arguments to syntactic dependents that are designed to capture prevailing semantic regularities within a language and across languages.   The respective systems differ greatly in their notation and formal properties, but it is unclear whether 
there are any theoretically interesting differences, such as types of analysis that are available in one but not the other.  This section identifies some of the most important  analogues across the two systems, namely LFG's \isi{Lexical Mapping Theory} \citep[LMT;][Chapter 14]{BATW2016a} and the theory of macro-roles proposed by Davis and Koenig for HPSG (\citealt{Davis96a-u,DK2000b-u}; see \crossrefchapteralt[Section~\ref{arg-st:sec-linking}]{arg-st}).\footnote{A recent alternative to LMT based on Glue Semantics has been developed by \citet{asudeh;giorgolo-lfg12} and \citet{asudeh;ea14-lfg}, incorporating the formal mapping theory of \citet{findlay16}, which in turn is based on \citet{kibort07}. It preserves the monotonicity of LMT, discussed below, but uses Glue Semantics to model argument realization and valence alternations. \citet{MuellerLFGphrasal} discusses this Glue approach in contrast to HPSG approaches in light of \isi{lexicalism} and \isi{argument structure}. Note, though, that despite what might be implied by the title of the \citeauthor{MuellerLFGphrasal} volume, the \citeauthor{asudeh;ea14-lfg} treatment is not \emph{necessarily} \qterm{phrasal} (i.e., non-lexicalist). The  Glue framework assumed by \citeauthor{asudeh;ea14-lfg} can accommodate either a lexicalist or non-lexicalist position. It is a theoretical matter as to which is correct.}

\largerpage[2] 
In LMT, the argument structure is a list of a verb's argument slots, each labeled with a thematic role type such as Agent, Instrument, Recipient, Patient, Location, and so on, in the tradition of Charles Fillmore's \textit{Deep Cases} \citep{Fillmore68,Fillmore77} and P\={a}\d{n}ini's \textit{k\={a}rakas} \citep{kiparsky+staal:1969}.  The ordering is determined by a thematic hierarchy that reflects priority for subject selection.\footnote{The particular ordering proposed in \cite[23]{BresnanK89a-u} and 
\cite[329]{BATW2016a} is the following:  
{\it agent $\succ$ beneficiary $\succ$
experiencer/goal $\succ$ instrument $\succ$ patient/theme $\succ$
locative}.}  The thematic role type influences a further classification by the features $[\pm o]$ and $[\pm r]$ that conditions the mapping to syntactic functions \citep[this version is from][331]{BATW2016a}:

\begin{exe}
  \ex\label{class}
  \begin{tabular}[t]{@{}l}
    Semantic classification of argument structure roles for function:\\
  \begin{tabular}[t]{@{}lcc}
        patientlike roles: &&    \begin{tabular}[t]{c}
                                         $\theta$ \\[-.5ex]
                                          $[-r]$
                                 \end{tabular}  \\
                           && \\
        secondary patientlike roles:  && \begin{tabular}[t]{c}
                                          $\theta$\\[-.5ex]
                                          $[+o]$
                                 \end{tabular}\\ 
                           && \\
        other semantic roles: &&          \begin{tabular}[t]{c}
                                          $\theta$\\[-.5ex]
                                          $[-o]$
                                 \end{tabular}
  \end{tabular} 
  \end{tabular}
\end{exe} 
  
\begin{sloppypar}
\noindent  
The features $[\pm r]$ (thematically \textit{restricted}) and $[\pm o]$ (\textit{objective}) cross-classify grammatical functions: subject is $[-r, -o]$, object is $[-r, +o]$, obliques are $[+r, -o]$ and
restricted objects are $[+r, +o]$.   A monotonic derivation (where feature values cannot be changed) starts from the argument list with the Intrinsic Classification (I.C. in example  (\ref{yam}) below), then morpholexical operations such as passivization can suppress a role (not shown), then the thematically highest role (such as the Agent), if $[-o]$, is selected as Subject, %(\textit{Sbj.} in example (\ref{yam}))
and then any remaining features receive positive values by default.
%(\textit{Def.} in example  (\ref{yam})).  
\end{sloppypar}

 \begin{exe}
\ex\label{yam}{Derivation of \textit{eat} as in \textit{Pam ate a yam.}:\\
\begin{tabular}[t]{@{}lllccll}
a-structure: &{\it eat}& $<$& \textit{agent} & \textit{theme}   & $>$ & \\
             & I.C.      &    & $[-o]$ & $[-r]$   &   & \\
             &  Subject     &    & $[-r]$ &            &              & \\
             &  Default     &    &     & $[+o]$   &   & \\
             &       &    &\vline    & \vline &    & \\
f-structure: &       &    &{\sc subj} &{\sc obj}  &   &
\end{tabular}
  }
\end{exe}

\largerpage[2]
\noindent
In the macro-role theory formulated for HPSG, the analogues of $[-o]$ and $[-r]$ are the macro-roles Actor (\LFGfeat{act}) and Undergoer (\LFGfeat{und}), respectively.  The names of these features reflect large general groupings of semantic role types, but there is not a unique semantic entailment such as ``agency'' or ``affectedness'' associated with each of them.  Actor  and Undergoer  name whatever semantic roles map to the subject and object, respectively, of a transitive verb.\footnote{Note, for example, that within this system the \qterm{Undergoer} argument of the \ili{English} verb \textit{undergo}, as in \textit{John underwent an operation}, is the object, and not the subject as one might expect if being an Undergoer involved actually undergoing something.}  On the semantic side they are disjunctively defined: \textit{x} is the Actor and \textit{y} is the Undergoer iff ``\textit{x} causes a change in \textit{y}, or \textit{x} has a notion of \textit{y}, or \ldots '' (quoted from \crossrefchapteralt[\pageref{def-act-und-rel}]{arg-st}).  Such disjunctive definitions are the HPSG analogues of the LMT \qterm{semantic classifications} shown in (\ref{class}) above.   In the HPSG macro-role system, linking constraints dictate that the \LFGfeat{act} argument maps to the first element of \LFGfeat{arg-st}, and that the \LFGfeat{und} argument maps to some nominal element of \LFGfeat{arg-st}; (\ref{act}) and (\ref{und}) are from \crossrefchaptert[\pageref{act-vb-linking}]{arg-st}  
% , examples (\ref{act-vb-linking}) and (\ref{und-vb-linking});
(each set of dots in the list value of \textsc{arg-st} represents zero or more list items):

\ea
\label{act}
\avm{
  [content|key & [act & \1 ] \\
   arg-st & < NP$_{\1}$, \ldots > ]
}
\z

\ea
\label{und}
\avm{
  [content|key & [und & \2 ] \\
   arg-st & < \ldots, NP$_{\2}$, \ldots > ]
}
\z
\largerpage[2]
The first element of \LFGfeat{arg-st} maps to the subject of an active voice verb, so (\ref{act})--(\ref{und}) imply that if there is an \LFGfeat{act}, then that \LFGfeat{act} is the subject, and otherwise the \LFGfeat{und} is the subject (in the latter case \textsc{np}$_{\ibox{2}}$ is the initial item in the list, given that the ellipsis represents zero or more items).  Similarly, in LMT as described above, the subject is the $[-o]$ highest argument, if there is one, and otherwise it is the $[-r]$ argument.  In this simple example we can see how the two systems accomplish exactly the same thing.  A careful examination of more complex examples might point up theoretical differences, but it seems more likely that the two systems can express virtually the same set of mappings.  

In LFG the \textit{argument structure} (or \textit{a-structure}) contains the predicator and its argument roles classified and ordered by thematic role type and further classified by  Intrinsic Classification.  It is considered a distinct level of representation, along with c-structure and f-structure.  As a consequence, the grammar can make reference to the initial item in a-structure, such as the agent
%(\textit{ag})
in (\ref{yam}), which is considered the \qterm{most prominent} role and often called the \textit{a-subject} (\qterm{argument structure subject}) in LFG parlance.  To derive the passive voice mapping, the a-subject is suppressed in a morpholexical operation that crucially takes place before the subject is selected:  

\begin{exe}
\ex\label{yam2}{Derivation of \textit{eaten} as in \emph{A yam was eaten} (\emph{by Pam}).:\\
\begin{tabular}[t]{@{}lllccll}
a-structure: &{\it eat}& $<$& \textit{agent} & \textit{theme}   & $>$ & \\
             & I.C.      &    & $[-o]$ & $[-r]$   &   & \\
             & Passive      &    & \O  &    &   & \\
             &  Subject     &    &  &       $[-o]$     &              & \\
  %           &  Def.     &    &     & $[+o]$   &   & \\
             &       &    &    & \vline &    & \\
f-structure: &       &    & &{\sc subj} &   &
\end{tabular}
  }
\end{exe}
(The optional \textit{by}-phrase is considered to be an adjunct realizing the passivized a-subject.)  Note that the passive alternation is \textit{not} captured by a procedural rule that replaces one grammatical relation (such as \textsc{obj}) with another (such as \textsc{subj}).   The mapping from word strings to f-structures in LFG is monotonic, in the sense that information cannot be destroyed or changed.  As a result, the mapping between internal and external structures is said to be transparent in the sense that the grammatical relations of parts of the sentence are preserved in the whole (for discussion of this point, see \citealt[Chapter 5]{BATW2016a}).  In early versions of LFG, monotonicity was assumed for the syntax proper, while destructive procedures were permitted in the lexicon.  This was canonized in the \textit{Principle of Direct Syntactic Encoding}, according to which all grammatical relation changes are lexical (\citealt[180]{KB82a-u}; \citealt[77]{BATW2016a}).  At that time, an LFG passive rule operated on fully specified predicate argument structures, replacing \textsc{obj} with \textsc{subj}, and \textsc{subj} with an \textsc{obl}\textsubscript{\emph{by}} or an existentially bound variable.  The advent of LMT brought monotonicity to the lexicon as well.  The HPSG lexicon is also monotonic, if lexical rules are formulated as unary branching rules (see \crossrefchapteralt[Section~\ref{lexicon-sec-nature-of-lexical-rules}]{lexicon}). 


\section{Long distance dependencies}
In LFG a long distance dependency is modeled as a reentrancy in the f-structure.  The HPSG theory of long distance dependencies is based on that of GPSG \citep{Gazdar81a} and uses the percolation of a \LFGfeat{slash} feature through the constituent structure.  But LFG and HPSG accounts are essentially very similar, both working by decomposing a long distance dependency into a series of local dependencies.  As we will see, there are nevertheless some minor differences with respect to what hypothetical extraction patterns can be expressed.  

Both frameworks allow accounts either with or without gaps: regarding LFG see \citet[Chapter 9]{BATW2016a} for gaps and \citet[Chapter 17]{dalrymple;ea19} for gapless; regarding HPSG see \citet[Chapter 4]{ps2} for gaps and \citet[Chapter 14]{SWB2003a} for gapless.  Gaps have been motivated by the (controversial) claim that the linear position of an empty category matters for the purpose of weak crossover and other binding phenomena \citep[210--223]{BATW2016a}.  In this section we compare gapless accounts.

\largerpage
LFG has two grammaticalized discourse functions, \LFGfeat{top} (\isi{topic}) and \LFGfeat{foc} (\isi{focus}).  A sentence with a left-adjoined topic position is depicted in Figure~\ref{fig-tree5}.  The topic phrase \textit{Ann} serves as the object of the verb \textit{like} within the clausal complement of \textit{think}.  This dependency is encoded in the second equation annotating the topic node, where the variable \ensuremath{x} ranges over strings of attributes representing grammatical functions such as \textsc{subj}, \textsc{obj}, \textsc{obl}, or \textsc{comp}.  These strings describe paths through the f-structure.   In this example \ensuremath{x} is resolved to the string \textsc{comp obj}, so this equation has the effect of adding to the f-structure in (\ref{ann}) the curved line representing token identity.  

\begin{figure}
\begin{forest}
sm edges without translation
[IP 
    [DP \\{(\up \LFGfeat{top}) $=$ \down}\\
                         {(\up \LFGfeat{top}) $=$ (\up \ensuremath{x})}
    [Ann] ]
    [IP                      
    		[DP [D [I]]]
    		[VP [V [think]]
    			[IP
    				[DP [D [he]]]
    				[VP [V [likes]]] ] ] ] ] 
\end{forest}
\caption{Long distance dependencies in LFG}\label{fig-tree5}
\end{figure}
		
\ea
\label{ann} 
\avm{
\lfgfst{f}[ top  & \rnode{top}{[ ``\textup{Ann}'' ]} \smallskip\\
            subj & [ ``I'' ] \\
pred &  \predvall{think$\leftangle$(\lfgfst{f} \LFGfeat{subj}) (\lfgfst{f} \LFGfeat{comp}) $\rightangle$}\smallskip\\
comp & \lfgfst{g}[  subj & [ ``\vall{he}'' ] \\
		    pred & \predvall{like$\leftangle$(\lfgfst{g} \LFGfeat{subj})(\lfgfst{g} \LFGfeat{obj}) $\rightangle$}\\
		    obj  & \rnode{obj}{} ] ]
}
\z
% ASH-HELP:  needs curvy line from TOP to OBJ
\nccurve[ncurv=5.6,angleA=0,angleB=0,linewidth=.5pt]{top}{obj}

\largerpage
\noindent
HPSG accounts are broadly similar.  One HPSG version relaxes the requirement that the arguments specified in the lexical entry of a verb or other predicator must all appear in its valence lists.    Arguments are represented by elements of the \textsc{arg-st} list, so the list for the verb \textit{like} contains two NPs, one each for the subject and object.  In a sentence with no extraction, those \textsc{arg-st} list items map to the valence lists, the  first item appearing in \textsc{subj} and any remaining ones in \textsc{comps}.  To allow for extraction, one of those \textsc{arg-st} list items is permitted to appear on the \textsc{slash} list instead.  The \textsc{slash} list item is then passed up the tree by means of strictly local constraints, until it is bound by the topicalized phrase (see \crossrefchapteralt{udc} and \citealt{BMS2001a}).   

The LFG dependency is expressed in the f-structure, not the c-structure.  \citet[Chapter 2]{BATW2016a} note that this allows for category mismatches between the phrases serving as filler and those in the canonical, unextracted position.  This was discussed in Section~\ref{valence-sec} and illustrated with example (\ref{sick}) above.  
%The lexical entry for \textit{talk (about)} in (\ref{talk-about}) selects an \textsc{obl}$_{about}$ \textsc{obj} but does not specify the part of speech category of that argument; meanwhile, the phrase structure rules dictate that the top position allows (at least) DP and CP, while the position right adjacent to \textit{about} can only house a DP.   Category mismatches pose a problem for transformational theories that assume the Projection Principle, since the `moved' constituent should satisfy the conditions imposed on the phrase in its source position.  
%But HPSG seems to permit essentially the same analysis as the LFG analysis just sketched.   In HPSG, the preposition \textit{about} would have a disjunctive NP/CP on its \textsc{arg-st} list item, but only NP is selected for its \textsc{comps} list item; and the topic position would allow either NP or CP.  
 
\begin{sloppypar} 
Constraints on extraction such as accessibility conditions and island\isi{island constraint} constraints  can be captured in LFG by placing constraints on the attribute string \ensuremath{x} \citep[Chapter~17]{dalrymple;ea19}.\footnote{\citet{asudeh-lpr} shows that LFG's \term{off-path constraints} \citep[225--230]{dalrymple;ea19} can even capture quite strict locality conditions on extraction, in the spirit of \term{successive cyclicity} in movement-based accounts \citep{Chomsky73a,Chomsky77a-u}, but without movement/transformations.} If subjects are not accessible to extraction, then we stipulate that \textsc{subj} cannot be the final attribute in \ensuremath{x}; if subjects are islands, then we stipulate that \textsc{subj} cannot be a non-final attribute in \ensuremath{x}.  If the f-structure is the only place such constraints are stated, then 
this makes the interesting (but unfortunately false; see presently) prediction that the theory of extraction cannot distinguish between constituents that map to the same f-structure.  For example, as noted in Section~\ref{mobile-sec}, function words like determiners and their contentful sisters like NP are usually assumed to be f-structure co-heads, so the DP \textit{the lion} maps to the same f-structure as its daughter \textit{lion} (see  Figure~\ref{dp-tree}).  This predicts that if the DP can be extracted, then so can the NP, but of course that is not true:
\end{sloppypar}

\begin{exe} 
\ex	\label{nope}
\begin{xlist}
\ex[ ]{The lion, I think she saw.}
\ex[*]{Lion, I think she saw the.}
\end{xlist}
\end{exe}
\largerpage
These two sentences have exactly the same f-structures, so any explanation for the contrast in acceptability must involve some other level.  For example, one could posit that the phrase structure rules can introduce some items obligatorily (see \citealt[239]{snijders15}), such as an obligatory sister of the determiner \textit{the}.\footnote{This would depart from the assumption that all nodes are optional, adopted in \citet{BATW2016a}.} 

%extractions would involve the same attribute path, namely \textsc{compl obj}.  

%In fact LFG theory does not assume that path constraints exhaust the possibilities for expressing extraction conditions.   The `manner of speaking' verbs in \ref{offpath}a
%
%\begin{exe} 
%\ex	\label{manner}
%Who did Chris think/*whisper that David saw?
%\end{exe}
%\citet{Dalrymple2001a-u} notes that ``There is no reason to assume that the grammatical function of the sentential complements of these two verbs differs'' and instead proposes that  verbs place a boolean feature 
%$[\textsc{ldd} \pm]$ on their clausal complements; nonbridge verbs like whisper assigning $[\textsc{ldd} -]$ and other verbs assigning $[\textsc{ldd} +]$.  Then the extraction path is then subject an \textit{off-path constraint} stating that any \textsc{compl} in the path cannot contain the feature $[\textsc{ldd} +]$.  

\section{Nonconfigurationality}
\label{nonconfig-sec}
Some languages make heavy use of case and agreement morphology to indicate the grammatical relations, while allowing very free ordering of words within the clause.  Such radically nonconfigurational syntax receives a straightforward \pagebreak{}analysis in LFG, due to the autonomy of functional structure from constituent structure.  Indeed, the notion that phrasal position and word-internal morphology can be functionally equivalent is a foundational motivation for that separation of structure and function, as noted in Section~\ref{design-sec} above.  As \citet[5]{BATW2016a} observe, ``The idea that words and phrases are alternative means of expressing the
same grammatical relations underlies the design of LFG, and distinguishes it from other formal syntactic frameworks.''  

The LFG treatment of nonconfigurationality will be illustrated with a simplified analysis, from \citet[352--353]{BATW2016a}, of the clausal syntax of \ili{Warlpiri}, a Pama-Nyungan language of northern Australia.  The following example gives three of the many possible grammatical permutations of words, all expressing the same truth-conditional content.

 \begin{exe} 
\ex	\label{dog}
\begin{xlist}
\ex
\longexampleandlanguage{ 
\gll Kurdu-jarra-rlu wita-jarra-rlu ka-pala maliki wajilipi-nyi. \\
child-\textsc{dual-erg} small-\textsc{dual-erg} \textsc{pres-dual}
dog.\textsc{abs} chase-\textsc{nonpast}\\}{Warlpiri}
\glt `The two small children are chasing the dog.' 
\ex 
\gll Kurdu-jarra-rlu ka-pala maliki wajilipi-nyi wita-jarra-rlu. \\
child-\textsc{dual-erg}  \textsc{pres-dual}
dog.\textsc{abs} chase-\textsc{nonpast} small-\textsc{dual-erg}\\
\glt `The two small children are chasing the dog.' 
\ex 
\gll Maliki ka-pala  kurdu-jarra-rlu wajilipi-nyi wita-jarra-rlu. \\
dog.\textsc{abs} \textsc{pres-dual} child-\textsc{dual-erg} chase-\textsc{nonpast} small-\textsc{dual-erg} 
\\
\glt `The two small children are chasing the dog.' 
\end{xlist}
\end{exe}
\largerpage
The main constraint on word order is that the auxiliary (here, the word \textit{kapala}) must immediately follow the first daughter of S, where that first daughter can be any other word in the sentence, or else a multi-word NP as in (\ref{dog}a).  Apart from that constraint, all word orders are possible.  Any word or phrase in the clause can precede the auxiliary, and the words following the auxiliary can appear in any order. 

The LFG analysis of these sentences works by directly specifying the auxiliary-second constraint within the c-structure rule in (\ref{psg-w}a).\footnote{The c-structure is slightly simplified for illustrative purposes. In the actual c-structure proposed for \ili{Warlpiri}, the second position auxiliary is the c-structure head (of IP) taking a flat S as its right sister (see \citealt[225]{austin+bresnan:1996}).  Because the IP functional projection and its complement S map to the same f-structure (as discussed in Section~\ref{mobile-sec} above),  the analysis presented here works in exactly the same way regardless of whether this extra structure appears.}  Then the lexical entries directly specify the case, number, and other grammatical features of the word forms, including case-assignment properties of the verb (see (\ref{lex-w})).  The framework does the rest, licensing all and only grammatical word orderings and generating an appropriate f-structure (see (\ref{fs-w})).\footnote{The value of LFG's \LFGfeat{adj} feature is a set of f-structures, as there can be multiple adjuncts, in fact indefinitely many. We use the set membership symbol as an attribute \citep[229--230]{dalrymple;ea19}, which results in the f-structure for `small' being in a set.}

\eal  \label{psg-w}
\ex
{
\phraserule{S}{\rulenode{X} \rulenode{(Aux)} \rulenode{X$^*$}
  \hspace*{2em} where X = NP or V}}

\ex 
{
\phraserule{NP}{\rulenode{N$^+$}}}
\zl


\eal  \label{ann-w}
\ex
\label{ann-w-a}
{ Assign (\up \textsc{subj}) = \down or (\up \textsc{obj}) = \down freely to NP.
\ex 
\label{ann-w-b}
{ Assign \up = \down to N, V and Aux.} }
\zl

%{
%\makebox[4em][l]{{\it lion}\/: N}\qquad\feqs{(\up \LFGfeat{pred}) = `lion'\\
%(\up \LFGfeat{num}) = \textsc{sg}}}

\eal \label{lex-w} 
\ex 
{\makebox[8em][l]{{\it kurdu-jarra-rlu}\/: N}\qquad\feqs{
(\up \LFGfeat{pred}) = `child'\\
(\up \LFGfeat{num}) = \LFGfeat{dual} \\
(\up \LFGfeat{case}) = \LFGfeat{erg} }}

\ex 
{\makebox[8em][l]{{\it maliki}\/: N}\qquad\feqs{
    (\up \LFGfeat{pred}) = `dog' \\
    (\up \LFGfeat{num}) = \LFGfeat{sg} \\
    (\up \LFGfeat{case}) = \LFGfeat{abs}}}

\ex 
{\makebox[8em][l]{{\it wita-jarra-rlu}\/: N}\qquad\feqs{
    (\up \LFGfeat{adj}\ \ensuremath{\in} \LFGfeat{pred}) = `small' \\
    (\up \LFGfeat{num}) = \LFGfeat{dual} \\
    (\up \LFGfeat{case}) = \LFGfeat{erg} }}

\ex 
{\makebox[8em][l]{{\it wajilipi-nyi}\/: V}\qquad\feqs{
    (\up \LFGfeat{pred}) = `chase$\langle (\up \LFGfeat{subj}) (\up \LFGfeat{obj})\rangle$'\\
    (\up \LFGfeat{tense}) = \LFGfeat{nonpast}\\
    (\up \LFGfeat{subj}\ \LFGfeat{case}) = \LFGfeat{erg}\\ 
    (\up \LFGfeat{obj}\ \LFGfeat{case}) = \LFGfeat{abs} }}
    
\ex 
{\makebox[8em][l]{{\it ka-pala}\/: Aux}\qquad\feqs{
    (\up \LFGfeat{aspect}) = \textsc{present.imperfect} \\
    (\up \LFGfeat{subj}\ \LFGfeat{num}) = \LFGfeat{dual} }} 
        
\zl


\ea
\label{fs-w} 
\avm{
\lfgfst{f}[ pred & \predvall{chase $\leftangle$(\lfgfst{f} \LFGfeat{subj})(\lfgfst{f} \LFGfeat{obj})$\rightangle$}\smallskip\\
            subj & [  pred & \predvall{child} \\
		      num  & \vall{dual} \\
		      case & \vall{erg} \\
		      adj  & \{ [  pred & \predvall{small} ] \} ] \\
obj & [pred & \predvall{dog} \\
       num  & \vall{sg}\\
       case & \vall{abs} ] \\
tense  & \vall{nonpast} \\
aspect & \vall{present.imperfect} ]
}
\z
The functional annotations on the NP nodes (see (\ref{ann-w})) can vary as long as they secure Completeness and Coherence, given the governing predicate.  In this case the main verb is transitive, so \textsc{subj} and \textsc{obj} must be realized, each with exactly one \textsc{pred} value.\footnote{By the Principle of Completeness, both \textsc{subj} and \textsc{obj} appear in the f-structure; by the Principle of Coherence, no other governable grammatical function appears in the f-structure.}  The noun \textit{wita} `small' is of category N, as \ili{Warlpiri} does not distinguish between nouns and adjectives; but it differs functionally from the nouns for `child' and `dog' in that it modifies another noun.  This is indicated by embedding its \textsc{pred} feature under the \textsc{adj} (\qterm{adjunct}) attribute (see the first equation in (\ref{lex-w}c)).

% Comparing HPSG accounts of nonconfigurationality is instructive.   Two HPSG approaches are described in \crossrefchaptert{order}: the \emph{domain order} approach (\citet{DS99a}, described in \crossrefchaptert{order}, Section~6.2 `Absolutely free'); and the noncancellation approach (\citet{Bender2008a}; described in \crossrefchaptert{order}, Section~7 `Free constituent order languages without order domains').  Interestingly, they both involve the adoption of a separation of levels analogous to LFG's c-structure/f-structure divide.  

Comparing HPSG accounts of nonconfigurationality is instructive.   Two HPSG approaches are described in \crossrefchaptert{order}: the \emph{order domain} approach \citep{DS99a} and the \emph{non-cancellation} approach \citep{Bender2008a}. 
%Interestingly, they both involve the adoption of a separation of levels analogous to LFG's c-structure/f-structure divide.
In the order domain\is{order domain} approach, the words of \ili{Warlpiri} are combined into constituent structures resembling those of a configurational language like \ili{English}.  For example, in an order domain analysis of the \ili{Warlpiri} sentences in (\ref{dog}), as well as all other acceptable permutations of those word forms, the words for `two small children' together form an NP constituent.  However, a domain feature \textsc{dom} lists the phonological forms of the words in each constituent, and allows that list order to vary freely relative to the order of the daughters.  This effective shuffling of the \textsc{dom} list applies recursively on the nodes of the tree, up to the clausal node.  It is the \textsc{dom} list order that determines the order of words for  pronunciation of the sentence.  That function of the \textsc{dom} feature is carried out in LFG by the c-structure.    
%Meanwhile, the phrase structure tree in the domain order account is reinterpreted as similar to LFG's f-structure, displaying the logical relations in the sentence rather than the word order (compare the tree diagram in \crossrefchapteralt{order}, Fig. 11, with the f-structure in (\ref{fs-w}) above).  

The \textit{non-cancellation}\is{non-cancellation} approach  effectively introduces into HPSG correlates of c-structure and f-structure.  In essence, an f-structure is added to HPSG in the form of a feature, much like the \textsc{fs} feature in the pseudo-HPSG rule in (\ref{psg2b}) above.  Instead of \textsc{fs}, that feature is called \textsc{comps} and has a list value.  Unlike the valence feature normally called \textsc{comps}, items of this \textsc{comps} feature are not canceled from the list (and unlike \textsc{arg-st}, this feature is shared between a phrase and its head daughter, so it appears on non-terminal nodes).  The items of that list are referenced by their order in the list.  Special phrasal types define grammatical relations between a non-head daughter and an item in the \textsc{comps} list of the head daughter.  These phrasal types are equivalent to LFG annotated phrase structure rules.  For example, suppose the second item in \textsc{comps} (\textit{2nd-comp}) is the object.  Then \textit{head-2nd-comp-phrase} in \citet[12]{Bender2008a} is equivalent to an LFG rule where the non-head daughter has the \textsc{obj} annotation (see (\ref{ann-w-a})).  Since the list item is not canceled from the list, it remains available for other items to combine and to modify the object, using a different phrasal type.  Non-cancellation mechanisms bring HPSG closer to LFG by relying on a level of structure that is autonomous from the constituent structure  responsible for the grouping and ordering of words for pronunciation. See also \citew{Mueller2008a} for a non-cancellation approach to depictive predicates in English and German. 

%\begin{sloppypar}
%These analyses are not equivalent, and an HPSG incorporating the mechanisms needed for the domain order or the non-cancellation approach to non-configurationality is not thereby equivalent to an LFG.  But such mechanisms do bring HPSG closer to LFG by relying on a level of structure that is autonomous from the constituent structure responsible for the grouping and ordering of words for pronunciation.  
%\end{sloppypar}

\section{Raising and control}
\label{lfg:sec-raising-control}

Raising and control (equi) words can be treated in virtually the same way in LFG and HPSG. Taking raising first, a subject raising word (such as \textit{seem} in (\ref{seem})) specifies that its subject is (also) the subject of its predicate complement (see \crossrefchapterw{control-raising}).  

\begin{exe} 
\ex	\label{seem}
Pam seems to visit Fred.
\begin{xlist} 
\ex	
\word{seem}: \qquad \feqs{(\up \textsc{pred}) $=$ `seem$\leftangle(\up \textsc{xcomp}) \rightangle (\up \textsc{subj})$'\\
(\up \textsc{subj})  $=$  (\up \textsc{xcomp} \textsc{subj})}
\ex 
\word{seem}:  \qquad  $[$ \textsc{arg-st} $\langle$ \ibox{1}\,NP, VP[\textit{inf}, \textsc{subj} $\langle$ \ibox{1} $ \rangle ] \rangle ]$
\end{xlist}
\end{exe}
The LFG entry for \textit{seem} in (\ref{seem}a) contains the grammatical function \textsc{xcomp} (\qterm{open complement}), the function reserved for predicate complements such as the infinitival phrase \textit{to visit Fred}.  The functional control equation specifies that its subject is identical to the subject of the verb \textit{seem}; the  tag \ibox{1} plays the same role in the simplified HPSG entry in (\ref{seem}b).  

The f-structure for (\ref{seem}) is shown here (with simplified structures for \textit{Pam} and \textit{Fred}):

\begin{exe}
\ex \label{seemfs} 
\avm{
\lfgfst{f}[ subj  & \rnode{top}{[ ``\textup{Pam}'' ]} \smallskip\\
            pred  & \predvall{seem$\leftangle$(\lfgfst{f} \LFGfeat{xcomp})$\rightangle$(\lfgfst{f} \LFGfeat{subj})}\smallskip\\
            xcomp & \lfgfst{g}[ pred & \predvall{visit$\leftangle$(\lfgfst{g} \LFGfeat{subj})(\lfgfst{g} \LFGfeat{obj})$\rightangle$}\\
				subj & \rnode{obj}{} \\
				obj  & [ ``\textup{Fred}'' ] ] ]
}
\nccurve[ncurv=8,angleA=0,angleB=0,linewidth=.5pt]{top}{obj}
\end{exe}
% ASH-HELP:  needs curvy line from TOP to OBJ

\largerpage
\enlargethispage{7pt}
Turning next to equi, similar proposals have been made in both frameworks, such that it is the referential index of the controller and the controllee that are identical:
%
\begin{exe} 
\ex	\label{hope}
Pam hopes to visit Fred.
\begin{xlist} 
\ex	
\word{hope}: \qquad \feqs{(\up \textsc{pred}) $=$ `hope$\leftangle (\up \textsc{subj})(\up \textsc{comp}) \rightangle$'\\
  (\up \textsc{comp subj pred})  $=$  `pro'\\
  (\up \LFGfeat{subj index}) $=$ (\up \LFGfeat{comp subj index})}
\ex 
\word{hope}:  \qquad  $[$ \textsc{arg-st} $\langle$ \,NP$_\ibox{1}$, VP[\textit{inf}, \textsc{subj} $\langle$ NP$_\ibox{1}$ $ \rangle ] \rangle ]$
\end{xlist}
\end{exe}
The LFG entry for \textit{hope} in (\ref{hope}a) is adapted from \citet[572]{dalrymple;ea19}. It states that the subject of the controlled infinitival is a pronominal that is coindexed with the subject of the control verb.  Similarly, the subscripted tags in (\ref{hope}b) represent coindexing between the subject of the control verb and the controlled subject of the complement. 


One interesting difference between the two frameworks concerns the representation of restrictions on the grammatical function of the target of control or raising.   The basic HPSG theory of control and raising \citep[for example, the one presented in][132--145]{ps2} allows only for control (or raising) of subjects and not complements.  More precisely, it allows for control/raising of the outermost or final dependent to be combined with the verbal projection that is a complement of the control verb.  This is because of the list cancellation regime that operates with valence lists (on non-cancellation theories, see Section~\ref{nonconfig-sec}).  The expression VP in  (\ref{seem}b/\ref{hope}b) represents an item with an empty \textsc{comps} list.  In a simple \ili{English} clause, the verb combines with its complement phrases to form a VP constituent, with which the subject is then combined to form a clause.  Assuming the same order of combination in the control or raising structure, it is not possible to raise or control the complement of a structure that contains a structural subject, as in (\ref{seemless}a):

\begin{exe} 
\ex 
\label{seemless}
\begin{xlist}
\ex[*]	{Fred seems Pam to visit.}  
\ex[] {Fred seems to be visited by Pam.}  
%\ex[]  {?! (\up \textsc{subj})  $=$  (\up \textsc{xcomp} \textsc{obj})}
\end{xlist}
\end{exe}
\largerpage
The intended meaning would be that of (\ref{seemless}b).  The passive voice is needed in order to make \textit{Fred} the subject of \textit{visit} and thus available to be raised.  This restriction that only subjects can be raised follows from the basic HPSG theory of \citet[132-145]{ps2}, while in LFG it follows only if raising equations like the one in (\ref{lfg-seemless}) are systematically excluded.
\begin{exe} 
\ex 
\label{lfg-seemless}
 {(\up \textsc{subj})  $=$  (\up \textsc{xcomp} \textsc{obj})}
\end{exe}
At the same time, the HPSG framework allows for mechanisms that can be used to violate the restriction to subjects, and such mechanisms have been proposed, including the adoption of something similar to an f-structure in HPSG (this is the non-cancellation theory described in Section~\ref{nonconfig-sec}).  This illustrates the point made in Section~\ref{design-sec} above, that the framework was originally designed to capture locality conditions, but is flexible enough to capture non-local relations as well.  

This raises the question of whether the restriction to subject controllees is universal. In fact, it appears that some languages allow the control of non-subjects, but it is still unclear whether these control relations are established via the grammatical relations and therefore justify equations such as (\ref{lfg-seemless}).  For instance, \citet{kroeger:1993} shows that \ili{Tagalog} has two types of control relation.  In the more specialized type, which occurs only with a small set of verbs or in a special construction in which the downstairs verb appears in non-volitive mood, both the controller and controllee must be subjects.  Kroeger analyzes this type using a functional control equation like the one in (\ref{seem}a).  In the more common type of \ili{Tagalog} control, the controllee must be the Actor argument, while the grammatical relations of controllee and controller are not restricted.  (\ili{Tagalog} has a rich voice system, often called a focus marking system, regulating which argument of a verb is selected as its subject.)  This latter type of \ili{Tagalog} control is defined on argument structure (Actors, etc.), so a-structure rather than f-structure is appropriate for representing the control relations.

% Instead of functional control, Kroeger analyzes this latter type of control with\-in LFG as \textit{anaphoric control}, in which the controllee has the status of a null pronoun whose antecedent is the controller.  In anaphoric control, the f-structures of the controller and controllee are not identified, and instead each has its own \LFGfeat{pred} feature.  An anaphoric control analysis of (\ref{hope}) would involve a structure resembling (\ref{hopefs}) except that the curved line would be eliminated and the \LFGfeat{subj} of \textit{visit} would contain the feature [\LFGfeat{pred} \predvall{pro}].  In the HPSG analogue of anaphoric control, an element of a valence list represents a null pronoun in the following sense:  the rules mapping the valence lists to the phrase structure do not project such an element, and it is accorded the semantic and syntactic properties of a pronoun, such as definiteness and/or specificity, and binding features.


\section{Semantics}

%\subsection{Background: Semantics for LFG and HPSG}

\largerpage\enlargethispage{2pt}
HPSG  was conceived from the
start as  a theory of the \textit{sign} \citep{Saussure16a-Fr}, 
wherein each constituent is a pairing of form and meaning.  
So semantic representation and composition  were built into HPSG (and the 
%subsequent sister 
related framework of Sign-Based Construction Grammar;
%\footnote{Whether SBCG is a sister or daughter framework of HPSG is perhaps up for debate, but probably not a particularly interesting one. \citet[486]{Sag2010b} called it a \scare{version} of HPSG, but that paper also advocates for SBCG based in part on strong subcategorization locality arguments analogous to those in \citet[23--24, for example]{ps2}, which are no longer universally accepted in the HPSG community \citep[e.g.,][]{Meurers99b,Prze99,MuellerLehrbuch1,Bender2008a}. We therefore feel it's more accurate to call SBCG a \qterm{sister} framework than a \qterm{daughter} framework. Forget it, reader. It's Chinatown.}
 \citealt{BS2012a-ed}), as reflected in the title of the first HPSG
book \citep{pollard;sag87}, \textit{Information-Based Syntax and
  Semantics}.  LFG was not founded as a theory that included semantics, but a semantic component was developed for LFG shortly after its foundation \citep{halvorsen83}.  The direction of semantics for LFG changed some ten years later and the dominant tradition is now Glue Semantics \citep{dalrymple;ea93,dalrymple99,Dalrymple2001a-u,asudeh-lpr,dalrymple;ea19}.  

This section presents a basic introduction to Glue Semantics (\glue); this is necessary to fully understand a not insignificant portion of LFG literature of the past fifteen years, which interleaves LFG syntactic analysis with \glue\ semantic analysis. The section is not meant as a direct comparison of LFG and HPSG semantics, for two reasons. First, as explained in the previous paragraph, HPSG is inherently a theory that integrates syntax and semantics, but LFG is not; the semantic module that \glue\ provides for LFG can easily be pulled out, leaving the syntactic component exactly the same.\footnote{On the relation between the \LFGfeat{pred} feature and the semantic component, see footnote~\ref{fn:valence} below.} 
%Some work in Glue Semantics has argued that the subcategorization information in LFG's \LFGfeat{pred} values, governed by Completeness and Coherence, is captured by the compositional logic of \glue\ and therefore can be eliminated from f-structure; see footnote~\ref{fn:valence} below.  However, it is important to realize that this not a necessary move: Glue Semantics is perfectly consistent with the standard f-structural conception of Completeness and Coherence \citep[see, e.g., the discussion of syntactic versus semantic Completeness and Coherence in][53--56, 315--317]{dalrymple;ea19}. If Completeness and Coherence continue to hold at f-structure (or hold redundantly at f-structure), then the \glue\ module can be pulled out, leaving a purely traditional LFG syntax.
%} 
Second, as will become clear in the next section, at a suitable level of abstraction, \glue\ offers an underspecified theory of semantic composition, in particular scope relations, which is also the goal of an influential  HPSG semantic approach, Minimal Recursion Semantics \citep[MRS;][]{copestake;ea05}. But beyond observing this big-picture commonality, comparison of \glue\ and MRS would require a chapter in its own right.
%As this is a handbook on HPSG and this is primarily a chapter on LFG, but Glue Semantics is in fact a general theory of semantic composition that is not necessarily tied to LFG, 
Our goal is to present enough of Glue Semantics for readers to grasp the main intuitions behind it, without presupposing much knowledge of formal semantic theory.  The references listed at the end of the previous paragraph \citep[especially][]{dalrymple;ea19} are good places to find additional discussion and references.

The rest of this section is organized as follows. In Section~\ref{sec:background} we present some more historical background on semantics for LFG and HPSG.  In Section~\ref{sec:glue-semantics}, we present Glue Semantics, as a general compositional system in its own right. Then, in Section~\ref{sec:glue-lfg}, we look at the syntax--semantics interface with specific reference to an LFG syntax. For further details on semantic composition and the syntax--semantics interface in constraint-based theories of syntax, see \crossrefchapterw{semantics} for semantics for HPSG and \citet{asudeh-lfg-glue} for Glue Semantics for LFG.

\subsection{Brief history of semantics for LFG and HPSG}
\label{sec:background}

Various theories of semantic representation have been adopted by the different non-transformational syntactic frameworks over the years.  
The precursor to HPSG, GPSG \citep{GKPS85a}, was paired by its designers with a then fairly standard static Montogovian semantics \citep{Montague73a-u}, but GPSG itself was subsequently adopted as the syntactic framework used by \citet[9]{kamp;reyle93} for Discourse Representation Theory, a dynamic theory of semantics.  Initial work on semantics for LFG also assumed a Montogovian semantics \citep{halvorsen83,halvorsen;kaplan88}.  But with the increasing interest in Situation Semantics \citep{barwise;perry83} in the 1980s at Stanford University and environs (particularly SRI International and Xerox PARC), the sites of the foundational work on both HPSG and LFG, both frameworks 
%also became associated with 
incorporated a Situation Semantics component (on LFG see  \citealt{fenstad;ea87}).  
Interest in the use of Situation Semantics did not last as long in LFG as it did in HPSG, 
%this association was not foundational, manifesting primarily in the form of \citet{fenstad;ea87}, and soon waned. 
%In contrast, as noted above, HPSG is a theory of signs and therefore had semantics incorporated into the theory from the outset.  
%Moreover, the chosen semantic theory was 
%But in HPSG, 
where Situation Semantics was
carried over into the second main HPSG book \citep{ps2} and beyond \citep{ginzburg;sag00}.

\largerpage
Beginning in the nineties, the focus subsequently shifted in new directions due to a new interest in computationally tractable
theories of the syntax--semantics interface, to support efforts at
large-scale grammar development, such as the ParGram project for LFG
\citep{butt;ea99,butt;ea02-pargram} and the LinGO/Grammar Matrix and CoreGram projects for HPSG
\citep{flickinger00,bender;ea02,bender;ea10,MuellerCoreGram}.\footnote{Readers can
  explore the current incarnations of these projects at the following
links (checked 2021-04-30):
{
\begin{itemize}[leftmargin=2em]
\item ParGram: \url{https://pargram.w.uib.no}
%\item[LinGO] \url{http://lingo.stanford.edu}
\item Grammar Matrix: \url{http://matrix.ling.washington.edu/}
\item CoreGram: \url{https://hpsg.hu-berlin.de/Projects/CoreGram.html}
\end{itemize}}
\vspace{-\baselineskip}
} This naturally led to an
interest in underspecified semantic representations, so that semantic
ambiguities such as scope ambiguity could be compactly encoded without
the need for full enumeration of all scope possibilities. Two examples
for HPSG are \emph{Lexical Resource Semantics}\indexlrs \citep{richter04,penn;richter04}
and \emph{Minimal Recursion Semantics}\indexmrs
\citep{copestake;ea05}. Similarly, focus in semantics for LFG shifted
to ways of encoding semantic ambiguity compactly and efficiently. This
led to the development of Glue Semantics.



\subsection{General Glue Semantics}
\label{sec:glue-semantics}

In this section, we briefly review Glue Semantics itself, 
without reference to a particular
syntactic framework.  Glue Semantics is a general framework for
semantic composition that requires \term{some} independent syntactic
framework but does not presuppose anything about syntax except
headedness, which is an uncontroversial assumption across
frameworks. This makes the system flexible and adaptable, and it has
been paired not just with LFG, but also with Lexicalized
Tree-Adjoining Grammar \citep{frank;genabith01}, HPSG
\citep{asudeh;crouch01-hpsg-glue}, Minimalism \citep{Gotham2018}, and
Universal Dependencies \citep{gotham;haug18}.

In Glue Semantics, meaningful
linguistic expressions---including lexical items but possibly also
particular syntactic configurations---are associated
with \term{meaning constructors} of the following form:\footnote{It is
in principle possible for a linguistic expression to have a phonology
and syntax but not contribute to interpretation, such as the expletives \word{there} and \word{it} or the \word{do}-support
auxiliary in \ili{English}; see \citet[113]{asudeh-lpr} for some discussion of
expletive pronouns in the context of \glue.}
\begin{exe}
\ex \formula{\mathcal{M}:G}
\end{exe}
%
\formula{\mathcal{M}} is an expression from a \term{meaning language}
which can be anything that supports the lambda calculus; \formula{G} is an
expression of \term{linear logic} \citep{girard87}, which 
specifies the semantic composition (it ``glues meanings
together''), based on a syntactic parse.  By convention a colon separates them. Glue Semantics is related to (Type-Logical) Categorial
Grammar \citep{carpenter97,morrill94a,morrill11,moortgat97}, but 
%it assumes a separate syntactic representation for handling word order, so 
the terms of the linear logic specify just semantic
composition without regard to word order 
(see \citealt{asudeh-lpr} for further discussion). Glue Semantics is
therefore useful in helping us focus on semantic composition in its
own right.

The principal compositional rules for Glue Semantics are those for the
linear implication connective, $\multimap$, which are here presented in a
natural deduction format, in which each connective has an elimination rule (\formula{\linimpE}, in this case) and an introduction rule (\formula{\linimpI}, in this case).
%
%\begin{exe}
%\ex \label{ex:implE} 
%{\textbf{Functional application : Implication 
%        elimination}} \\ \textbf{(modus ponens)}  \\[2ex] 
%          \begin{tabular}[t]{c}
%    \begin{lfgprooftree}
%      \formula{f:A \linimp B} \hspace*{2em} \formula{a:A}
%      \justifies
%      \formula{f(a):B} \using \linimpE
%    \end{lfgprooftree}
%  \end{tabular}
%  
%  \bigskip
%
%\ex \label{ex:implI} 
%   \textbf{Functional abstraction : Implication 
%        introduction} \\ \textbf{(hypothetical reasoning)}} \\[2ex] 
%          \begin{tabular}[t]{c}
%    \begin{lfgprooftree}
%      \[\formula{[a:A]}^1 
%      \leadsto
%      \formula{f:B}\]
%      \justifies
%      \formula{\lambda a.f:A \linimp B} \using \linimpIi{1}
%    \end{lfgprooftree}
%  \end{tabular} 
%\end{exe}
  
\begin{exe}
% % \ex \label{ex:implE} 
% %   \begin{tabular}[t]{c}
% %     \multicolumn{1}{l}{\textbf{Functional application : Implication 
% %         elimination}} \\ 
% %     \multicolumn{1}{l}{\textbf{(modus ponens)}}  \\[2ex] 
% %     \begin{lfgprooftree}
% %       \formula{f:A \linimp B} \hspace*{2em} \formula{a:A}
% %       \justifies
% %       \formula{f(a):B} \using \linimpE
% %     \end{lfgprooftree}
% %   \end{tabular}

\ex \label{ex:implE}  Functional application~:~Implication elimination \hfill (modus ponens)\medskip\\ 
    \begin{prooftree}
      \hypo{f:A \multimap B} \hypo{a:A}
      \infer2[$\multimap_{\mathcal{E}}$]{f(a):B}
    \end{prooftree}\medskip\\

\ex \label{ex:implI} 
    Functional abstraction~:~Implication introduction \hfill (hypothetical reasoning)\medskip\\ 
    \begin{prooftree}
      \hypo{[a:A]\textsuperscript{1}} 
      \ellipsis{}{f:B}
      \infer1[$\multimap_{\mathcal{I},\,1}$]{\lambda a.f:A \multimap B}
    \end{prooftree}
\end{exe}

\noindent
Focusing first on the right-hand, linear logic side, the implication elimination rule is just standard modus ponens. The implication introduction rule is hypothetical reasoning. A hypothesis is made in the first line as an assumption, indicated by presenting it in square brackets with an index that flags the particular hypothesis/assumption. Given this hypothesis, if through some series of proof steps, indicated by the vertical ellipsis, we derive a term, then we are entitled to discharge the assumption, using its flag to indicate that it is this particular assumption that has been discharged, and conclude that the hypothesis implies the term so-derived. In each of these rules, the inference over the linear logic term corresponds to an operation on the meaning term, via the Curry-Howard Isomorphism between formulas and types \citep{curry;feys58-ch9,howard80}. The rule for eliminating the linear implication corresponds to functional application. The rule for introducing the linear implication corresponds to functional abstraction. These rules will be seen in action shortly.

In general, given some head \formula{h} and some arguments of the head
\formula{a_1, \ldots, a_n}, an implicational term like the following
models consumption of the arguments to yield the saturated meaning of
the head: \formula{a_1 \linimp \ldots \linimp a_n \linimp h}. For
example, let us assume the following meaning constructor for the verb
\word{likes} in the sentence \word{Max likes Sam}:
%
\begin{exe}
  
\ex \label{ex:semantics-1} \formula{\lambda
  y.\lambda x.\func{like}(y)(x):s \linimp m \linimp l}
\end{exe}
%
Let's also assume that \formula{s} is mnemonic for the semantic
correspondent of the (single word) phrase \word{Sam}, \formula{m}
similarly mnemonic for \word{Max}, and \formula{l} for
\word{likes}. In other words, the meaning constructor for \word{likes}
would be associated with the lexical entry for the verb and specified
in some general form such that it can be instantiated by the syntax
(we will see an LFG example shortly); here we are assuming that the
instantiation has given us the meaning constructor in
(\ref{ex:semantics-1}).

Given this separate level of syntax, the glue logic does not
have to worry about word order and is permitted to be commutative
(unlike the logic of Categorial Grammar, see also \crossrefchapteralt{cg} on Categorial Grammar and \crossrefchapteralt[\page \pageref{order-hcs-binary}]{order} on HPSG approaches allowing saturation of elements from the valence lists in arbitrary order). We could therefore freely
reorder the arguments for \word{likes} in \exra{semantics-1} above, as in \exra{semantics-2} below, such that we instead
first compose with the subject and then the object, but still yield
the meaning appropriate for the intended sentence \word{Max likes Sam}
(rather than for \word{Sam likes Max}):
%
\begin{exe}
  
\ex \label{ex:semantics-2}
  \formula{\lambda
  x.\lambda y.\func{like}(y)(x):m \linimp s \linimp l}
\end{exe}
%
As we will see below, the commutativity of the glue logic yields a
simple and elegant treatment of quantifiers in non-subject positions,
which are challenging for other frameworks \citep[see, for example, the
careful pedagogical presentation of the issue
in][244--263]{jacobson14}. 

First, though, let us see how this argument reordering, otherwise
known as Currying or SchÃ¶nfinkelization, works in a
proof, which also demonstrates the rules of implication elimination
and introduction:
%
\begin{exe}
  \ex \label{ex:semantics-3}
  \attop{
  \begin{prooftree}
    \hypo{\lambda y.\lambda x.f(y)(x):a \linimp b \linimp c}     \hypo{[v:a]\textsuperscript{1}}
    \infer2[$\multimap_{\mathcal{E}}$]{\lambda x.f(v)(x):b \multimap c} 
    \hypo{[u:b]\textsuperscript{2}}
    \infer2[$\multimap_{\mathcal{E}}$]{f(v)(u):c}
    \infer1[$\multimap_{\mathcal{I},\,1}$]{\lambda v.f(v)(u):a \multimap c}    
    \infer1[$\Rightarrow_\alpha$]{\lambda y.f(y)(u):a \multimap c}    
    \infer1[$\multimap_{\mathcal{I},\,2}$]{\lambda u.\lambda y.f(y)(u):b \multimap a \multimap c}
  \infer1[$\Rightarrow_\alpha$]{\lambda x.\lambda y.f(y)(x):b \multimap a \multimap c}
\end{prooftree}
}
%\medskip\\
\end{exe}

\largerpage
\noindent
The general structure of the proof is as follows. First, an assumption (hypothesis)
 is formed for each argument, in the order in which they
originally occur, corresponding to a variable in the meaning
language. Each assumed argument is then allowed to combine with the
implicational term by implication elimination. Once the implicational
term has been entirely reduced, the assumptions are then discharged in
the same order that they were made, through iterations of implication
introduction. The result is the original term in curried form, such
that the order of arguments has been reversed but without any change
in meaning. The two steps of $\alpha$-equivalence, notated
$\Rightarrow_\alpha$, are of course  not strictly
necessary, but have been added for exposition. 

This presentation has been purposefully abstract to highlight what is
intrinsic to the glue logic, but we  need to see how this
 works with a syntactic framework to see how Glue Semantics
actually handles semantic composition and the syntax--semantics interface. So next, in
Section~\ref{sec:glue-lfg}, we will review \lfgglue.
% , as this
% is the predominant pairing of syntactic framework and \glue.
% Then, in
% Section~\ref{sec:glue-hpsg} we briefly review \hpsgglue, following
% \citet{asudeh;crouch01-hpsg-glue}.

\subsection{Glue Semantics for LFG}
\label{sec:glue-lfg}

\glue\ for LFG will be demonstrated by analyses of the following three
examples:
%
\eal
\label{ex:calling}
\ex \label{ex:bca} Blake called Alex.
\ex \label{ex:bce} Blake called everybody.
\ex \label{ex:ecs} Everybody called somebody.
\zl
% \begin{exe}
% \ex \label{ex:bca} Blake called Alex.
% \ex \label{ex:bce} Blake called everybody.
% \ex \label{ex:ecs} Everybody called somebody.
% \end{exe}
%
Example (\ref{ex:bca}) is a simple case of a transitive verb with two
proper name arguments, but is sufficient to demonstrate the basics of
the syntax--semantics interface in \lfgglue. Example (\ref{ex:bce}) is a
case of a quantifier in object position, which is challenging to
compositionality because there is a type clash between the simplest
type we can assign to the verb, \bracket{e,\bracket{e,t}}, and the
simplest type that would be assigned to the  quantifier,
\bracket{\bracket{e,t},t}. In other theories, this necessitates either
a syntactic operation which is undermotivated from a purely syntactic perspective, e.g.\  
Quantifier Raising (QR)  in interpretive theories of composition, such as Logical Form semantics \citep{may77,May85a-u,HK98a-u},
or a type shifting operation of some kind in directly compositional
approaches, as in categorial or type-logical frameworks; see
\citet[Chapter 14]{jacobson14} for further discussion and references. Example
(\ref{ex:ecs}) also demonstrates this point, but it more importantly
demonstrates that quantifier scope ambiguity can be handled in \glue\
 without positing an undermotivated syntactic ambiguity, but nevertheless  while
maintaining the simplest types for both quantifiers.

\largerpage
The relevant aspects of the lexical entries
involved are shown in \tablew~\ref{tab:gen-lex}.  Other syntactic aspects of the lexical items, such as the fact that  
\word{called} has a \LFGfeat{subj} and an \LFGfeat{obj}, are specified
in its meaning constructor. Minimal f-structures are
provided below for each example. The subscript $\sigma$ indicates the
semantic structure that corresponds to the annotated
f-structure term. The types for the lexical items are the minimal types
that would be expected. Note that in \glue\ these are normally
associated directly with the semantic structures, for example
\Up$_{\sigma{_e}}$ and (\up \LFGfeat{obj})$_{\sigma{_e}}$ \linimp\ (\up
\LFGfeat{subj})$_{\sigma{_e}}$ \linimp\ \Up$_{\sigma{_t}}$, but they have
been presented separately for better exposition; see
\citet[299--305]{dalrymple;ea19} for further discussion. We do not
show semantic structures here, as they are not necessary for this
simple demonstration. 


The functions associated with \word{everybody} and \word{somebody} are, respectively, \func{every} and \func{some} in the meaning language, where these are the standard quantificational determiners from generalized quantifier theory \citep{Montague73a-u,BC81,keenan;faltz85}. The function \func{every} returns true iff the set characterized by its restriction is a subset of the set characterized by its scope. The function \func{some} returns true iff the intersection of the set characterized by its restriction and the set characterized by its scope is non-empty.  The universal quantification  symbol $\forall$ in the glue logic/linear logic terms for the quantifiers ranges over semantic structures of type $t$.  It is unrelated to the meaning language functions \func{every} and \func{some}.  Hence even the existential word \word{somebody} has the universal $\forall$ in its linear logic glue term.  The $\forall$-terms thus effectively say that \emph{any} type $t$ semantic structure $S$ that can be found by application of proof rules such that the quantifier's semantic structure implies $S$ can serve as the scope of the quantifier; see \citet[393--394]{asudeh05-lp} for basic discussion of the interpretation of $\forall$ in linear logic. This will become clearer when quantifier scope is demonstrated shortly.

\begin{table}
  \centering
\begin{tabular}{lll}
\lsptoprule
Expression & Type & Meaning Constructor\\\midrule
  \word{Alex} & $e$ & \formula{\func{alex}:\upsig}\\
  \word{Blake} & $e$ & \formula{\func{blake}:\upsig}\\
  \word{called} & \bracket{e,\bracket{e,t}} & \formula{\lambda
                                              y.\lambda x.\func{call}(y)(x):(\up
    \LFGfeat{obj})\sig \linimp\ (\up \LFGfeat{subj})\sig \linimp\ \upsig}\\
  \word{everybody} & \bracket{\bracket{e,t},t} &
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(\upsig \linimp\ S)
    \linimp\ S}\\
  \word{somebody} & \bracket{\bracket{e,t},t} &
  \formula{\lambda Q.\func{some}(\func{person},Q):\forall S.(\upsig \linimp\ S) \linimp\ S}
  \\\lspbottomrule
\end{tabular}
\caption{Relevant lexical details  for the three examples in (\ref{ex:calling})}
\label{tab:gen-lex}
\end{table}

Let us assume the following f-structure for (\ref{ex:bca}):
\ea
\avm{
\lfgfst{c}[pred & \predvall{call}\\
           subj & \lfgfst{b}[pred & \predvall{Blake}]\\
           obj  & \lfgfst{a}[pred & \predvall{Alex}] ]
}
\z
%
Note that here, unlike in previous sections, the \LFGfeat{pred} value for the verb does not list its subcategorization information. This is because we've made the move that is standard in much \glue\ work to suppress this information.\footnote{\label{fn:valence}Indeed, one could go further and argue that 
\LFGfeat{pred} values do not list subcategorization at all, in which case the move is not just notational, and that the Principles of Completeness and Coherence instead follow from the resource-sensitivity of Glue Semantics; for some
discussion, see
\citet[112--114]{asudeh-lpr}  and \citet[299--301]{dalrymple;ea19}.} % 
 The
f-structures are named mnemonically by the first character of their
\LFGfeat{pred} value. All other f-structural information has been
suppressed for simplicity. Based on these f-structure labels, the
relevant meaning constructors in the lexicon in \tablew~\ref{tab:gen-lex} are
instantiated as follows ($\sigma$ subscripts suppressed):
%
\begin{exe}
\ex Instantiated meaning constructors:
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\func{blake}:b}\\
  \formula{\func{alex}:a}\\
  \formula{\lambda y.\lambda x.\func{call}(y)(x):a \linimp\ b \linimp\ c}
\end{tabular}
\end{exe}
%
These meaning constructors yield the following proof, which is the only available normal form proof
for the sentence, where $\Rightarrow_\beta$ indicates $\beta$-equivalence:\footnote{\label{fn:norm-proof}%
The reader can think of the normal form proof as the minimal proof that yields the conclusion, without unnecessary steps of introducing and discharging assumptions; see \citet{asudeh;crouch02-wccfl-ellipsis} for some basic discussion.}

\begin{exe}
  \ex
  \begin{minipage}[t]{.99\linewidth}
    Proof:\medskip\\
  \nopagebreak
\oneline{%
\begin{prooftree}
\hypo{\lambda y.\lambda x.\func{call}(y)(x):a \multimap b \multimap c} 
\hypo{\func{alex}:a}
\infer2[$\multimap_{\mathcal{E}}$]{(\lambda y.\lambda
            x.\func{call}(y)(x))(\func{alex}):b \multimap c} 
\infer1[$\Rightarrow_\beta$]{\lambda x.\func{call}(\func{alex})(x):b \multimap c}
\hypo{\func{blake}:b} 
\infer2[$\multimap_{\mathcal{E}}$]{(\lambda x.\func{call}(\func{alex})(x))(\func{blake}):c}
\infer1[$\Rightarrow_\beta$]{\func{call}(\func{alex})(\func{blake}):c}
\end{prooftree}}
\end{minipage}
\end{exe}

\noindent
The final meaning language expression,
\formula{\func{call}(\func{alex})(\func{blake})}, gives the correct truth
  conditions for \word{Blake called Alex}, based on a standard
  model theory.  

Let us next assume the following f-structure for (\ref{ex:bce}):
\ea
\avm{
\lfgfst{c}[pred & \predvall{call}\\
           subj & \lfgfst{b}[pred & \predvall{Blake}]\\
           obj  & \lfgfst{e}[pred & \predvall{everybody}] ]}
\z
%
Based on these f-structure labels, the
meaning constructors in the lexicon are
instantiated as follows ($\sigma$ subscripts again suppressed):
%
\begin{exe}
\ex Instantiated meaning constructors:
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\lambda y.\lambda x.\func{call}(y)(x):e \linimp\ b \linimp\
  c}\\
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(e \linimp\ S)
  \linimp\ S}\\
  \formula{\func{blake}:b}
\end{tabular}
\end{exe}
%
These meaning constructors yield the following proof, which is again 
the only available normal form proof:\footnote{We have not presented
  the proof rule for Universal Elimination, $\forall_{\mathcal{E}}$, but it is trivial; see \citet[396]{asudeh-lpr}.}

\begin{exe}
  \ex
  \begin{minipage}[t]{.99\linewidth}
    Proof:\medskip\\
\nopagebreak
\scalebox{0.95}{
\begin{prooftree}[separation=.5em]\footnotesize
\hypo{\parbox[t]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright$\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap S) \multimap S$}}
\infer1[$\forall_{\mathcal{E}}[c/S]$]{\parbox[t]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$(e \multimap c) \multimap c$}}

\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright$\lambda y.\lambda x.\func{call}(y)(x) :$\\$e \multimap b \multimap c$}}
\hypo{[z:e]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(z)(x): b \multimap c}
\hypo{\func{blake}:b}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{call}(z)(\func{blake}):c}
\infer1[$\multimap_{\mathcal{I},\,1}$]{\lambda z.\func{call}(z)(\func{blake}):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{every}(\func{person}, \lambda z.\func{call}(z)(\func{blake})):c}
\end{prooftree}
}
\end{minipage}
\end{exe}

\noindent
The final meaning language expression,
\formula{\func{every}(\func{person}, \lambda
  z.\func{call}(z)(\func{blake}))}, again gives the correct truth
  conditions for \word{Blake called everybody}, based on a standard
  model theory with generalized quantifiers. 
  
  Notice that the
  quantifier does not move in the syntax, contra QR analyses; see \citet{Gotham2018} for contrastive discussion. The quantifier is just an
  \LFGfeat{obj} in f-structure, and no special type shifting was
  necessary. This is because the proof rules  allow 
  us to temporarily fill the position of the object quantifier with a
  hypothetical meaning constructor that consists of a type $e$ variable
  paired with the linear logic term for the object; this assumption is
  then discharged to return the scope
  of the quantifier, \formula{e \linimp\ c}, and the corresponding
  variable is bound, to yield the function that maps individuals called
  by Blake to a truth value. In other words, we have demonstrated that
  this approach scopes the quantifier without positing an \emph{ad
    hoc} syntactic operation and without complicating the type of the
  object quantifier or the transitive verb. This is ultimately due to
  the commutativity of the glue logic, linear logic, since the proof
  does not have to deal with the elements of composition (words) in
  their syntactic order, because the syntax is separately represented by
  c-structure (not shown here) and f-structure. 

Lastly, let us  assume the following f-structure for (\ref{ex:ecs}), \word{Everybody called somebody}:

\ea
\avm{
\lfgfst{c}[ pred & \predvall{call}\\
            subj & \lfgfst{e}[pred & \predvall{everybody}]\\
            obj & \lfgfst{s}[pred & \predvall{somebody}] ] }
\z

\noindent
Based on these f-structure labels, the
meaning constructors in the lexicon are
instantiated as follows:

\begin{exe}
\ex Instantiated meaning constructors:
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\lambda y.\lambda x.\func{call}(y)(x):s \linimp\ e \linimp\
  c}\\
    \formula{\lambda Q.\func{some}(\func{person},Q):\forall S.(s \linimp\ S)
    \linimp\ S}\\
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(e \linimp\ S)
  \linimp\ S}
\end{tabular}
\end{exe}

\noindent
These meaning constructors yield the following proofs, which are 
the only available normal form proofs, but there are two distinct
proofs, because of the scope ambiguity:\footnote{We have made the
  typical move in \glue\ work of not showing the trivial universal
  elimination step this time.}
%
\begin{exe}
  \ex\label{ex:sws}
  \begin{minipage}[t]{.99\linewidth}
    Proof 1 (subject wide scope):\\
    \nopagebreak
\oneline{
\begin{prooftree}[separation=0.5em]\footnotesize
\hypo{\parbox[b]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap  S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda Q.\func{some}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{some}(\func{person},Q) :$\\$\forall S.(s \multimap S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright $\lambda y.\lambda x.\func{call}(y)(x) :$\\$s \multimap e \multimap c$}}
\hypo{[v:s]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(v)(x):e \multimap c}
\hypo{[u:e]\textsuperscript{2}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{call}(v)(u):c}
\infer1[$\multimap_{\mathcal{I},\,1}$]{\lambda v.\func{call}(v)(u):s \linimp c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{some}(\func{person},\lambda v.\func{call}(v)(u)):c}
\infer1[$\multimap_{\mathcal{I},\,2}$]{\lambda u.\func{some}(\func{person},\lambda v.\func{call}(v)(u)):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{every}(\func{person},\lambda u.\func{some}(\func{person},\lambda v.\func{call}(v)(u))):c}
\infer1[$\Rightarrow_\alpha$]{\func{every}(\func{person},\lambda
  u.\func{some}(\func{person},\lambda y.\func{call}(y)(u))):c}
\infer1[$\Rightarrow_\alpha$]{\func{every}(\func{person},\lambda
  x.\func{some}(\func{person},\lambda y.\func{call}(y)(x))):c}
\end{prooftree}
}
\end{minipage}
\medskip\\

\ex\label{ex:ows}
\begin{minipage}[t]{.99\linewidth}
    Proof 2 (object wide scope):\\
    \nopagebreak
    \oneline{
\begin{prooftree}[separation=0.5em]\footnotesize
\hypo{\parbox[b]{\widthof{$\lambda Q.\func{some}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{some}(\func{person},Q) :$\\$\forall S.(s \multimap S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap  S) \multimap S$}}


\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright $\lambda y.\lambda x.\func{call}(y)(x) :$\\$s \multimap e \multimap c$}}

\hypo{[v:s]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(v)(x):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{every}(\func{person},\lambda x.\func{call}(v)(x)):c}
\infer1[$\multimap_{\mathcal{I},\,1}$]{\lambda v.\func{every}(\func{person},\lambda x.\func{call}(v)(x)):s \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{some}(\func{person},\lambda v.\func{every}(\func{person},\lambda x.\func{call}(v)(x))):c}
\infer1[$\Rightarrow_\alpha$]{\func{some}(\func{person},\lambda
  y.\func{every}(\func{person},\lambda x.\func{call}(y)(x))):c}
\end{prooftree}
}
\end{minipage}\smallskip
\end{exe}
%
\largerpage
%\begin{sloppypar}\noindent 
The final meaning language expressions in \exra{sws} and \exra{ows} 
% , respectively  
%   \formula{\func{every}(\func{person},\lambda
%     x.\func{some}(\func{person},\lambda y.\func{call}(y)(x)))} and
%   \formula{\func{some}(\func{person},\lambda
%     y.\func{every}(\func{person},\lambda x.\func{call}(y)(x)))},
   give
  the two possible readings for the scope ambiguity, again assuming  a
  standard model theory with generalized quantifiers. Once more,
  notice that neither quantifier moves in the syntax (again, contra QR analyses):  they 
  are respectively just a \LFGfeat{subj} and an \LFGfeat{obj} in
  f-structure. 
% \inlinetodostefan{Since you are comparing LFG to HPSG here the readers could draw the wrong conclusion
%   that HPSG does ad hoc syntactic operations (Principle of Relevance). Since this is not the case
%   you should name the enemy \eg \citet{May85a-u} and crossref the chapter on semantics in which MRS
%   and other underspecification techniques are referenced.}
And, once more, no special type shifting is necessary. It
  is a key strength of this approach that even quantifier scope
  ambiguity can be captured without positing \emph{ad hoc} syntactic
  operations (and, again, without complicating the type of the object
  quantifier or the transitive verb). Again, this is  ultimately due to the
  commutativity of the glue logic.
%\end{sloppypar}



\section{Conclusion}

HPSG and LFG  are rather similar syntactic frameworks, both of them important declarative lexicalist alternatives to Transformational Grammar.  They allow for the expression of roughly the same set of substantive analyses, where analyses are individuated in terms of deeper theoretical content rather than superficial properties.  The same sort of analytical options can be compared under both systems, answers to questions such as whether a phenomenon is to be captured on a lexical level or in the syntax, whether a given word string is a constituent or not, the proper treatment of complex predicates, and so on.   Analyses in one framework can often be translated into the other, preserving the underlying intuition of the account.   % This stands in sharp contrast to frameworks such as Distributed Morphology and Minimalism, whose literature includes many theory-internal issues whose substantive content can be difficult to discern.  For example, phrasal constituency is directly represented in both LFG and HPSG, but not in Distributed Morphology or Minimalism, because the tree diagrams in those theories include many putative constituents for which evidence is non-existent, or at the very least scant.   

Against the backdrop of a general expressive similarity, we have pointed out a few specific places where one framework makes certain modes of analysis available that are not found in the other.   The main thesis of this chapter is that the differences between the frameworks stem from different design motivations, reflecting subtly different methodological outlooks.  HPSG is historically rooted in context-free grammars and an interest in the study of locality.  LFG is based on the notion of functional similarity or equivalence between what are externally rather different structures. For example, fixed phrasal positions, case markers, and agreement inflections can all function similarly in signaling grammatical relations.  LFG makes this  functional similarity highly explicit.
%, reflecting its functionalist-friendly outlook on grammar.  
 
\section*{Abbreviations}

\begin{tabularx}{.99\textwidth}{@{}lX}
\textsc{fv} & final vowel\\
\end{tabularx}

\section*{\acknowledgmentsUS}

We would like to thank the editors of the volume, Anne AbeillÃ©, Bob Borsley, Jean-Pierre Koenig, and Stefan MÃ¼ller. JP and Stefan provided particularly close reads of the paper that greatly improved it. We would also like to thank the anonymous reviewers for their insightful comments. Any remaining errors are our own. 

%\fi
%REVERSE THESE AS NEEDED for lfg.tex:
{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
%\input lfg-fast-bibliography.tex
%\bibliography{../bibliographies/stmue,../bibliographies/lfg.bib}
\let\upashtmp\up

\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->
