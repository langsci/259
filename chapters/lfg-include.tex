%% -*- coding:utf-8 -*-

% This is the body of the paper. It should compile with latex and xelatex

% xelatex lfg.tex; biber lfg
% latex   lfg-fast.tex; biber lfg-fast




\begin{document}
\maketitle
\label{chap-lfg}


%\if0
\section{Introduction} 
Head-Driven Phrase Structure Grammar is similar in many respects to its cousin framework, Lexical Functional Grammar or LFG \citep{BATW2015a,dalrymple;ea19}.  Both  HPSG and LFG are lexicalist frameworks in the sense that they distinguish between the morphological system that creates words, and the syntax proper that combines those fully inflected words into phrases and sentences.  Both frameworks assume a lexical theory of argument structure \citep{MWArgSt} in which verbs and other predicators come equipped with valence structures indicating the kinds of complements and other dependents that the word is to be combined with.  Both theories treat control (equi) and raising as lexical properties of certain control or raising predicates.  Both systems are historically related to unification grammar \citep{Kay84a-u}, but replace the procedural operations of unification with mutually consistent declarative  descriptions of formal objects.  Both frameworks use directed graphs that are often represented in the form of recursively embedded feature structures.
%\footnote{For the purpose of this paper, given its goal of exposition of LFG with reference to HPSG, this characterization will do, but it's not strictly true that LFG is a unification-based framework, because unification is really an operation on structures, as in HPSG, whereas the effects of unification in LFG are rather captured by mutually consistent \emph{descriptions} of structures.}   
Phonologically empty nodes of the constituent structure are avoided in both theories, with the gaps appearing in long-distance dependencies as the sole exception in some versions of the theories, and complete elimination of empty categories even in those cases, in others.   

At the same time, there are interesting differences.  Each theory makes available certain representational resources that the other theory lacks.   LFG has output filters in the form of \textit{constraining equations}, HPSG does not.  HPSG's feature structures are \textit{typed}, those of LFG are not.  The feature descriptions (directed graphs) are fully integrated with the phrase structure grammar in the case of HPSG, while in LFG they are intentionally separated in an autonomous level of representation in the form of a \textit{functional structure} or f-structure.  These differences lead some linguists to feel that certain types of generalization are more perspicuously stated in one framework than the other.   Because LFG's functional structure is autonomous from the constituent structure whose terminal yield gives the order of words in a sentence, that functional structure can instead serve as a representation of the grammatical functions played by various components of a sentence.  This makes LFG more amenable to a functionalist motivation, and also provides a standard representation language for capturing the more cross-linguistically invariant properties of syntax.  Meanwhile, HPSG is more deeply rooted in phrase structure grammar, and thus provides a clearer representation of the locality conditions that are important for the proper functioning of grammars.  

This chapter presents a comparison of the two theories with an emphasis on contrasts between the two.  It is organized by grammatical topic.  

\section{Design principles of HPSG and LFG} 
The HPSG and LFG  frameworks are motivated by rather different design considerations.  In order to make a meaningful comparison between the frameworks, it is essential to understand those differences.  

HPSG grew out of the tradition established by \citet{Chomsky57a}, of studying the computational properties of natural language syntax as a window onto the capabilities of human cognitive processes.  The advent of Generalized Phrase Structure Grammar  \citep{GKPS85a} was an important milestone in that tradition, bringing as it did the surprising prospect that the entire range of syntactic phenomena known to exist at the time could be described with a context-free grammar (CFG).  If this could be maintained it would mean that natural language is context-free in the sense of the Chomsky Hierarchy \citep{Chomsky57a}--- an answer to the question raised by Chomsky that was very different from, and more interesting than, Chomsky's own answer.  Then Shieber (1985) and Culy (1985) showed that certain phenomena such as recursive cross-serial dependencies in Swiss German and Bambara exceeded the generative capacity of a context-free grammar.   

Despite that development, it was nonetheless clear that languages for the most part hewed rather closely to the context free design.  Thus began the search for more powerful grammatical formalisms that would preserve the insight of a context-free grammar while allowing for certain phenomena that exceed that generative capacity.\footnote{There were also some problems with the GPSG theory of the lexicon, in which complement selection was assimilated to the phrase structure grammar.  For discussion see \citet{MWArgSt}.}  HPSG grew out of this search. As  \citet[83]{SWB2003a} observe, HPSG ``is still  closely related to standard CFG.''  In fact, an HPSG grammar basically consists of constraints on local sub-trees (i.e., trees consisting of a node and her immediate daughters), which would make it a context-free grammar were it not that the nodes themselves are complex, recursively-defined feature structures.   This CFG-like character of HPSG means that the framework itself embodies an interesting theory of locality.    

The architecture of LFG is motivated by rather different concerns:  an interest in typological variation, the search for linguistic universals and a desire to explain the prevailing grammatical patterns found across languages.  For this reason two levels of representation are discerned, a functional structure or \textit{f-structure} representing the internal logical relations in a sentence that are largely invariant across languages; and a categorial constituent structure or \textit{c-structure} representing the external morphological and syntactic expression of those relations, which vary, often rather dramatically, across various typological varieties of language.  For example, probably all (or nearly all) languages have subjects and objects, hence those relations are represented in f-structure.  But languages vary as to the mechanisms for signaling subjecthood and objecthood,  the three main mechanisms being word order, head-marking, and dependent-marking \citep{Nichols86a-u}, hence those mechanisms are distinguished at c-structure.    The word \textit{functional} in the name of the LFG framework  is a three-way pun, referring to the grammatical \textit{functions} that play such an important role in the framework, the mathematical \textit{functions} that are the basis for the representational formalism, and the generally \textit{functionalist}-friendly nature of the LFG approach.  

Evidence for these different underlying motivations can be found in the later proposals for further theory development by the principle architects of the respective theories.  In some of his last work before his death, Ivan Sag worked on the development of Sign-Based Construction Grammar, a variant of HPSG in which those locality conditions are even more deeply built into the representational system \citep{BS2012a-ed}.  Meanwhile, Joan Bresnan had developed Optimality Theoretic LFG \citep{Bresnan97e}, a framework in which the OT \textit{input} was defined as an underspecified f-structure representing the universal aspects of a syntactic structure, the \textit{candidates set} is the set of external structures mapping to that input, and cross-linguistic variation is modeled with constraint re-rankings.  So while Sag sought to strengthen the locality conditions of the framework, Bresnan sought a more explicit account of universality and variation.  

In the remainder of this chapter we will survey various syntactic and semantic phenomena, showing how the the approaches of the different frameworks reflect those differing design motivations.  

\section{Phrases and Endocentricity} 
A phrasal node shares certain grammatical features with specific daughters. %such as the \textsc{head} features that it shares with the head daughter.  
In HPSG this is accomplished
by means of structure-sharing (reentrancies) in the immediate dominance schemata and other 
constraints on local sub-trees such as the Head Feature Principle.  LFG employs essentially the same mechanism for feature sharing in a local sub-tree but implements it slightly differently, so as to better address the design motivations of LFG.  Each node in a phrase structure is paired with a so-called functional structure or \textit{f-structure}, which is formally a set of attribute-value pairs.  It is through the f-structure that the nodes of the phrase structure share features.   The phrase structure is referred to as \textit{c-structure}, for categorial or constituent structure, in order to distinguish it from f-structure.  The grammar, in the form of a standard rewriting system, directly generates only c-structures, not f-structures.   Those c-structure rules introduce equations that form a projection function from c-structure to f-structure.  For example, the phrase structure grammar in (\ref{psg}) and lexicon in (\ref{lex}) generate the tree in Figure~\ref{fig-tree1}.  

\eal  \label{psg}
\ex
{
\phraserule{S}{\rulenode{NP\\(\up \feat{subj})=\down}
               \rulenode{VP\\ \up=\down}}}

\ex 

{
\phraserule{NP}{\optrulenode{Det\\ \up=\down}
                \rulenode{N\\ \up=\down}}}

\ex 

{
\phraserule{VP}{\rulenode{V\\ \up=\down}
                \optrulenode{NP\\(\up \textsc{obj})=\down}}}

\zl

\eal \label{lex} 
\ex 

{
\makebox[4em][l]{{\it lion}\/: N}\qquad\feqs{(\up \feat{pred}) = `lion'\\
(\up \feat{num}) = \textsc{sg}}}
%\makebox[4em][l]{-{\it s}\/: {\it infl}\/\pslabel{n}}\qquad\feqs{(\up \num) = \sg}

\ex 

{\label{indent}
\makebox[4em][l]{{\it rule}\/: V}\qquad\feqs{(\up \feat{pred}) = `rule$\leftangle
(\up \feat{subj})$'}

\makebox[4em][l]{-{\it s}\/: {\it infl}\/\pslabel{v}}\qquad\feqs{(\up \feat{tense}) = \feat{pres}\\
                  (\up \feat{subj}) = \down\\
                  \qquad(\down\ \feat{pers}) = 3\\
                  \qquad(\down\ \feat{num}) = \feat{sg}}}


\zl



%\begin{forest}
%sm edges
%[S 
%  [NP
%    [Det [those]]
%    [N   [lions]]]
%  [VP
%    [V   [rule]]]]
%\end{forest}

\begin{figure}
\begin{forest}
sm edges without translation
[S 
  [\csn{(\up\feat{subj}) $=$ \down}{DP}
    [\csn{\updown}{Det} [that\\
                         {(\up \feat{num}) $=$ \feat{plur}}\\
                         {(\up \feat{prox}) $=$ \feat{+}}]]
    [\csn{\updown}{N}   [lion\\
                         {(\up \feat{pred}) $=$ \feat{`lion'}}\\
                         {(\up \feat{num}) $=$ \feat{plur}}]]]
  [\csn{\updown}{VP}
    [\csn{\updown}{V}   [rule-s\\
                         {(\up \feat{pred}) $=$ `rule$\langle (\up\feat{subj}) \rangle $'}
                         \\ {(\up \feat{tense}) = \feat{pres}}\\
                  {(\up \feat{subj}) = \down}\\
                 {(\down\ \feat{pers}) = 3}\\
                  {(\down\ \feat{num}) = \feat{sg}} ]]]]
\end{forest}
\caption{Add caption}\label{fig-tree1}
\end{figure}

\noindent
Each node in the c-structure maps to a function, that is, to a set of attribute-value pairs.  Within the equations, the up and down arrows are metavariables over function names, interpreted as follows:  the up arrow refers to the function to which the mother node maps, and the down arrow refers to the function that its own node maps to.  To derive the f-structure from (\ref{fig-tree1}), we instantiate the metavariables to specific function names and solve for the function associated with the root node (here, S).  In Figure~\ref{fig-tree2}, the function names \textit{f1, f2,} etc. are subscripted to the node labels.  The arrows have been replaced with those function names.  

\begin{figure}
\begin{forest}
sm edges without translation
[S$_{f1}$ 
  [\csn{(f1 \feat{subj}) $=$ f2}{DP$_{f2}$}
    [\csn{f2 = f4}{Det$_{f4}$} [this$_{f7}$\\
                         {(f4 \feat{num}) $=$ \feat{sg}}\\
                         {(f4 \feat{prox}) $=$ \feat{+}}]]
    [\csn{f2 = f5}{N$_{f5}$}   [lion$_{f8}$\\
                         {(f5 \feat{pred}) $=$ \feat{`lion'}}\\
                         {(f5 \feat{num}) $=$ \feat{sg}}]]]
  [\csn{f1 = f3}{VP$_{f3}$}
    [\csn{f3 = f6}{V$_{f6}$}   [rules$_{f9}$\\
                         {(f6 \feat{pred}) $=$ `rule$\langle (f6 \feat{subj}) \rangle $'}
                         \\ {(f6 \feat{tense}) = \feat{pres}}\\
                  {(f6 \feat{subj}) = f9}\\
                 {(f9\ \feat{pers}) = 3}\\
                  {(f9\ \feat{num}) = \feat{sg}} ]]]]
\end{forest}
\caption{Add caption}\label{fig-tree2}
\end{figure}
\noindent
Collecting all the equations from this tree and solving for \textit{f1}, we arrive at the f-structure in (\ref{fs1}):

\ea		
\label{fs1} 
{\avmoptions{center}
\begin{avm}
\[ subj &  \[ pred & `\textsc{lion}' \\ num & \textsc{sg} \\ pers & 3 \\ prox & + \] \\
pred & \predvall{rule$\langle (\up\feat{subj}) \rangle $} \\
tense & \textsc{pres} \]
\end{avm}
}
\z

\noindent
Since the up and down arrows refer to nodes of the local subtree, LFG annotated phrase structure rules like those in (\ref{psg}) can often be directly translated into HPSG immediate dominance schemata and principles constraining local subtrees.  
By way of illustration, let \textsc{fs} (for \textit{f-structure}) be an HPSG attribute corresponding to the f-structure projection function.  Then the LFG rule in (\ref{psg2}a) (repeated from (\ref{psg}a above)) is equivalent to the  HPSG rule in (\ref{psg2}b):

\eal 
 \label{psg2}
\ex
{
\phraserule{S}{\rulenode{DP\\(\up \feat{subj})=\down}
               \rulenode{VP\\ \up=\down}}}
               
\ex 
{
\phraserule{S[\textsc{fs} \ibox{1}]}{\rulenode{DP[\textsc{fs} \ibox{2}]}  \rulenode{VP[\textsc{fs}  \ibox{1}[\textsc{subj} \ibox{2}]]}}}
\zl

\noindent
Let us compare the two representations with respect to heads and dependents.

Taking heads first, the VP node annotated with \updown{} is an \textit{f-structure head}, meaning that the features of the VP are identified with those of the mother S.  This effect is equivalent to the tag \ibox{1} in (\ref{psg2}b).    Hence  \updown{} has an effect similar to HPSG's Head Feature Principle.  However, in LFG the part of speech categories and their projections such as N, V, Det, NP, VP, DP, etc. belong to the c-structure and not the f-structure.  As a consequence those features are not subject to sharing, and any principled correlations between such categories, such as the fact that N is the head of NP, V the head of VP, C the head of CP, and so on, are instead captured in an explicit version of (extended) X-bar theory applying to the c-structure \citep{grimshaw98}.  The  LFG based theory of endocentricity is considerably weaker (more permissive) than what is typically found in most transformation based grammars.  The version of extended X-bar theory in \citet[Chapter 6]{BATW2015a} assumes that all nodes on the right side of the arrow of the phrase structure rule are optional, with many unacceptable partial structures ruled out in the f-structure instead.  Also not all structures need to be endocentric (i.e. not all structures have a head daughter in c-structure).  The LFG category S shown in (\ref{psg2}a) is inherently exocentric, lacking a c-structure head, and is used for the analysis of copulaless clauses.   (English is also assumed to have endocentric clauses of category IP, where an auxiliary verb of category I (for Inflection) serves as the c-structure head.)  S is also used for flat structures in non-configurational clauses found in languages such as Warlpiri.   

Functional projections like DP, IP, and CP are typically assumed to form a `shell' over the lexical projections NP, VP, AP, and PP (plus CP can appear over S).  In fact this idea of extending X-bar to functional categories has its origin in the LFG work of the late Yehuda Falk \citep{Falk84a-u}, from which it then spread to  transformational theories.   This is formally implemented by having the functional head (such as Det) and its lexical complement (such as NP) be f-structure co-heads.  See for example the DP \textit{that lion} in (\ref{fig-tree1}), where Det and N are both annotated with \updown .  The DP, Det, and N nodes all map to the same f-structure, namely the subsidiary structure serving as the value of SUBJ in (\ref{fs1}).  What makes this unification possible is that function words lack a PRED (for `predicate') feature that would otherwise indicate a semantic form.  Content words such as \textit{lion} have such a feature ([\textsc{pred} `\textsc{lion}']), and so if the Det had one as well then they would clash in the f-structure.  Note more generally that the f-structure flattens out much of the hierarchical structure of the corresponding c-structure.  



Complementation works a little differently in LFG from HPSG.  Note that the LFG  rule (\ref{psg2}a) indicates the SUBJ grammatical function on the subject DP node, while the pseudo-HPSG rule (\ref{psg2}b) indicates the SUBJ function on the VP functor selecting the subject.   A consequence of the use of functional equations in LFG is that a grammatical relation such as SUBJ can be locally associated with its formal exponents, whether a configurational position in phrase structure (as in (\ref{fig-tree1})), head-marking (agreement, see (\ref{indent})), or dependent marking (case).  A subject-marking case affix such as a nominative inflection can introduce a so-called `inside out' functional designator, (\textsc{subj} \up), which requires that the f-structure of the DP bearing that case ending be the value of a SUBJ attribute \citep{Nordlinger98a-u}.\footnote{For function \textit{f}, attribute \textit{a} and value \textit{v}, $(f \, a) = v$ iff $(a \, v) = f$.}  This aspect of LFG representations makes it convenient for functionalist and typological work on grammatical relations.  


 
\section{Valence} 
\label{valence-sec}
In LFG a lexical predicator such as a verb selects its complements via f-structure rather than c-structure.  A transitive verb selects a SUBJ and OBJ, which are features of f-structure, but it cannot select for the category `DP' because such part of speech categories belong only to c-structure.  For example the verb stem rule in (\ref{lex}b) has a PRED feature whose value contains (\up \feat{subj}), which has the effect of requiring a SUBJ function in the f-structure.    The f-structure (shown in (\ref{fs1})) is built using the defining equations, as described above.  Then that f-structure is checked against any \textit{existential constraints} such as  the expression (\up \feat{subj}), which requires that the f-structure contain a SUBJ feature.  That constraint is satisfied, as shown in (\ref{fs1}).  Moreover, the fact that (\up \feat{subj}) appears in the angled brackets means that it expresses a semantic role of the `rule' relation, hence the SUBJ value is required to contain a PRED feature, which is satisfied by the feature [\textsc{pred} `\textsc{lion}'] in  (\ref{fs1}).  

Selection for grammatical relations instead of formal categories enables LFG to capture the  flexibility in the expression of a given grammatical relation described at the end of the previous section.  As noted there, in many languages the subject can be expressed either as an independent DP phrase as in English, or as a pronominal affix on the verb.  As long as the affix introduces a PRED feature and is designated by the grammar as filling the SUBJ relation, then it satisfies the subcategorization requirements imposed by a verb.  A more subtle example of flexible expression of grammatical functions  can be seen in English constructions where an argument can in principle take the form of either a DP (as in (\ref{sick}a)) or a clause (as in (\ref{sick}b)) (example (\ref{sick}) is from \cite{BATW2015a} PAGE).  

\eal 
 \label{sick}
\ex That problem, we talked about for days.

\ex  That he was sick, we talked about for days.

\ex We talked about that problem for days.

\ex *We talked about that he was sick for days.

\zl
The variant of \textit{talk} taking an \textit{about}-PP selects neither a DP nor a clausal complement, but rather an object (OBJ) of an oblique (OBL$_{about}$) function:  

\eal \label{talk-about}
{{\it talk}\/: V}\qquad\feqs{(\up \feat{pred}) = `talk-about$\leftangle
(\up \feat{subj})(\up \feat{obl}_{about} \feat{obj})\rangle$'}    
\zl

\noindent
It is not the verb but the local c-structure environment that conditions the category of that argument: the canonical object position right-adjacent to \textit{about} can only house a DP, while the topic position allows either DP or clause (as seen by comparing (\ref{sick}c) and (\ref{sick}d)).  In LFG the grammatical functions such as SUBJ and OBJ represent equivalence classes across various modes of c-structure expression.  

HPSG captures this variability in the expression of arguments in essentially the same way as LFG, despite some terminological differences.  The HPSG correspondent of the LFG \textsc{subj} specification is not an HPSG \textsc{subj} valence list item, but rather the item of the \textsc{arg-st} list that the (optional) \textsc{subj}  item maps to, when it appears.  The same applies to complements.  The disjunction between DP and clausal expression of an argument is encoded in the \textsc{arg-st;} the restriction to DP (observed in examples (\ref{sick}c,d)) is encoded in the relevant \textsc{valence} list item.  


\section{Head mobility} 
\label{mobile-sec}
The lexical head of a phrase can sometimes appear in an alternative position apparently outside of what would normally be its phrasal projection.  Assuming that an English finite auxiliary verb is the (category I) head of its (IP) clause, then that auxiliary appears outside its clause in a yes/no question:

\eal 
\label{mad}
\ex {} [\sub{IP} she is mad].
\ex  Is   [\sub{IP} she $\rule{0.5cm}{0.15mm}$ mad]?
\zl
Transformational grammars capture the systematic relation between these two structures with a head-movement transformation that leaves the source IP structure intact, with a trace replacing the moved lexical head of the clause.  The landing site of the moved clausal head is often assumed to be C, the complementizer position, as motivated by complementarity between the fronted verb and a lexical complementizer.  This complementarity is observed most strikingly in German verb-second versus verb-final alternations, but is also found in other languages, including some English constructions such as the following:  

\eal 
\label{mad2}
\ex  I wonder whether [\sub{IP} she is mad]. 
\ex  I wonder,  is  [\sub{IP} she $\rule{0.5cm}{0.15mm}$ mad]?
\ex  *I wonder whether is she mad.
\zl
In HPSG the sentences in (\ref{mad2}) are treated as displaying two distinct structures generated by the grammar.  For example, assuming ternary branching in (\ref{mad}b) then the subject DP \textit{she} and predicate AP \textit{mad} would normally be assumed to be sisters of the fronted auxiliary \textit{is}.  On that analysis the structure is flattened out so that \textit{she mad} is not a constituent.  In fact for English the fronting of \textit{is} can even be seen as a consequence of that flattening:  English is a head-initial language so the two dependents \textit{she} and \textit{mad} are expected to follow their selecting head \textit{is}.  

Although LFG is non-transformational, it can express the intuition behind the I-to-C movement analysis due to  the separation of autonomous c- and f-structures.  Recall from the above discussion of the DP in Figure~\ref{fig-tree1} that functional heads such as determiners, auxiliaries and complementizers do not introduce new f-structures, but rather map to the same f-structure as their complement phrases.   The finite auxiliary can therefore appear in either I or C without affecting the f-structure, as we will see presently.  Recall also that c-structure nodes are optional and can be omitted as long as a well-formed f-structure is generated.  Comparing the non-terminal structures of Figure~\ref{fig-tree3} and Figure~\ref{fig-tree4}, the I node is omitted from the latter structure but otherwise they are identical.   

\begin{figure}
 \begin{forest}
sm edges without translation
[CP
 [\csn{\updown}{C} [whether\\
 {(\up \feat{fin}) $=$ +}]]
[\csn{\updown}{IP} 
  [\csn{(\up\feat{subj}) $=$ \down}{DP} [she\\
      {(\up \feat{pred}) $=$ `\textsc{pro}'}\\ 
      {(\up \feat{pers}) $=$ 3}\\
      {(\up \feat{num}) $=$ \feat{sg}}\\
      {(\up \feat{gend}) $=$ \feat{fem}} ]]
[\csn{\updown}{I'}
    [\csn{\updown}{I} [is\\ 
    {(\up \feat{fin}) $=$ +}\\
    {(\up \feat{subj}) = \down}\\
                 {(\down \feat{pers}) $=$ 3}\\
                  {(\down \feat{num}) $=$ \feat{sg}}
                 ]]
    [\csn{\updown}{AP} [mad\\
     {(\up \feat{pred}) $=$ `mad$\langle (\up\feat{subj}) \rangle $'}]]
     ]]]
   \end{forest}
\caption{Add caption}\label{fig-tree3}
\end{figure}

\begin{figure}
\begin{forest}
sm edges without translation
[CP
 [\csn{\updown}{C} [is]]
[\csn{\updown}{IP} 
  [\csn{(\up\feat{subj}) $=$ \down}{DP} [she]]
  [\csn{\updown}{I'}
 %   [\csn{\updown}{I} [is]]
    [\csn{\updown}{AP} [mad\\
     {(\up \feat{pred}) $=$ `mad$\langle (\up\feat{subj}) \rangle $'}]] ]]]
   \end{forest}
\caption{Add caption}\label{fig-tree4}
\end{figure}
%ASH-HELP:  The example numbers should go above the trees.  How?
%STEVE: Fixed
\noindent
(Most of the lexical equations are omitted from Figure~\ref{fig-tree4} for clarity.)  Given the many \updown{} annotations, the C, I, and AP nodes (as well as IP and CP) all map to the same f-structure, namely the one shown in (\ref{fs2}).  

\ea		
\label{fs2} 
{\avmoptions{center}
\begin{avm}
\[ subj &  \[ pred & `\textsc{pro}' \\ num & \textsc{sg} \\ pers & 3 \\ gen & \textsc{fem} \] \\
pred & \predvall{mad$\langle (\up\feat{subj}) \rangle $} \\
fin & $+$ \]
\end{avm}
}
\z
The C and I positions are appropriate for markers of clausal grammatical features such as finiteness ([\textsc{fin} \textpm]), encoded either by auxiliary verbs like finite \textit{is} or complementizers like finite \textit{that} and infinitival \textit{for}: \textit{I said that/*for she is present} vs. \textit{I asked for/*that her to be present}.  English has a specialized class of  auxiliary verbs for marking finiteness from the C position, while in languages like German all finite verbs, including main verbs, can appear in a C position that is unoccupied by a lexical complementizer.  
Summarizing, the LFG framework enables a theory of head mobility based on the intuition that a clause has multiple head positions where inflectional features of the clause are encoded.  

\section{Case, agreement, and constraining equations} 
The basic theory of agreement is the same in LFG and HPSG (see \crossrefchapteralt{agreement}):  agreement occurs when 
multiple feature sets
 arising from distinct elements of a sentence specify information about a single abstract object, so that the information must be mutually consistent \citep{Kay84a-u}.  
The two forms are said to agree when the values imposed by the two constraints are compatible, while ungrammaticality results when they are incompatible.  An LFG example is seen in (\ref{fig-tree1}) above, where the noun, determiner and verbal suffix each specify person and/or number features of the same \subj{} value.   

The basic mechanism for case marking works in essentially the same way as agreement, in both frameworks: in case marking, distinct elements of a sentence specify case information about a single abstract object, hence that information must be compatible.   To account for the contrast in (\ref{case}a), nominative \feat{case} equations are associated with the pronoun \textit{she} and added to the entry for the verbal agreement suffix \textit{-s}:

\eal 
 \label{case}
\ex She/*Her/*You rules.
\ex {\makebox[4em][l]{{\it she}\/: {\it Pron}}\qquad\feqs{
				(\up \feat{case}) = \feat{nom}\\
                  (\up \feat{pers}) = 3\\
                  (\up \feat{num}) = \feat{sg}\\ 
                  (\up \feat{gend}) = \feat{fem}} }
\ex {\makebox[4em][l]{{\it her}\/: {\it Pron}}\qquad\feqs{
				(\up \feat{case}) = \feat{acc}\\
                  (\up \feat{pers}) = 3\\
                  (\up \feat{num}) = \feat{sg} \\
                  (\up \feat{gend}) = \feat{fem}}}
\ex {\makebox[4em][l]{-{\it s}\/: {\it infl}}\qquad\feqs{
				(\up \feat{tense}) = \feat{pres}\\
                  (\up \feat{subj}) = \down\\
                  \qquad(\down\ \feat{pers}) = 3\\
                  \qquad(\down\ \feat{num}) = \feat{sg}\\
                  \qquad(\down\ \case) = \feat{nom} }}
 \zl
The variant of (\ref{case}a) with \textit{her} as subject is ruled out due to a clash of \feat{case} features within the value of \subj{} in the f-structure.  The variant with \textit{you} as subject is ruled out due to a clash of \feat{pers} features.  This mechanism is essentially the same as in HPSG, where it operates via the \textsc{valence} feature.  

This account allows for underspecification of both the case assigner and the case bearing element, and of both the controller and target of agreement.   In  English, for example, gender is marked on some pronouns but not on a verbal affix; and (nominative) case is marked on the verbal affix but not on  nominals, with the exception of the pronouns.  But certain case and agreement phenomena do not tolerate underspecification, and for those phenomena LFG offers an account using a \textit{constraining equation}, a mechanism absent from HPSG and indeed ruled out by current principles of HPSG theory.  (Some early precursors to HPSG included a special feature value called \feat{any} that functioned much like an LFG constraining equation \citep[36-7]{Shieber86a}, but that device has been eliminated from HPSG.)  The functional equations described so far in this chapter function by building the f-structure, as illustrated in (\ref{fig-tree1}) and (\ref{fs1}) above; such equations are called \textit{defining equations}.  A constraining equation has the same syntax as a defining equation, but it functions by checking the completed f-structure for the presence of a feature.  An f-structure lacking the feature designated by the constraining equation is ill-formed.  

The following lexical entry for \textit{she} is identical to the one in (\ref{case}b) above, except that the \feat{case} equation has been replaced with a constraining equation, notated with $=_c$.  

\ea
\label{constrain}
{\makebox[4em][l]{{\it she}\/: {\it Pron}}\qquad\feqs{
				(\up \feat{case}) $=_c$ \feat{nom}\\
                  (\up \feat{pers}) = 3\\
                  (\up \feat{num}) = \feat{sg}\\ 
                  (\up \feat{gend}) = \feat{fem}} }
\z
The f-structure is built from the defining equations, after which the \subj{} field is checked for the presence of the [\feat{case} \feat{nom}] feature, as indicated by the constraining equation.  If this feature has been contributed by the finite verb, as in (\ref{case}), then the sentence is predicted to be grammatical; if there is no finite verb (and there is no other source of nominative case) then it is ruled out.  This predicts the following grammaticality pattern:

\begin{exe}
\ex  Who won the popular vote in the 2016 election? 
\label{she}
\begin{xlist}
\ex[]{She did!  / *Her did!}
\ex[*]{She! / Her!}
\end{xlist}
\end{exe}
English nominative pronouns require the presence of a finite verb, here the finite auxiliary \textit{did}.  Constraining equations operate as output filters on f-structures and are the primary way to grammatically specify the obligatoriness of a form, especially under the assumption that all daughter nodes are optional in the phrase structure.  As described in Section \ref{valence-sec} above, obligatory dependents are specified in the lexical form of a predicator using existential constraints like (\up \subj) or (\up \textsc{obj}).  These are equivalent to constraining equations in which the particular value is unspecified, but some value must appear in order for the f-structure to be well-formed.  

A constraining equation for case  introduced by the case-\textit{assigner} rather than the case-bearing element, predicts that the appropriate case-bearing element must appear.  A striking example from Serbo-Croatian is described by \citet[134]{WZ2003a}, who give this descriptive generalization:

\ea
{\label{dat-inst}
Serbo-Croatian Dative/Instrumental Case 
Realization Condition.\medskip

If a verb or noun assigns dative or instrumental case to an NP, then that case must be morphologically realized by some element within the NP.}
\z

\noindent
In Serbo-Croatian most common nouns, proper nouns, adjectives, and determiners are inflected for case.  An NP in a dative position must contain at least one such item morphologically inflected for dative case, and similarly for instrumental case.  The verb {\it pokloniti} `give' governs a dative object, such as \textit{ovom  studentu} in (\ref{pokloniti}a).  But a quantified NP like \textit{ovih pet studenata} `these five students' has invariant case, namely genitive on the determiner and noun, and an undeclinable numeral \textit{pet} `five'.  Such a quantified NP can appear in any case position--- except when it fails to satisfy the condition in (\ref{dat-inst}), such as this dative position \citep[125]{WZ2003a}:

\begin{exe} 
\ex	\label{pokloniti}
\begin{xlist}
\ex[ ]{ 		\gll  pokloniti  knjige  ovom  studentu \\
give.{\sc inf}  books.{\sc acc}  this.{\sc dat.sg}  student.{\sc dat.sg} \\
\glt `to give books to this student'}
\ex[*] {  \gll {pokloniti}  knjige  [ovih  pet  studenata] \\
give.{\sc inf}  books.{\sc acc}  this.{\sc gen.pl}  five  student.{\sc gen.pl} \\
\glt (`to give books to these five students')}
\end{xlist}
\end{exe}

\noindent
Similarly, certain foreign names such as \textit{Miki} and loanwords such as {\it braon} `brown, brunette' are undeclinable, and can appear in any case position, except those ruled out by (\ref{dat-inst}).  Thus the
dative example in (\ref{admireMiki}a) is unacceptable unless the inflected possessive
adjective {\it mojoj/mojom} `my'  appears.   When the possessive adjective realizes the case
feature, it is acceptable.  In  (\ref{admireMiki}b) we contrast the undeclined loan word {\it
braon} `brown' with the inflected form {\it lepoj} `beautiful'.  The example is acceptable only
with the inflected adjective \citep[134]{WZ2003a}.  


\begin{exe} 
\ex	\label{admireMiki}
\begin{xlist}
\ex {\gll  Divim  se  *(mojoj)  Miki. \\
admire.{\sc 1sg}  {\sc refl}  my.{\sc dat.sg}  Miki \\
\glt `I admire (my) Miki.'}
\ex {  \gll Divim  se  {*braon/ lepoj}  Miki. \\
admire.1sg  {\sc refl}  {brown/ beautiful.{\sc dat.sg}}  Miki \\
\glt (`I admire {brunette/ beautiful} Miki')}
\end{xlist}
\end{exe}

\noindent
This complex distribution is captured simply by positing that the dative (and instrumental) case assigning equations on verbs and nouns, such as the verbs \textit{pokloniti} and \textit{divim} in the above examples, are constraining equations:

\ea
(\up \feat{obl}$_{dat}$ \feat{case}) =$_c$ \feat{dat}
\z
Any item in dative form within the NP, such as \textit{ovom} or \textit{studentu} in (\ref{pokloniti})a or \textit{mojoj} or \textit{lepoj} in (\ref{admireMiki}), could introduce the [\feat{case} \feat{dat}] feature that satisfies this equation, but if none appears then the sentence fails.  In contrast, other case-assigning equations (e.g. for nominative, accusative, or genitive case, or for cases assigned by prepositions) are defining equations, which therefore allow the undeclined NPs to appear.  This sort of phenomenon is easy to capture using an output filter such as a constraining equation, but rather difficult otherwise.  See \citet{wechsler2001case} for further examples and discussion.  

\section{Agreement and affixal pronouns}

Agreement inflections that include the person feature derive historically from incorporated pronominal affixes.  Distinguishing between agreement markers and affixal pronouns can be a subtle and controversial matter.  LFG provides a particular formal device for representing this distinction within the f-structure:  a true pronoun, whether affixal or free, introduces a semantic form (formally a \textsc{pred} feature) with the value `\feat{pro}', while an agreement inflection does not.  
For example, \citet{bresnan+mchombo:1987} argue that the Chiche\^{w}a (Bantu) object marker (OM) is an incorporated pronoun, while the subject marker (SM) alternates between agreement and incorporated pronoun, as in this example: 


 \begin{exe} 
\ex	\label{bees}
{\gll Nj\^{u}chi   zi-n\'a-w\'{a}-lum-a  a-lenje. \\
 10.bee 10.\textsc{sm}-\textsc{pst}-2.\textsc{om}-bite-\textsc{fv} 2-hunter \\
\glt `The bees bit them, the hunters.' }
\end{exe}

\noindent
According to \citet{bresnan+mchombo:1987}  the class 2 object marker \textit{w\'{a}-} is a pronoun, so the phrase \textit{alenje} `the hunters' is not the true object but rather a postposed topic cataphorically linked to the object marker, with which it agrees in noun class (a case of \textit{anaphoric agreement}).  Meanwhile, the class 10 subject marker \textit{zi-} alternates: when an associated subject NP (\textit{njûchi} ‘bees’ in (2)) appears then it is a grammatical agreement marker, but when no subject NP appears then it functions as a pronoun.  This is captured in LFG with the simplified lexical entries in (\ref{affixes}):

\eal 
 \label{affixes}
\ex {\makebox[4em][l]{{\it lum}\/: {\it V}}\qquad\feqs{
(\up \feat{pred}) = `bite$\leftangle(\up \feat{subj})(\up \feat{obj})\rangle$'}  }  
\ex {\makebox[4em][l]{{\it w\'{a}-}\/: {\it Aff}}\qquad\feqs{
				(\up \feat{obj} \feat{gend}) = \feat{2}\\
                  (\up \feat{obj} \feat{pred}) = `\feat{pro}'   } }
\ex {\makebox[4em][l]{{\it zi-}\/: {\it Aff}}\qquad\feqs{
				(\up \feat{subj} \feat{gend}) = \feat{10}\\
                  ((\up \feat{subj} \feat{pred}) = `\feat{pro}') } }
 \zl 
 
\noindent
The \feat{pred} feature in (\ref{affixes}b) is obligatory while that of (\ref{affixes}c) is optional, as indicated by the parentheses around the latter.  These entries interact with the grammar in the following manner.  The two grammatical functions governed by the verb in (\ref{affixes}a) are \feat{subj} and \feat{obj} (the \textit{governed} functions are the ones designated in the predicate argument structure of a predicator).  According to the \textit{Principle of Completeness}, a \feat{pred} feature must appear in the f-structure for each governed grammatical function that appears within the angle brackets of a predicator (indicating assignment of a semantic role).  By the uniqueness condition it follows that there must be \textit{exactly one} \feat{pred} feature, since a second such feature would cause a clash of values.\footnote{Each \feat{pred} value is assumed to be unique, so that two `\feat{pro}' values cannot unify.}  

The OM {\it w\'{a}-} introduces the [\feat{pred} `\feat{pro}'] into the object field of the f-structure of this sentence; the word \textit{alenje} `hunters' introduces its own \feat{pred} feature with value `\feat{hunters}', so it cannot be the true object, and instead is assumed to be in a topic position.  \citet{bresnan+mchombo:1987} note that the OM can be omitted from the sentence, in which case the phrasal object (here, \textit{alenje}) is fixed in the immediately post-verbal position, while that phrase can alternatively be preposed when the OM appears.  This is explained by assuming that the post-verbal position is an \feat{obj} position, while the adjoined \feat{topic} position is more flexible.  

The subject \textit{nj\^{u}chi} can be omitted from (\ref{bees}), yielding a grammatical sentence meaning `They (some plural entity named with a class 10 noun) bit them, the hunters.'  The optional \feat{pred} feature equation in (\ref{affixes}b) captures this pro-drop property: when the equation appears then a phrase such as \textit{nj\^{u}chi} cannot appear in the subject position, since this would lead to a clash of \feat{pred} values 
(`\feat{pro}' versus `\feat{bees}'); but when the equation is not selected then  \textit{nj\^{u}chi} must appear in the subject position in order for the f-structure to be complete.  

The diachronic process in which a pronominal affix is reanalyzed as agreement has been modeled in LFG as the historic loss of the \feat{pred} feature, along with the retention of the pronoun's person, number, and gender features \citep{coppock+wechsler:2010}.  The anaphoric agreement of the older pronoun with its antecedent then becomes reanalyzed as grammatical agreement of the inflected verb with an external nominal.  Finer transition states can also be modeled in terms of selective feature loss.  Clitic doubling can be modeled as optional loss of the \feat{pred} feature with retention of some semantic vestiges of the pronominal, such as specificity of reference.

% (SUNER CITE).  
%Selective loss of a phi feature leads to paradigm leveling.   Coppock and Wechsler hypothesize that Hungarian `definite object agreement' resulted from the radical loss of all phi features, leaving only  vestigial definiteness.  

The LFG analysis of  affixal pronouns and agreement inflections can be approximated in HPSG by associating the former but not the latter with pronominal semantics in the \textsc{content} field.  However, because HPSG lacks output filters like the LFG Principle of Completeness, it is more difficult for the HPSG framework to directly capture the complementarity between the appearance of a pronominal affix and an analytic phrase filling the same grammatical relation. 
                  
\section{Lexical mapping}
LFG and HPSG both adopt \textit{lexical approaches to argument structure} in the sense of \citet{MWArgSt}: a verb or other predicator is equipped with a valence structure indicating the grammatical expression of its semantic arguments as syntactic dependents.  Both frameworks have complex systems for mapping semantic arguments to syntactic dependents that are designed to capture prevailing semantic regularities within a language and across languages.   The respective systems differ greatly in their notation and formal properties but it is unclear whether 
there are any theoretically interesting differences, such as types of analysis that are available in one but not the other.  This section identifies some of the most important  analogues across the two systems, namely LFG's Lexical Mapping Theory \citep[Chapter 14]{BATW2015a} and the theory of macro-roles proposed by Davis and Koenig for HPSG (see \crossrefchapteralt{arg-st}).\footnote{A recent alternative to LMT based on Glue Semantics has been developed by \citet{asudeh;giorgolo-lfg12} and \citet{asudeh;ea14-lfg}. It preserves the monotonicity of LMT, discussed below, but uses Glue Semantics to model argument realization and valence alternations.}
 
In Lexical Mapping Theory, the argument structure is a list of a verb's argument slots, each labeled with a thematic role type such as Agent, Instrument, Recipient, Patient, Location, and so on, in the tradition of Charles Fillmore's \textit{Deep Cases} \citep{Fillmore68,Fillmore77} and P\={a}\d{n}ini's \textit{k\={a}rakas} \citep{kiparsky+staal:1969}.  The ordering is determined by a thematic hierarchy that reflects priority for subject selection.\footnote{The particular ordering proposed in \cite{BresnanK89a-u} and 
\cite{Bresnan+etal:2015} is the following:  
{\it agent $\succ$ beneficiary $\succ$
experiencer/goal $\succ$ instrument $\succ$ patient/theme $\succ$
locative}}  The thematic role type influences a further classification by the features $[\pm o]$ and $[\pm r]$ that conditions the mapping to syntactic functions (this version is from \citet[p. \ 331]{Bresnan+etal:2015}):

\begin{exe}
\ex\label{class}{\bf Semantic classification of argument structure roles for
    function}: 

  \begin{tabular}[t]{@{}lcc}
        patientlike roles: &&    \begin{tabular}[t]{c}
                                         $\theta$ \\[-.5ex]
                                          $[-r]$
                                 \end{tabular}  \\
                           && \\
        secondary patientlike roles:  && \begin{tabular}[t]{c}
                                          $\theta$\\[-.5ex]
                                          $[+o]$
                                 \end{tabular}\\ 
                           && \\
        other semantic roles: &&          \begin{tabular}[t]{c}
                                          $\theta$\\[-.5ex]
                                          $[-o]$
                                 \end{tabular}
  \end{tabular} 
  \end{exe} 
  
\noindent  
The features $[\pm r]$ (thematically \textit{restricted}) and $[\pm o]$ (\textit{objective}) cross-classify grammatical functions: subject is $[-r, -o]$, object is $[-r, +o]$, obliques are $[+r, -o]$ and
restricted objects are $[+r, +o]$.   A monotonic derivation (where feature values cannot be changed) starts from the argument list with the Intrinsic Classification (\textit{I.C.} in example  (\ref{yam}) below), then morpholexical operations such as passivization can suppress a role (not shown), then the thematically highest role (such as the Agent), if $[-o]$, is selected as Subject (\textit{Sbj.} in example (\ref{yam})) and then any remaining features receive positive values by default (\textit{Def.} in example  (\ref{yam})).  

 \begin{exe}
\ex\label{yam}{Derivation of \textit{eat} as in \textit{Pam ate a yam}.\\
\begin{tabular}[t]{@{}lllccll}
a-structure: &{\it eat}& $<$& $ag$ & $th$   & $>$ & \\
             & I.C.      &    & $[-o]$ & $[-r]$   &   & \\
             &  Sbj.     &    & $[-r]$ &            &              & \\
             &  Def.     &    &     & $[+o]$   &   & \\
             &       &    &\vline    & \vline &    & \\
f-structure: &       &    &{\sc subj} &{\sc obj}  &   &
\end{tabular}
  }
\end{exe}

\noindent
In the macro-role theory formulated for HPSG, the analogues of $[-o]$ and $[-r]$ are the macro-roles `Actor' (\feat{act}) and `Undergoer' (\feat{und}), respectively.  The names of these features reflect large general groupings of semantic role types, but there is not a unique semantic entailment such as `agency' or `affectedness' associated with each of them.  `Actor' (\feat{act}) and `Undergoer' (\feat{und}), name whatever semantic roles map to the subject and object, respectively, of a transitive verb.\footnote{Note, for example, that within this system the `Undergoer' argument of the English verb \textit{undergo}, as in \textit{John underwent an operation}, is the object--- and not the subject, as one might expect if being an `Undergoer' involved actually undergoing something.}  On the semantic side they are disjunctively defined: \textit{x} is the Actor and \textit{y} is the Undergoer iff `\textit{x} causes a change in \textit{y}, or \textit{x} has a notion of \textit{y}, or \ldots ' (quoted from \crossrefchapteralt{arg-st}, example (\ref{def-act-und-rel})).  Such disjunctive definitions are the HPSG analogues of the Lexical Mapping Theory `semantic classifications' shown in (\ref{class}) above.   In the HPSG macro-role system, linking constraints dictate that the \feat{act} argument maps to the first element of \feat{arg-st}, and that the \feat{und} argument maps to some nominal element of \feat{arg-st} (\ref{act}) and (\ref{und}) are from \crossrefchapteralt{arg-st}, examples (\ref{act-vb-linking}) and (\ref{und-vb-linking})):

\begin{exe}
	\ex\label{act}
	{\avmoptions{center}
	\begin{avm}
		\[content$|$key & \[act & \@1 \] \\
		arg-st & \<\textsc{np}$_{\@1}$,  \ldots \>
		\]
	\end{avm}
	}
\end{exe}

\begin{exe}
	\ex\label{und}
	{\avmoptions{center}
	\begin{avm}
		\[content$|$key & \[und & \@2 \] \\
		arg-st & \<\ldots, \textsc{np}$_{\@2}$,  \ldots \>
		\]
	\end{avm}
	}
\end{exe}
The first element of \feat{arg-st} maps to the subject of an active voice verb, so (\ref{act})--(\ref{und}) imply that the subject is the \feat{act} if there is one, and otherwise it is the \feat{und}.  Similarly, in Lexical Mapping Theory as described above, the subject is the $[-o]$ highest argument, if there is one, and otherwise it is the $[-r]$ argument.  In this simple example we can see how the two systems accomplish exactly the same thing.  A careful examination of more complex examples might point up theoretical differences, but it seems more likely that the two systems can express virtually the same set of mappings.  

In LFG the \textit{argument structure} (or \textit{a-structure}) contains the predicator and its argument roles classified and ordered by thematic role type and further classified by  Intrinsic Classification.  It is considered a distinct level of representation, along with c-structure and f-structure.  As a consequence the grammar can make reference to the initial item in a-structure, such as the agent (\textit{ag}) in (\ref{yam}), which is considered the `most prominent' role and often called the \textit{a-subject} (`argument structure subject') in LFG parlance.  To derive the passive voice mapping, the a-subject is suppressed in a morpholexical operation that crucially takes place before the subject is selected:  

 \begin{exe}
\ex\label{yam2}{Derivation of \textit{eaten} as in \textit{A yam was eaten (by Pam)}.\\
\begin{tabular}[t]{@{}lllccll}
a-structure: &{\it eat}& $<$& $ag$ & $th$   & $>$ & \\
             & I.C.      &    & $[-o]$ & $[-r]$   &   & \\
             & passive      &    & \O  &    &   & \\
             &  Sbj.     &    &  &       $[-o]$     &              & \\
  %           &  Def.     &    &     & $[+o]$   &   & \\
             &       &    &    & \vline &    & \\
f-structure: &       &    & &{\sc subj} &   &
\end{tabular}
  }
\end{exe}
(The optional \textit{by}-phrase is considered to be an adjunct referring to the passivized a-subject.)  Note that the passive alternation is \textit{not} captured by a procedural rule that replaces one grammatical relation (such as \textsc{obj}) with another (such as \textsc{subj}).   The mapping from word strings to f-structures in LFG is monotonic, in the sense that information cannot be destroyed or changed.  As a result the mapping between internal and external structures is said to be transparent in the sense that the grammatical relations of parts of the sentence are preserved in the whole (for discussion of this point, see \citet[Chapter 5]{BATW2015a}).  In early versions of LFG, monotonicity was assumed for the syntax proper, while destructive procedures were permitted in the lexicon.  This was canonized in the \textit{Principle of Direct Syntactic Encoding}, according to which all grammatical relation changes are lexical \citep{Bresnan82a-ed}.  At that time an LFG passive rule operated on fully specified predicate argument structures, replacing \textsc{obj} with \textsc{subj}, and \textsc{subj} with an \textsc{obl$_{by}$} or an existentially bound variable.  The advent of LMT brought monotonicity to the lexicon as well.  


\section{Long distance dependencies}
In LFG a long distance dependency is modeled as a reentrancy in the f-structure, a structure lacking from HPSG.  The HPSG theory of long distance dependencies is based on that of GPSG and uses the percolation of a `slash' feature through the constituent structure.  But the two frameworks are essentially very similar, both working by decomposing a long distance dependency into a series of local dependencies.  As we will see, there are nevertheless some minor differences with respect to what hypothetical extraction patterns can be expressed.  

Both frameworks allow either a `gap' or `gapless' account:  regarding LFG see \citet{BATW2015a} for gap and \citet{dalrymple;ea19} for gapless; regarding HPSG see \citet{ps2} for gap and \citet{SWB2003a} for gapless.  Gaps have been motivated by the (controversial) claim that the linear position of an `empty category' matters for the purpose of weak crossover and other binding phenomena 
\citep[Chapter 9]{BATW2015a}.  In this section I compare gapless accounts.

LFG has two grammaticalized discourse functions, \feat{top} (`topic') and \feat{foc} (`focus').  A sentence with a left-adjoined topic position is depicted in Figure~\ref{fig-tree5}.  The topic phrase \textit{Ann} serves as the object of the verb \textit{like} within the clausal complement of \textit{think}.  This dependency is encoded in the second equation annotating the topic node, where the variable \textit{x} ranges over strings of attributes representing grammatical functions such as \textsc{subj}, \textsc{obj}, \textsc{obl}, or \textsc{comp}.  These strings describe paths through the f-structure.   In this example \textit{x} is resolved to the string \textsc{comp obj}, so this equation has the effect of adding to the f-structure in (\ref{ann}) the curved line representing token identity.  

\begin{figure}
\begin{forest}
sm edges without translation
[IP 
    [DP \\{(\up \feat{top}) $=$ \down}\\
                         {(\up \feat{top}) $=$ (\up x)}
    [Ann] ]
    [IP                      
    		[DP [D [I]]]
    		[VP [V [think]]
    			[IP
    				[DP [D [he]]]
    				[VP [V [likes]]] ] ] ] ] 
\end{forest}
\caption{Add cpation}\label{fig-tree5}
\end{figure}
		
\ea
\label{ann} 
{\avmoptions{center}
\begin{avm}
\[ top & \rnode{top}{\[ ``Ann'' \]} \\
subj &  \[ ``I'' \] \\
pred &  \predvall{think $\leftangle (f\, \feat{subj}) (f\, \feat{comp}) \rightangle$}\\
comp & \[  subj & \[ ``he'' \] \\
				 pred & \predvall{like $\leftangle (g\, \feat{subj})(g\, \feat{obj}) \rightangle$}\\
				 obj  &   \rnode{obj}{}
				 \]
 \]
\end{avm}
}
\z
% ASH-HELP:  needs curvy line from TOP to OBJ
\nccurve[ncurv=4.6,angleA=0,angleB=0,linewidth=.5pt]{top}{obj}

\noindent
HPSG accounts are broadly similar.  One HPSG version relaxes the requirement that the arguments specified in the lexical entry of a verb or other predicator must all appear in its \textsc{valence} lists.   Verbal arguments are represented by elements of the \textsc{arg-st} list, so the list for the verb \textit{like} contains two DPs, one each for the subject and object.  In a sentence with no extraction, those \textsc{arg-st} list items map to the \textsc{valence} lists, the  first item appearing in \textsc{subj} and any remaining ones in \textsc{comps}.  To allow for extraction, one of those \textsc{arg-st} list items is permitted to appear on the \textsc{slash} list instead.  The \textsc{slash} list item is then passed up the tree by means of strictly local constraints, until it is bound by the topicalized phrase.   

The LFG dependency is expressed in the f-structure, not the c-structure.  \citet[Chapter 2]{BATW2015a} note that this allows for category mismatches between the phrases serving as filler and those in the canonical, unextracted position.  This is illustrated in example (\ref{sick}) above.  The lexical entry for \textit{talk (about)} in (\ref{talk-about}) selects an \textsc{obl}$_{about}$ \textsc{obj} but does not specify the part of speech category of that argument; meanwhile, the phrase structure rules dictate that the top position allows (at least) DP and CP, while the position right adjacent to \textit{about} can only house a DP.   Category mismatches pose a problem for transformational theories that assume the Projection Principle, since the `moved' constituent should satisfy the conditions imposed on the phrase in its source position.  But HPSG seems to permit essentially the same analysis as the LFG analysis just sketched.   In HPSG, the preposition \textit{about} would have a disjunctive DP/CP on its \textsc{arg-st} list item, but only DP is selected for its \textsc{comps} list item; and the topic position would allow either DP or CP.  

Constraints on extraction such as accessibility conditions and island constraints  can be captured in LFG by placing constraints on the attribute string \textit{x} \citep{dalrymple;ea19}.  If subjects are not accessible to extraction then we stipulate that \textsc{subj} cannot be the final attribute in \textit{x}; if subjects are islands then we stipulate that \textsc{subj} cannot be a non-final attribute in \textit{x}.  If the f-structure is the only place such constraints are stated then 
this makes the interesting (but unfortunately false; see presently) prediction that the theory of extraction cannot distinguish between constituents that map to the same f-structure.  For example, as noted in Section \ref{mobile-sec} function words like determiners and their contentful phrases like NP are usually assumed to be f-structure co-heads, so the DP \textit{the lion} maps to the same f-structure as its NP daughter \textit{lion} (see diagram Figure~\ref{fig-tree1}.  This predicts that if the DP can be extracted then so can the NP, but of course that is not true:

\begin{exe} 
\ex	\label{nope}
\begin{xlist}
\ex[ ]{The lion, I think she saw.}
\ex[*]{Lion, I think she saw the.}
\end{xlist}
\end{exe}
These two sentences have exactly the same f-structures, so any explanation for the contrast in acceptability must involve some other level.  For example, one could posit that the phrase structure rules can introduce obligatory daughters (see \cite[239]{snijders15}), contra the assumptions of \citet{BATW2015a}.  

%extractions would involve the same attribute path, namely \textsc{compl obj}.  

%In fact LFG theory does not assume that path constraints exhaust the possibilities for expressing extraction conditions.   The `manner of speaking' verbs in \ref{offpath}a
%
%\begin{exe} 
%\ex	\label{manner}
%Who did Chris think/*whisper that David saw?
%\end{exe}
%\citet{Dalrymple2001a-u} notes that ``There is no reason to assume that the grammatical function of the sentential complements of these two verbs differs'' and instead proposes that  verbs place a boolean feature 
%$[\textsc{ldd} \pm]$ on their clausal complements; nonbridge verbs like whisper assigning $[\textsc{ldd} -]$ and other verbs assigning $[\textsc{ldd} +]$.  Then the extraction path is then subject an \textit{off-path constraint} stating that any \textsc{compl} in the path cannot contain the feature $[\textsc{ldd} +]$.  


\section{Control and raising}

Raising and control (equi) words are treated in virtually the same way in LFG and HPSG: a subject control word (such as \textit{hope} in (\ref{hope})) specifies that its subject is (also) the subject of its predicate complement.  

\begin{exe} 
\ex	\label{hope}
Pam hopes to visit Fred.
\begin{xlist} 
\ex	
\word{hope}: \qquad \feqs{(\up \textsc{pred}) $=$ `hope$\leftangle(\up \textsc{subj}) (\up \textsc{xcomp}) \rightangle$'\\
(\up \textsc{subj})  $=$  (\up \textsc{xcomp} \textsc{subj})}
\ex 
\word{hope}:  \qquad  $[$ \textsc{arg-st} $\langle$ \ibox{1}\textsc{np}, \textsc{vp}[\textit{inf}; \textsc{subj} $\langle$ \ibox{1} $ \rangle ] \rangle ]$
\end{xlist}
\end{exe}
The LFG entry for \textit{hope} in (\ref{hope}a) contains the grammatical function \textsc{xcomp} (`open complement'), the function reserved for predicate complements such as the infinitival phrase \textit{to visit Fred}.  The control equation specifies that its subject is equivalent to the subject of the verb \textit{hope}; the  tag \ibox{1} plays the same role in the simplified HPSG entry in (\ref{hope}b).  

The f-structure for (\ref{hope}) is shown here:

\begin{exe}
\ex \label{hopefs} 
{\avmoptions{center}
\begin{avm}
\[ subj & \rnode{top}{\[ ``Pam'' \]} \\
pred &  \predvall{hope $\leftangle (f\, \feat{subj}) (f\, \feat{xcomp}) \rightangle$}\\
xcomp & \[  pred & \predvall{visit $\leftangle (g\, \feat{subj})(g\, \feat{obj}) \rightangle$}\\
				 subj  &   \rnode{obj}{} \\
				 obj & \[ ``Fred'' \]
				 \]
 \]
\end{avm}
}
\end{exe}
% ASH-HELP:  needs curvy line from TOP to OBJ
\nccurve[ncurv=8,angleA=0,angleB=0,linewidth=.5pt]{top}{obj}

\noindent
One interesting difference between the two frameworks is that the HPSG representation allows only for control (or raising) of subjects and not complements.  More precisely, it allows for control of the outermost or final dependent to be combined with the verbal projection.  This is because of the list cancellation regime that operates with valence lists.  The expression `VP' in  (\ref{hope}b) represents an item with an empty \textsc{comps} list.  In a simple English clause the verb combines with its complement phrases to form a VP constituent, which which the subject is then combined to form a clause.  Assuming the same order of combination in the control structure, it is not possible to control the complement of a structure that contains a structural subject, as in (\ref{hopeless}a):

\begin{exe} 
\ex 
\label{hopeless}
\begin{xlist}
\ex[*]	{Fred hopes Pam to visit.}  
\ex[] {Fred hopes to be visited by Pam.}  
\ex[]  {?! (\up \textsc{subj})  $=$  (\up \textsc{xcomp} \textsc{obj})}
\end{xlist}
\end{exe}
The intended meaning would be that of (\ref{hopeless}b).  The passive voice is needed in order to make the intended controllee (\textit{Fred})  the subject of \textit{visit} and thus available to be controlled.  This restriction of controllees to subjects follows from the HPSG theory, while in LFG it follows only if control equations  
 like the one in (\ref{hopeless}c) are systematically excluded.    

This raises the question of whether the restriction to subject controllees is universal.  In fact it appears that some languages allow the control of non-subjects, but it is still unclear whether such languages justify equations such as (\ref{hopeless}c).  For example,   \citet{kroeger:1993} shows that Tagalog has two types of control relation.  In the more specialized type, which occurs only with a small set of verbs or in a special construction in which the downstairs verb appears in non-volitive mood, both the controller and controllee must be subjects.  
Kroeger analyzes this type as functional control using a control equation like the one in (\ref{hope}a).  In the more common type of Tagalog control, the controllee must be the Actor argument, while the grammatical relations of controllee and controller are not restricted.  (Tagalog has a rich voice system, often called a focus marking system, regulating which argument of a verb is selected as its subject.)  
This latter type of Tagalog control is defined on argument structure (Actors, etc.), so a-structure rather than f-structure is appropriate for representing the control relations.    

Instead of functional control, Kroeger analyzes this latter type of control within LFG as \textit{anaphoric control}, in which the controllee has the status of a null pronoun whose antecedent is the controller.  In anaphoric control the f-structures of the controller and controllee are not identified, and instead each has its own \feat{pred} feature.  An anaphoric control analysis of (\ref{hope}) would involve a structure resembling (\ref{hopefs}) except that the curved line would be eliminated and the \feat{subj} of \textit{visit} would contain the feature [\feat{pred} \feat{`pro'}].  In the HPSG analogue of anaphoric control, an element of a \textsc{valence} list represents a null pronoun in the following sense:  the rules mapping the \textsc{valence} to the phrase structure do not project such an element; and it is accorded the semantic and syntactic properties of a pronoun, such as definiteness and/or specificity, and binding features.  


\section{Semantics}

%\subsection{Background: Semantics for LFG and HPSG}

HPSG  was conceived from the
start as  a theory of the \textit{sign} \citep{Saussure16a-Fr}, 
wherein each constituent is a pairing of form and meaning.  
So semantic representation and composition  was built into HPSG (and the subsequent sister framework of Sign-Based Construction Grammar \citealt{BS2012a-ed}), as reflected in the title of the first HPSG
book \citep{pollard;sag87}, \textit{Information-Based Syntax and
  Semantics}.  LFG was not founded as a theory that included semantics, but a semantic component was developed for LFG shortly thereafter \citep{halvorsen83}.  The direction of semantics for LFG changed some ten years later and the dominant tradition is now Glue Semantics \citep{dalrymple;ea93,dalrymple99,Dalrymple2001a-u,asudeh-lpr,dalrymple;ea19}.  

This section presents a basic introduction to Glue Semantics. 
%As this is a handbook on HPSG and this is primarily a chapter on LFG, but Glue Semantics is in fact a general theory of semantic composition that is not necessarily tied to LFG, 
Our goal is to present enough of the approach for readers to grasp the main intuitions behind it, without presupposing much knowledge of formal semantic theory.  The references listed at the end of the previous paragraph (especially \citet{dalrymple;ea19}) are good places to find additional discussion and references.  
The rest of this section is organized as follows. In Section~\ref{sec:background} we present some more historical background on semantics for LFG and HPSG.  In Section~\ref{sec:glue-semantics}, we present Glue Semantics (\glue), as a general compositional system in its own right. Then, in Section~\ref{sec:glue-lfg} we look at the syntax--semantics interface with specific reference to an LFG syntax. For further details on semantic composition and the syntax--semantics interface in constraint-based theories of syntax, see \crossrefchapterw{semantics} for semantics for HPSG and \citet{asudeh-lfg-glue} for semantics for LFG.

\subsection{Brief history of semantics for LFG and HPSG}
\label{sec:background}

%In developing a system of compositional semantics (i.e., rule-based
%interpretation) for a syntactic framework, we are really engaged in
%two tasks. First, we are engaged in a specification of the
%syntax--semantics interface: How are primitives of the syntactic
%framework mapped to primitives of the semantic framework and what are
%the compositional reflexes of syntactic combinatorics? In other words,
%what are the minimal semantic parts, how are they related to syntactic
%parts, and how are syntactic rules interpreted such that combinations
%of the parts yield interpretations? Second, we are engaged in a specification
%of the semantic representation: How are meaningful parts formally
%represented and how is their composition formally captured? For example, do
%we have a classic static semantics \citep{montague70,montague73} or do
%we have a dynamic semantics \citep{kamp81,heim82}? Is the semantics
%extensional or intensional and, if the latter, how is intensionality
%captured? What are the ontological and metaphysical assumptions about
%the semantic representation?
%
%
%
%If we are approaching these questions from the direction of a syntactic theory and its attendant syntactic framework, the first question, to do with the syntax--semantics interface, tends to be primary. However, it is nevertheless important to bear in mind the second question, to do with semantic representation. Otherwise, the range of semantic approaches for a given syntactic theory might seem a bit bewildering. For example, 

Various theories of semantic representation have been adopted by the different non-transformational syntactic frameworks over the years.  
The precursor to HPSG, GPSG, was paired by its designers with a then fairly standard static Montogovian semantics \citep{GKPS85a}, but GPSG itself was subsequently adopted as the syntactic framework for Discourse Representation Theory \citep{kamp;reyle93}, a dynamic theory of semantics.  Initial work on semantics for LFG also assumed a Montogovian semantics \citep{halvorsen83,halvorsen;kaplan88}.  But with the increasing interest in Situation Semantics \citep{barwise;perry83} in the 1980s at Stanford University and environs (particularly SRI International and Xerox PARC), the sites of the foundational work on both HPSG and LFG, both frameworks also became associated with Situation Semantics.  In the case of LFG, this association was not foundational, manifesting primarily in the form of \citet{fenstad;ea87}, and soon waned. In contrast, as noted above, HPSG is a theory of signs and therefore had semantics incorporated into the theory from the outset.  Moreover, the chosen semantic theory was Situation Semantics, and this also carried over into the second main HPSG book \citep{ps2} and further \citep{ginzburg;sag00}.

Beginning in the 90s, the focus subsequently shifted in new directions due to a new interest in computationally tractable
theories of the syntax--semantics interface, to support efforts at
large-scale grammar development, such as the ParGram project for LFG
\citep{butt;ea99,butt;ea02-pargram} and the LinGO/Grammar Matrix projects for HPSG
\citep{flickinger00,bender;ea02,bender;ea10}.\footnote{Readers can
  explore the current incarnations of these projects at the following
links (checked January 13, 2020):
\begin{description}
\item[ParGram] \url{https://pargram.w.uib.no}
\item[LinGO] \url{http://lingo.stanford.edu}
\item[Grammar Matrix] \url{http://matrix.ling.washington.edu/index.html}
\end{description}
} This naturally led to an
interest in underspecified semantic representations, so that semantic
ambiguities such as scope ambiguity could be compactly encoded without
the need for full enumeration of all scope possibilities. Two examples
for HPSG are \term{Lexical Resource Semantics} \citep{richter04,penn;richter04}
and \term{Minimal Recursion Semantics}
\citep{copestake;ea05}. Similarly, focus in semantics for LFG shifted
to ways of encoding semantic ambiguity compactly and efficiently. This
led to the development of Glue Semantics.



\subsection{General Glue Semantics}
\label{sec:glue-semantics}

In this section, we briefly review Glue Semantics itself, 
without reference to a particular
syntactic framework.  Glue Semantics is a general framework for
semantic composition that requires \term{some} independent syntactic
framework but does not presuppose anything about syntax except
headedness, which is an uncontroversial assumption across
frameworks. This makes the system flexible and adaptable, and it has
been paired not just with LFG, but also with Lexicalized
Tree-Adjoining Grammar \citep{frank;genabith01}, HPSG
\citep{asudeh;crouch01-hpsg-glue}, Minimalism \citep{Gotham2018}, and
Universal Dependencies \citep{gotham;haug18}.

In Glue Semantics, meaningful
linguistic expressions --- including lexical items but also possibly
particular syntactic configurations --- are associated
with \term{meaning constructors} of the following form:\footnote{It is
in principle possible for a linguistic expression to have a phonology
and syntax but not contribute to interpretation, such as expletive
pronouns like \word{there} and \word{it} or the \word{do}-support
auxiliary in English; see \citet[113]{asudeh-lpr} for some discussion of
expletive pronouns in the context of \glue.}
\begin{exe}
\ex \formula{\mathcal{M}:G}
\end{exe}
%
\formula{\mathcal{M}} is an expression from a \term{meaning language}
which can be anything that supports the lambda calculus; G is an
expression of \term{linear logic} \citep{girard87}, which 
specifies the semantic composition (it ``glues meanings
together''), based on a syntactic parse.  By convention a colon separates them. Glue Semantics is related to (Type-Logical) Categorial
Grammar \citep{carpenter97,morrill94a,morrill11,moortgat97}, but it
assumes a separate syntactic representation for handling word order,
so the terms of the linear logic specify just semantic
composition without regard to word order 
(see \citealt{asudeh-lpr} for further discussion). Glue Semantics is
therefore useful in helping us focus on semantic composition in its
own right.

The principal compositional rules for Glue Semantics are those for the
linear implication connective, $\multimap$, which are here presented in a
natural deduction format:
%
%\begin{exe}
%\ex \label{ex:implE} 
%{\textbf{Functional application : Implication 
%        elimination}} \\ \textbf{(modus ponens)}  \\[2ex] 
%          \begin{tabular}[t]{c}
%    \begin{lfgprooftree}
%      \formula{f:A \linimp B} \hspace*{2em} \formula{a:A}
%      \justifies
%      \formula{f(a):B} \using \linimpE
%    \end{lfgprooftree}
%  \end{tabular}
%  
%  \bigskip
%
%\ex \label{ex:implI} 
%   \textbf{Functional abstraction : Implication 
%        introduction} \\ \textbf{(hypothetical reasoning)}} \\[2ex] 
%          \begin{tabular}[t]{c}
%    \begin{lfgprooftree}
%      \[\formula{[a:A]}^1 
%      \leadsto
%      \formula{f:B}\]
%      \justifies
%      \formula{\lambda a.f:A \linimp B} \using \linimpIi{1}
%    \end{lfgprooftree}
%  \end{tabular} 
%\end{exe}
  
\begin{exe}
% % \ex \label{ex:implE} 
% %   \begin{tabular}[t]{c}
% %     \multicolumn{1}{l}{\textbf{Functional application : Implication 
% %         elimination}} \\ 
% %     \multicolumn{1}{l}{\textbf{(modus ponens)}}  \\[2ex] 
% %     \begin{lfgprooftree}
% %       \formula{f:A \linimp B} \hspace*{2em} \formula{a:A}
% %       \justifies
% %       \formula{f(a):B} \using \linimpE
% %     \end{lfgprooftree}
% %   \end{tabular}

\ex \label{ex:implE}  Functional application : Implication elimination (modus ponens)\medskip\\ 
    \begin{prooftree}
      \hypo{f:A \multimap B} \hypo{a:A}
      \infer2[$\multimap_{\mathcal{E}}$]{f(a):B}
    \end{prooftree}\medskip\\
  
\ex \label{ex:implI} 
    Functional abstraction : Implication introduction (hypothetical reasoning)\medskip\\ 
    \begin{prooftree}
      \hypo{[a:A]\textsuperscript{1}} 
      \ellipsis{}{f:B}
      \infer1[$\multimap_{\mathcal{I}, 1}$]{\lambda a.f:A \multimap B}
    \end{prooftree}
\end{exe}

\noindent
In each of these rules, the inference over the linear logic term,
\formula{G}, corresponds to an operation on the meaning term, via the
Curry-Howard Isomorphism between formulas and types
\citep{curry;feys58,curry;feys95,howard80}\nocite{degroote95a}. The rule for eliminating the
linear implication, which is just modus ponens, corresponds to
functional application. The rule for introducing the linear
implication, i.e. hypothetical reasoning, corresponds to functional
abstraction. These rules will be seen in action shortly. 

In general, given some head \formula{h} and some arguments of the head
\formula{a_1, \ldots, a_n}, an implicational term like the following
models consumption of the arguments to yield the saturated meaning of
the head: \formula{a_1 \linimp \ldots \linimp a_n \linimp h}. For
example, let us assume the following meaning constructor for the verb
\word{likes} in the sentence \word{Max likes Sam}:
%
\begin{exe}
  
\ex \label{ex:semantics-1} \formula{\lambda
  y\lambda x.\func{like}(y)(x):s \linimp m \linimp l}
\end{exe}
%
Let's also assume that \formula{s} is mnemonic for the semantic
correspondent of the (single word) phrase \word{Sam}, \formula{m}
similarly mnemonic for \word{Max}, and \formula{l} for
\word{likes}. In other words, the meaning constructor for \word{likes}
would be associated with the lexical entry for the verb and specified
in some general form such that it can be instantiated by the syntax
(we will see an LFG example shortly); here we are assuming that the
instantiation has given us the meaning constructor in
(\ref{ex:semantics-1}).

Given this separate level of syntax, the glue logic does not
have to worry about word order and is permitted to be commutative
(unlike the logic of Categorial Grammar). We could therefore freely
reorder the arguments for \word{likes} above such that we instead
first compose with the subject and then the object, but still yield
the meaning appropriate for the intended sentence \word{Max likes Sam}
(rather than for \word{Sam likes Max}):
%
\begin{exe}
  
\ex \label{ex:semantics-2}
  \formula{\lambda
  x\lambda y.\func{like}(y)(x):m \linimp s \linimp l}
\end{exe}
%
As we will see below, the commutativity of the glue logic yields a
simple and elegant treatment of quantifiers in non-subject positions,
which are challenging for other frameworks \citep[see, for example, the
careful pedagogical presentation of the issue
in][244--263]{jacobson14}. 

First, though, let us see how this argument reordering, otherwise
known as Currying or Schönfinkelization, works in a
proof, which also demonstrates the rules of implication elimination
and introduction:
%
\begin{exe}
  
\ex \label{ex:semantics-3}
  \attop{\begin{prooftree}
    \hypo{\lambda y.\lambda x.f(y)(x):a \linimp b \linimp c}     \hypo{[v:a]\textsuperscript{1}}
    \infer2[$\multimap_{\mathcal{E}}$]{\lambda x.f(v)(x):b \multimap c} 
    \hypo{[u:b]\textsuperscript{2}}
    \infer2[$\multimap_{\mathcal{E}}$]{f(v)(u):c}
    \infer1[$\multimap_{\mathcal{I},1}$]{\lambda v.f(v)(u):a \multimap c}    
    \infer1[$\Rightarrow_\alpha$]{\lambda y.f(y)(u):a \multimap c}    
    \infer1[$\multimap_{\mathcal{I},2}$]{\lambda u.\lambda y.f(y)(u):b \multimap a \multimap c}
  \infer1[$\Rightarrow_\alpha$]{\lambda x.\lambda y.f(y)(x):b \multimap a \multimap c}
  \end{prooftree}\medskip\\}
\end{exe}

\noindent
The general structure of the proof is as follows. First, an assumption (hypothesis)
 is formed for each argument, in the order in which they
originally occur, corresponding to a variable in the meaning
language. Each assumed argument is then allowed to combine with the
implicational term by implication elimination. Once the implicational
term has been entirely reduced, the assumptions are then discharged in
the same order that they were made, through iterations of implication
introduction. The result is the original term in curried form, such
that the order of arguments has been reversed but without any change
in meaning. The two steps of $\alpha$-equivalence, notated
$\Rightarrow_\alpha$, are of course  not strictly
necessary, but have been added for exposition. 

This presentation has been purposefully abstract to highlight what is
intrinsic to the glue logic, but we of course need to see how this
 works with a syntactic framework to see how Glue Semantics
actually handles semantic composition and the syntax--semantics interface. So next, in
Section~\ref{sec:glue-lfg}, we will review \lfgglue, as this
is the predominant pairing of syntactic framework and \glue.
% Then, in
% section Section~\ref{sec:glue-hpsg} we briefly review \hpsgglue, following
% \citet{asudeh;crouch01-hpsg-glue}.

\subsection{Glue Semantics for LFG}
\label{sec:glue-lfg}

\glue\ for LFG will be demonstrated by analyses of the following three
examples:

\begin{exe}
\ex \label{ex:bca} Blake called Alex.
\ex \label{ex:bce} Blake called everybody.
\ex \label{ex:ecs} Everybody called somebody.
\end{exe}
%
Example (\ref{ex:bca}) is a simple case of a transitive verb with two
proper name arguments, but is sufficient to demonstrate the basics of
the syntax--semantics interface in \lfgglue. Example (\ref{ex:bce}) is a
case of a quantifier in object position, which is challenging to
compositionality because there is a type clash between the simplest
type we can assign to the verb, \bracket{e,\bracket{e,t}}, and the
simplest type that would be assigned to the  quantifier,
\bracket{\bracket{e,t},t}. In other theories, this necessitates either
a syntactic operation which is syntactically undermotivated, e.g.\  
Quantifier Raising in interpretive theories of composition, such as Logical Form semantics \citep{heim;kratzer98},
or a type shifting operation of some kind in directly compositional
approaches, as in categorial or type-logical frameworks; see
\citet{jacobson14} for further discussion and references. Example
(\ref{ex:ecs}) also demonstrates this point, but it more importantly
demonstrates that quantifier scope ambiguity can be handled in \glue\
without a) positing an undermotivated syntactic ambiguity and b) while
maintaining the simplest types for both quantifiers.

The relevant aspects of the lexical entries
involved are shown in \tablew~\ref{tab:gen-lex}.  Other syntactic aspects of the lexical items, such as the fact that  
\word{called} has a \feat{subject} and an \feat{object}, are specified
in its meaning constructor. Minimal f-structures are
provided below for each example. The subscript $\sigma$ indicates the
semantic structure that corresponds to the annotated
f-description. The types for the lexical items are the minimal types
that would be expected. Note that in \glue\ these are normally
associated directly with the semantic structures, for example
\Up$_{\sigma{_e}}$ and (\up \feat{obj})$_{\sigma{_e}}$ \linimp\ (\up
\feat{subj})$_{\sigma{_e}}$ \linimp\ \Up$_{\sigma{_t}}$, but they have
been presented separately for better exposition; see
\citet[299--305]{dalrymple;ea19} for further discussion. We do not
show semantic structures here, as they are not necessary for this
simple demonstration. 


The generalized quantifier functions associated with everybody and somebody are, respectively, \func{every} and
\func{some} in the meaning language.  The universal symbol $\forall$ in the glue
logic/linear logic terms for the quantifiers ranges over semantic structures of type $t$.  It is unrelated to the generalized quantifiers.  Hence even the existential word \word{somebody} has the 
universal $\forall$ in its linear logic glue term. 
The $\forall$-terms thus
effectively say that \emph{any} type $t$ semantic structure $S$ that
can be found by application of proof rules such that the quantifier's
semantic structure implies $S$ can serve as the scope of the quantifier;
see \citet[393--394]{asudeh05-lp} for basic discussion of the
interpretation of $\forall$ in linear logic. This will become clearer
when quantifier scope is demonstrated shortly. 

\begin{table}
  \centering
\begin{tabular}{lll}
\lsptoprule
Expression & Type & Meaning Constructor\\\midrule
  \word{Alex} & $e$ & \formula{\func{alex}:\upsig}\\
  \word{Blake} & $e$ & \formula{\func{blake}:\upsig}\\
  \word{called} & \bracket{e,\bracket{e,t}} & \formula{\lambda
                                              y.\lambda x.\func{call}(y)(x):(\up
    \feat{obj})\sig \linimp\ (\up \feat{subj})\sig \linimp\ \upsig}\\
  \word{everybody} & \bracket{\bracket{e,t},t} &
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(\upsig \linimp\ S)
    \linimp\ S}\\
  \word{somebody} & \bracket{\bracket{e,t},t} &
  \formula{\lambda Q.\func{some}(\func{person},Q):\forall S.(\upsig \linimp\ S) \linimp\ S}
  \\\lspbottomrule
\end{tabular}
\caption{Relevant lexical details  for the three examples in (\ref{ex:bca}--\ref{ex:ecs})}
\label{tab:gen-lex}
\end{table}

Let us assume the following f-structure for (\ref{ex:bca}):
\begin{exe}
\ex
\begin{avm}
  \lfgfst{c}[pred & \predvall{call}\\
   subj & \lfgfst{b}[pred & \predvall{Blake}]\\
   obj & \lfgfst{a}[pred & \predvall{Alex}]
  ]
\end{avm}
\end{exe}
%
Note that here, unlike in previous sections, the \feat{pred} value for the verb does not list its subcategorization information. This is because we've made the standard move in much \glue\ work to suppress this information.\footnote{Indeed, one could go further and argue that 
\feat{pred} values do not list subcategorization at all, in which case the move is not just notational, and that the Principles of Completeness and Coherence instead follow from the resource-sensitivity of Glue Semantics; for some
discussion, see
\citet{asudeh-lpr,asudeh;giorgolo-lfg12,asudeh;ea14-lfg}.} % 
% ASH-HELP: I don't understand this last sentence or how it is relevant here.
% STEVE: Hope this is better!
 The
f-structures are named mnemonically by the first character of their
\feat{pred} value. All other f-structural information has been
suppressed for simplicity. Based on these f-structure labels, the
meaning constructors in the lexicon in \tablew~\ref{tab:gen-lex} are
instantiated as follows ($\sigma$ subscripts suppressed):
%
\begin{exe}
\ex \high{Instantiated meaning constructors}
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\func{blake}:b}\\
  \formula{\func{alex}:a}\\
  \formula{\lambda y.\lambda x.\func{call}(y)(x):a \linimp\ b \linimp\ c}
\end{tabular}
\end{exe}
%
These meaning constructors yield the following proof, which is the
only available normal form proof for the
sentence:\footnote{\label{fn:norm-proof} The reader can think
of the normal form proof as the minimal proof that yields the
conclusion, without unnecessary steps of introducing and discharging
assumptions; see \citet{asudeh;crouch02-wccfl-ellipsis} for some
basic discussion.}

\begin{exe}
\ex\high{Proof}\medskip\\
\begin{prooftree}
\hypo{\lambda y.\lambda x.\func{call}(y)(x):a \multimap b \multimap c} 
\hypo{\func{alex}:a}
\infer2[$\multimap_{\mathcal{E}}$]{(\lambda y.\lambda
            x.\func{call}(y)(x))(\func{alex}):b \multimap c} 
\infer1[$\Rightarrow_\beta$]{\lambda x.\func{call}(\func{alex})(x):b \multimap c}
\hypo{\func{blake}:b} 
\infer2[$\multimap_{\mathcal{E}}$]{(\lambda x.\func{call}(\func{alex})(x))(\func{blake}):c}
\infer1[$\Rightarrow_\beta$]{\func{call}(\func{alex})(\func{blake}):c}
\end{prooftree}
\end{exe}

\noindent
The final meaning language expression,
\formula{\func{call}(\func{alex})(\func{blake})}, gives the correct truth
  conditions for \word{Blake called Alex}, based on a standard
  model theory.  

Let us next assume the following f-structure for (\ref{ex:bce}):
\begin{exe}
\ex
\begin{avm}
  \lfgfst{c}[pred & \predvall{call}\\
   subj & \lfgfst{b}[pred & \predvall{Blake}]\\
   obj & \lfgfst{e}[pred & \predvall{everybody}]
  ]
\end{avm}
\end{exe}
%
Based on these f-structure labels, the
meaning constructors in the lexicon are
instantiated as follows ($\sigma$ subscripts again suppressed):
%
\begin{exe}
\ex\high{Instantiated meaning constructors}
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\lambda y.\lambda x.\func{call}(y)(x):e \linimp\ b \linimp\
  c}\\
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(e \linimp\ S)
  \linimp\ S}\\
  \formula{\func{blake}:b}
\end{tabular}
\end{exe}
%
These meaning constructors yield the following proof, which is again 
the only available normal form proof:\footnote{We have not presented
  the proof rule for Universal Instantiation, but it is trivial; see \citet[396]{asudeh-lpr}.}

\begin{exe}
\ex \high{Proof}\medskip\\
\begin{prooftree}[separation=.5em]\footnotesize
\hypo{\parbox[t]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright$\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap S) \multimap S$}}
\infer1[$\forall_{\mathcal{E}}[c/S]$]{\parbox[t]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$(e \multimap c) \multimap c$}}

\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright$\lambda y.\lambda x.\func{call}(y)(x) :$\\$e \multimap b \multimap c$}}
\hypo{[z:e]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(z)(x): b \multimap c}
\hypo{\func{blake}:b}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{call}(z)(\func{blake}):c}
\infer1[$\multimap_{\mathcal{I}, 1}$]{\lambda z.\func{call}(z)(\func{blake}):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{every}(\func{person}, \lambda z.\func{call}(z)(\func{blake})):c}
\end{prooftree}
\end{exe}

\noindent
The final meaning language expression,
\formula{\func{every}(\func{person}, \lambda
  z.\func{call}(z)(\func{blake}))}, again gives the correct truth
  conditions for \word{Blake called everybody}, based on a standard
  model theory with generalized quantifiers. 
  
  Notice that the
  quantifier need not be moved in the syntax --- it's just an
  \feat{object} in f-structure --- and no special type shifting was
  necessary. This is because the proof logic allows
  us to temporarily fill the position of the object quantifier with a
  hypothetical meaning constructor that consists of a type $e$ variable
  paired with the linear logic term for the object; this assumption is
  then discharged to return the scope
  of the quantifier, \formula{e \linimp\ c}, and the corresponding
  variable bound, to yield the function that maps individuals called
  by Blake to a truth value. In other words, we have demonstrated that
  this approach scopes the quantifier without positing an \emph{ad
    hoc} syntactic operation and without complicating the type of the
  object quantifier or the transitive verb. This is ultimately due to
  the commutativity of the glue logic, linear logic, since the proof
  does not have to deal with the elements of composition (words) in
  their syntactic order, because the syntax is separately represented by
  c-structure (not shown here) and f-structure. 

Lastly, let us  assume the following f-structure for (\ref{ex:ecs}):
\begin{exe}
\ex
\begin{avm}
  \lfgfst{c}[pred & \predvall{call}\\
   subj & \lfgfst{e}[pred & \predvall{everybody}]\\
   obj & \lfgfst{s}[pred & \predvall{somebody}]
  ]
\end{avm}
\end{exe}
%
Based on these f-structure labels, the
meaning constructors in the lexicon are
instantiated as follows:
%
\begin{exe}
\ex \high{Instantiated meaning constructors}
  % for \exr{bca}
  % \word{Blake called Alex}}
\ \\
\begin{tabular}{@{}l}
  \formula{\lambda y.\lambda x.\func{call}(y)(x):s \linimp\ e \linimp\
  c}\\
    \formula{\lambda Q.\func{some}(\func{person},Q):\forall S.(s \linimp\ S)
    \linimp\ S}\\
  \formula{\lambda Q.\func{every}(\func{person},Q):\forall S.(e \linimp\ S)
  \linimp\ S}
\end{tabular}
\end{exe}
%
These meaning constructors yield the following proofs, which are 
the only available normal form proofs, but there are two distinct
proofs, because of the scope ambiguity:\footnote{We have made the
  typical move in \glue\ work of not showing the trivial universal
  instantiation step this time.}
%
\begin{exe}
\ex \high{Proof 1 (subject wide scope)}\medskip\\
\begin{prooftree}[separation=0.5em]\footnotesize
\hypo{\parbox[b]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap  S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda Q.\func{some}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{some}(\func{person},Q) :$\\$\forall S.(s \multimap S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright $\lambda y.\lambda x.\func{call}(y)(x) :$\\$s \multimap e \multimap c$}}
\hypo{[v:s]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(v)(x):e \multimap c}
\hypo{[u:e]\textsuperscript{2}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\func{call}(v)(u):c}
\infer1[$\multimap_{\mathcal{I}, 1}$]{\lambda v.\func{call}(v)(u):s \linimp c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{some}(\func{person},\lambda v.\func{call}(v)(u)):c}
\infer1[$\multimap_{\mathcal{I}, 2}$]{\lambda u.\func{some}(\func{person},\lambda v.\func{call}(v)(u)):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{every}(\func{person},\lambda u.\func{some}(\func{person},\lambda v.\func{call}(v)(u)))}
\infer1[$\Rightarrow_\alpha$]{\func{every}(\func{person},\lambda
  u.\func{some}(\func{person},\lambda y.\func{call}(y)(u)))}
\infer1[$\Rightarrow_\alpha$]{\func{every}(\func{person},\lambda
  x.\func{some}(\func{person},\lambda y.\func{call}(y)(x)))}
\end{prooftree}\medskip\\

\ex \high{Proof 2 (object wide scope)}\medskip\\
\begin{prooftree}[separation=0.5em]\footnotesize
\hypo{\parbox[b]{\widthof{$\lambda Q.\func{some}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{some}(\func{person},Q) :$\\$\forall S.(s \multimap S) \multimap S$}}

\hypo{\parbox[b]{\widthof{$\lambda Q.\func{every}(\func{person},Q) :$}}{\raggedright $\lambda Q.\func{every}(\func{person},Q) :$\\$\forall S.(e \multimap  S) \multimap S$}}


\hypo{\parbox[b]{\widthof{$\lambda y.\lambda x.\func{call}(y)(x) :$}}{\raggedright $\lambda y.\lambda x.\func{call}(y)(x) :$\\$s \multimap e \multimap c$}}

\hypo{[v:s]\textsuperscript{1}}
\infer2[$\multimap_{\mathcal{E}}, \Rightarrow_\beta$]{\lambda x.\func{call}(v)(x):e \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{every}(\func{person},\lambda x.\func{call}(v)(x)):c}
\infer1[$\multimap_{\mathcal{I}, 1}$]{\lambda v.\func{every}(\func{person},\lambda x.\func{call}(v)(x)):s \multimap c}
\infer2[$\multimap_{\mathcal{E}}, \forall_{\mathcal{E}} [c/S], \Rightarrow_\beta$]{\func{some}(\func{person},\lambda v.\func{every}(\func{person},\lambda x.\func{call}(v)(x)))}
\infer1[$\Rightarrow_\alpha$]{\func{some}(\func{person},\lambda
  y.\func{every}(\func{person},\lambda x.\func{call}(y)(x)))}
\end{prooftree}
\end{exe}

\begin{sloppypar}\noindent 
  The final meaning language expressions,
  \formula{\func{every}(\func{person},\lambda
    x.\func{some}(\func{person},\lambda y.\func{call}(y)(x)))} and
  \formula{\func{some}(\func{person},\lambda
    y.\func{every}(\func{person},\lambda x.\func{call}(y)(x)))}, give
  the two possible readings for the scope ambiguity, again based on a
  standard model theory with generalized quantifiers. Once more,
  notice that neither quantifier need be moved in the syntax --- they
  are respectively just a \feat{subject} and an \feat{object} in
  f-structure. And once more, no special type shifting is necessary. It
  is a key strength of this approach that even quantifier scope
  ambiguity can be captured without positing \emph{ad hoc} syntactic
  operations (and, again, without complicating the type of the object
  quantifier or the transitive verb). This is once more ultimately due to the
  commutativity of the linear logic.
\end{sloppypar}


\section{Conclusion}

HPSG and LFG  are rather similar syntactic frameworks, both of them important lexicalist alternatives to transformational grammar.  They allow for the expression of roughly the same set of substantive analyses, where analyses are individuated in terms of deeper theoretical content rather than superficial properties.  The same sort of analytical options can be compared under both systems, answers to questions such as whether a phenomenon is to be captured on a lexical level or in the syntax, whether a given word string is a constituent or not, the proper treatment of complex predicates, and so on.   Analyses in one framework can often be translated to the other, preserving the underlying intuition of the account.   This stands in sharp contrast to frameworks such as Distributed Morphology and Minimalism, whose literature includes many theory-internal issues whose substantive content can be difficult to discern.  For example, phrasal constituency is directly represented in both LFG and HPSG, but not in Distributed Morphology or Minimalism, because the tree diagrams in those theories include many putative constituents for which evidence is non-existent.  

Against the backdrop of a general expressive similarity, we have pointed out a few specific places where one framework makes certain modes of analysis available that are not found in the other.   The main thesis of this chapter is that the differences between the frameworks stem from different design motivations, reflecting subtly different methodological outlooks.  LFG is based on the notion of functional similarity or equivalence between what are externally rather different structures.   For example, fixed phrasal positions, case markers, and agreement inflections can all function similarly to signal grammatical relations.  LFG makes that functional similarity highly explicit, reflecting its functionalist-friendly outlook on grammar.  
 
\section*{Abbreviations}
\section*{Acknowledgements}

%\fi

\printbibliography[heading=subbibliography,notkeyword=this] 
\end{document}
