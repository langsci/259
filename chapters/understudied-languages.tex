\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.13644923}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[5]
}{}

\title{HPSG in understudied languages}
\author{Douglas L. Ball\orcid{0000-0002-8046-9452}\affiliation{Truman State University}}

\abstract{Work within HPSG has explored typologically-different and ge\-net\-i\-cal\-ly-diverse languages, though the framework is not well-known for such explorations. This chapter details some of that work, focusing on the phenomena of  argument indexing (pronoun incorporation or agreement), non-accusative alignment, and VSO constituent order. Examination of proposed and possible analyses within these areas reveals that HPSG can flexibly handle a wide range of languages all while maintaining a certain uniform ``underlying structure'' within the analyses.}

\begin{document}
\maketitle
\label{understudied-languages}


\section{Introduction}

%\largerpage
To date, the most intensely studied language within the HPSG framework has been \ili{English}; this follows the trend in modern syntactic theorizing at large: \ili{English} is currently the best described language in the world. Still, there has been plenty of work within HPSG on languages other than \ili{English} (ISO 639-3 code: eng); in fact, substantial work has occurred within the framework\footnote{Citations in this paragraph are  to works, if available, whose focus is on the entire morphosyntax of the language in question rather than on particular issues in these languages.} on 
\ili{German} (ISO: deu; \citealt{Crysmann2003b,MuellerLehrbuch3}), 
\ili{Danish} (ISO: dan; \citealt{MOeDanish}), 
\ili{Norwegian} (ISO: nor; \citealt{HH2004a-u}), 
\ili{French} (ISO: fra; \citealt{KS2002a,AG2000a,AG2002b-u,abegod04,ABGT2006a-u,Winckel2024a}), 
\ili{Spanish} (ISO: spa; \citealt{Marimon2013a-u}), 
\ili{Portuguese} (ISO: por; \citealt{CB2010a-u}), 
\ili{Mandarin Chinese} (ISO: cmn; \citealt{ML2013a,YF2014a-u}), 
\ili{Japanese} (ISO: jpn; \citealt*{SBB2016a}), and 
\ili{Korean} (ISO: kor; \citealt*{KYSB2011a-u,Kim2016a-u}),
\ili{Persian} (ISO: fas; \citealt*{Taghvaipour2004a,Taghvaipour2005b,Taghvaipour2005a,Taghvaipour2010a-u,MuellerPersian,MG2010a,BS2009a,ST2010a-u}), 
among others. However, work within HPSG is not particularly well-known for exploring a wide range of typologically- and genetically-diverse languages, certainly not to the degree of its constraint-based lexicalist cousin, Lexical\indexlfg Functional Grammar. Nevertheless, there has been work within HPSG on such languages; this chapter will discuss some of this work as well as suggesting some further avenues for HPSG work within these languages.

%\largerpage[2]
The term I will employ for these typologically- and genetically-diverse languages is
\emph{understudied languages}. Which languages qualify as understudied languages, though? Does the
term just cover languages that have not previously been investigated, syntactically? Or maybe all
the languages without any previous HPSG work? Or maybe the term encompasses any language that is not
the most described language, \ili{English}? Though I reject all these (somewhat jocular)
definitions, I do grant that \textit{understudied language} is surely a fuzzy category, with
boundaries that are difficult to demarcate and with conditions for inclusion that could be
controversial. As a working benchmark for this chapter, I will suppose that the term
\textit{understudied languages} includes those languages that have a combined native and non-native
speaker population of 1.2 million or fewer (roughly 0.01\% of the world's population at present),
that are spoken currently or have gone extinct within the last 120 years, that are generally spoken
in a smaller, contiguous part of the globe, and that are not usually employed in international
diplomacy or commerce. With this benchmark, languages\footnote{The locations and genetic
  affiliations of the languages listed here were checked at \citet{glottolog18}.} like
\ili{Tongan} (Polynesian, \ili{Austronesian}; Tonga; ISO: ton), 
\ili{Kimaragang} (Dusunic, \ili{Austronesian}; Sabah, Malaysia; ISO: kqr),
\ili{Warlpiri} (Ngumpin-Yapa, Pama-Nyungan; west central Northern Territory, Australia; ISO: wbp), 
\ili{Burushaski} (isolate; Gilgit-Baltistan, Pakistan; ISO: bsk), 
\ili{Lezgian} (Lezgic, Nakh-Dagestanian; southern Dagestan, Russia; ISO: lez), 
\ili{Maltese} (\ili{Semitic}, Afro-Asiatic; Malta; ISO: mlt), 
\ili{Basque} (isolate; \ili{Basque} Country, Spain \& France; ISO: eus),  
\ili{Welsh} (\ili{Celtic}, Indo"=European; Wales, UK; ISO: cym), 
\ili{Oneida} (\ili{Iroquoian}; New York \& Wisconsin, USA; Ontario, Canada; ISO: one), 
\ili{Coast Tsimshian} (Tsimshianic; NW British Columbia, Canada \& SE Alaska, USA; ISO: tsi),
\ili{Yucatec Maya} (Mayan; Yucatan Peninsula, Mexico \& Belize; ISO: yua), and
\ili{Macushi} (Cariban; Roraima, Brazil, E Venuzuela, \& SE Guyana; ISO: mbc)
would all be included, while the eleven languages mentioned in the first paragraph would
not.\footnote{\label{list expln}Some of the languages listed above will be discussed further in this
  chapter. Others from the above list are well-known understudied languages from the linguistics
  literature. A few of these languages have HPSG work that is not mentioned elsewhere in this
  chapter: 
on \ili{Basque}, see also \citet{CB11}; 
on \ili{Batsbi}, see \citet{Crysmann2018a-u};
on \ili{Benabena}, see \citet{Crysmann2018b-u};
on \ili{Coptic Egyptian}, see \citet{CrysmannReintges2014a-u};
on \ili{Eton}, see \citet{Form2021a-u};
on \ili{Fox}, see \citet{Crysmann99a};
on \ili{Ga}, see \citet{KDHB2007a};
on \ili{Hausa}, see \citet{Crysmann2005b-u,Crysmann2012a-u,Crysmann2016a,Crysmann2017b-u};
on \ili{Khoekhoe}, see \citet{Hahn2013a-u,Hahn2014a-u};
on \ili{Limbu}, see \citet{Loreau-UngerCrysmann2024a-u};
on \ili{Maltese}, see \citet{MuellerMalteseSketch}; 
on \ili{Mauritian creole}, see \citet{HA2007a-u,Henri2010a-u,Henri2018a-u,HL2011a-u,HA2014a-u};
on \ili{Moro}, see \citet{AMM2017a-u};
on \ili{Murrinh-Patha}, see \citet{Crysmann2023a-u};
on \ili{Nias}, see \citet{Crysmann2009b-u};
on \ili{Oneida}, see also \citet{KM2010a-u}; 
on \ili{Passamaquoddy}, see also \citet{Lesourd2023a-u};
on \ili{Soranî Kurdish}, see \citet{Samvelian2007a-u,AHS2022a-u,SK2023a-u};
on \ili{Tongan}, see also \citet{dukes2000}; 
on \ili{Warlpiri}, see \citet{donosag99}; 
on \ili{West Benue}, see \citet{Mache2022a-u};
%\added{on Yiddish see also \citet{MOe2011a};} 
% spoken globally maybe does not count
on \ili{Yimas}, see \citet{Crysmann2020a-u};
on \ili{Yucatec Maya}, see \citet{dabkowski17}. This list was extended and updated in 2024 by the editors.}   


The denotation of the term \textit{understudied languages} is intended to be different from \textit{endangered language}; there is nothing in the above supposition of what an understudied language is that says that an understudied language may (or may not) potentially cease to be spoken within the next five to seventy years (see discussion in \citealt{krauss92} and \citealt{SL13} for more on endangered languages and the crisis they face).  However, the two terms do, in actuality, overlap: many understudied languages are endangered languages. While the use of HPSG (or other formal syntactic frameworks) has no direct bearing on the continued viability of a particular language, the practitioners of HPSG join with other linguists in seeing the importance of documenting such languages and supporting the rights of communities of endangered languages to continue to speak these languages.       

%\largerpage
Understudied languages exhibit a great variety of syntactic behaviors -- some of them quite similar to ``well-studied languages'', some of them quite different -- and these languages do not form an obvious natural class, syntactically. Due to space limitations, I will focus on just a very small portion of the syntactic phenomena of understudied languages: argument indexing, non-accusative alignment (chiefly ergativity), and VSO constituent order. These phenomena and their analyses will give the reader a sense of how HPSG has been or could be applied to understudied languages. Unfortunately, this means that a collection of phenomena made famous by understudied languages -- including, among others, noun incorporation (but see \citealt{malouf99,runara03,ball05afla,ball05hpsg,ball08thesis}), serial verbs (but see \citealt{muansuwan01,muansuwan02,KDHB2007a,ML2009a,lee14}), clause-chaining, evidentiality systems (but see \citealt{lee12}), object-initial word order, and applicatives (but see \citealt{runara03,ball08thesis,ball10}) -- will not be discussed.
\itdsecond{I find the ``but''s before the ``not'' strange. Difficult to process. Maybe rephrase?}

In going through the phenomena to be discussed, it will become clear that HPSG can flexibly handle a wide range of languages even while keeping its core characteristics. In fact, in most areas of analytic interest, several different approaches within the framework are equally viable at the outset. Relatedly, the analysis of many areas, especially from a cross-linguistic perspective, is far from settled. This seems to me to be an advantage: it allows competing analyses to be modeled clearly and precisely, while allowing empirical facts to better adjudicate between approaches.\footnote{This point is also made in \citet{fokkens14}, especially in Chapter~1.}

In my discussion, I will move through the three areas of argument indexing (in Section~\ref{ul:sec-argument-indexing}), non-accusative alignment (Section~\ref{ul:sec-non-accusative-alignments}), and VSO constituent order (Section~\ref{ul:sec-vso}), which corresponds to decreasing pervasiveness -- roughly estimated -- for each phenomenon across the world's languages.       

\section{Argument indexing}
\label{ul:sec-argument-indexing}

Widespread among all sorts of natural languages -- understudied or not -- is what \citet{haspelmath13} terms \isi{argument indexing}. In argument indexing, morphologically dependent elements -- that is, affixal elements usually located within or near the verb and with denotations (seemingly) similar to pronouns -- either occur in place of arguments of the main semantic predicate of the clause or alongside them.\footnote{Thus, this area includes what has been considered to be predicate-argument agreement as well as what some consider to be ``pronoun incorporation'', though one of the key points of \citet{haspelmath13} is that the pre-existing terminology -- if not also the pre-existing analyses -- in this domain has been misleading.} While this phenomenon occurs even a bit in \ili{English} and throughout other European languages, argument indexing in understudied languages tends to be more ``rampant'': that is, all (or most) of the verb's arguments are indexed, rather than just the subject being indexed, as is the most common pattern in Europe \citep{siewierskaWALSvpm}. When the argument indexing is ``rampant'': its treatment within the syntax (and within the morphology-syntax interface) of a language becomes a key question. HPSG analyses offer several possible answers how the syntax of argument indexing works, all while maintaining the framework's surface orientation. Empirically, it is clear that not all argument indexes behave in quite the same way in all languages, so I will explore the analysis of two subtypes of indexes in the sections to follow: first, indexes that do not co-occur with external, role-sharing noun phrases, and, second, indexes that can co-occur with external, role-sharing noun phrases.\footnote{See also \citet{saleem10} for a similar -- though not identical -- analysis of the same analytical domain.}   

\subsection{Indexing in complementary distribution with conominals}
\label{pro-indexes}

%\largerpage[-1]
Some argument indexes in some languages have the property that they do not -- and cannot -- appear
with a non-pronominal element sharing their same syntactic/semantic role in the same narrow clause
(I will refer to these non-pronominal elements as \emph{conominals}\is{conominal}, following \citealt[\page 205]{haspelmath13}). The term that Haspelmath suggests -- and I will use -- for such argument indexes is \emph{pro-index}\is{pro-index}. One language showing pro-index behavior with its argument indexes is \ili{Macushi}, as revealed from the examples in (\ref{macushi}): 
%
\begin{samepage}
\begin{exe}
\ex \label{macushi} \ili{Macushi} [mbc] (\citealt[\page 25]{abbott91} via \citealt[\page
226]{siewierska99} and \citealt[\page 186]{corbett03}) \nolistbreak
\begin{xlist}
\ex[]{\label{mac a}
\gll i-koneka-'p\^{\i}-u-ya \\
\textsc{3sg.abs}-make-\textsc{pst}-\textsc{1sg}-\textsc{erg} \\
\trans `I made it.'}
\itdsecond{The papers translate this as ``He made it.''}
\ex[*]{\label{mac b}
\gll uur\^{\i}-ya i-koneka-'p\^{\i}-u-ya \\
\textsc{1sg}-\textsc{erg} \textsc{3sg.abs}-make-\textsc{pst}-\textsc{1sg}-\textsc{erg} \\
\trans Intended: `I made it.'}
\itdsecond{Sources say ``I, I made it.''}
\end{xlist}
\end{exe}
\end{samepage}
The example in (\ref{mac a}) is just a verb with all its arguments realized as argument indexes. The example in (\ref{mac b}) clearly reveals the pro-index behavior of the argument indexes: the affixed verb is incompatible with an independent pronoun, such as \textit{uur\^{\i}ya} `\textsc{1sg.erg}'. 

%\largerpage[-1]
The \isi{pro-index} phenomenon has a straightforward (and, as a result, commonly assumed) analysis within HPSG. The analysis was originally proposed by \citet{MS97a-u} for \ili{French} ``clitics'', but could equally be applied to the \ili{Macushi} case above, among others. Key to this analysis is the idea found in most versions of HPSG (emerging in the mid-to-late 1990s) that there are separate kinds of lists for the combinatorial potential(s) of heads. In fact, not only are there these separate lists, but there can be (principled) mismatches between them (see \crossrefchapteralt[Section~\ref{properties:lexemes-and-words}]{properties} and \crossrefchapteralt{arg-st}). The first of these lists is the \textsc{argument-structure} (\textsc{arg-st}) list. This list handles phenomena related to the syntax-semantic interface, like linking \citep{Davis2001a-u}, case assignment (\citealt{Meurers99b,Prze99b}; \crossrefchapteralt{case}), and binding restrictions (\citealt{mannsag98,wecharka98}; \crossrefchapteralt{binding}). The other lists are the two valence lists, the \textsc{subject} (\textsc{subj}) list and the \textsc{complements} (\textsc{comps}) list. These are concerned with the ``pure'' syntax and mediate which syntactic elements can combine with which others.

On the \citeauthor{MS97a-u}-style analysis of pro-indexes, the verb's \textsc{arg-st} list contains
feature descriptions corresponding to all of its arguments. For the examples in (\ref{macushi}), the
verb's \textsc{arg-st} list would include a feature description for both the semantic maker and the
semantic element that is made (as will appear in (\ref{made it})). However, the same verb's
\textsc{subj} and \textsc{comps} lists would contain no elements corresponding to any affixed
arguments. What prompts this disparity? The arguments realized by affixes correspond to a special
kind of feature description on the \textsc{arg-st} list, typed \textit{non-canonical}.\footnote{In
  the version of HPSG of \citet{GSag2000a-u}, \textit{non-canonical} was an immediate subtype of
  \textit{synsem}; and the relevant feature descriptions were thus seen as syntactico-semantic
  complexes. In the latter-day version of HPSG known as Sign-Based Construction Grammar
  \citep{Sag2012a}, the \textit{non-canonical} type was re-christened \textit{covert} and was an
  immediate subtype of \textit{sign}, the relevant feature descriptions being entire signs. In spite
  of these differences, which may seem significant, the analysis is very similar in both versions of
  the framework. Several subtypes of \textit{non-canonical}/\textit{covert} have been recognized,
  the subtype relevant for this example would be \textit{aff}. But I will just use the
  \textit{non-canonical} type here. For a general comparison of the version of HPSG used here and
  throughout the volume with SBCG see \crossrefchapterw[Section~\ref{cxg:sec-sbcg}]{cxg}.}
(Intuitively, these arguments are realized in a non-canonical way.) Feature descriptions of the
\textit{non-canonical} type differ from their sibling type \textit{canonical} in how they interact
with the \textsc{subj} and \textsc{comps} lists. Governing the relationship between the
\textsc{arg-st} and these valence lists is the Argument Realization Principle\is{principle!Argument
  Realization (ARP)}, which is stated in (\ref{ARP}):%
	\footnote{The append operator $\oplus$ allows two lists to be combined, preserving the
          pre-existing order of the elements on the new list. Thus, \phonliste{ a, b } $\oplus$
          \phonliste{ c, d } will yield \phonliste{ a, b, c, d }. \citet[\page 170]{GSag2000a-u} define
   $\ominus$\is{$\ominus$} as follows: ``Here `$\ominus$' designates a relation of contained list difference. If
$\lambda_2$ is an ordering of a set $\sigma_2$ and $\lambda_1$ is a subordering of $\lambda_2$, then
$\lambda_2 \ominus \lambda_1$ designates the list that results from removing all members of
$\lambda_1$ from $\lambda_2$; if $\lambda_1$ is not a sublist of $\lambda_2$, then the contained
list difference is not defined. For present purposes, $\ominus$ is interdefinable with the sequence
union operator ($\bigcirc$\is{$\bigcirc$}\isrel{shuffle}) of
\citet{Reape94a} and Kathol (1995): $(A \ominus B = C) \Leftrightarrow (C \bigcirc B = A)$.''
$\bigcirc$, which is called \emph{shuffle}, is also explained in \crossrefchapterw[\page
\pageref{rel-shuffle}]{order}.
}

% \added{For a different version of the Argument Realization Principle that is used elsewhere in this book
% see \crossrefchapterw[\pageref{ex:prop20}]{properties}. See \crossrefchapterw[Section~\ref{argst-valence-sec}]{arg-st}
% for different ways of linking.}} 
%
\ea\label{ARP} 
Argument Realization Principle\is{principle!Argument Realization (ARP)} adapted from \citew[171]{GSag2000a-u}:\\
% Doug's original version.
%\emph{word} \impl
% \avm{
%  	[ss|loc|cat & [subj & \tag{1} \\
%  		       comps & \tag{2} \\
%  	               arg-st & \listOf{canonical} (= \tag{1} \+ \tag{2}) \shuffle \list*(non-canonical) ] ] 
% }
\emph{word} \impl
\avm{
 	[ss|loc|cat & [subj   & \tag{1}\\
 		       comps  & \tag{2} \- \listOf{non-canonical} \\
 	               arg-st & \tag{1} \+ \tag{2}  ] ] 
}
\z
\largerpage%[2]
\enlargethispage{2pt}
The constraint says that the \argstl is split in two parts: \ibox{1} and \ibox{2}. The first list is
identified with the \subjl. The list can be empty or it can contain one or more elements. Usually
the length of the \subjl is limited to one element.\footnote{
But see \citew{MOe2013b} for an analysis of pronoun shift in \ili{Danish} assuming multiple subjects.
} A list of \type{non-canonical} elements is
subtracted from \ibox{2}. The result of this difference is the value of \comps. This formulation of
the Argument Realization Principle allows non-canonical elements like clitics and gaps in the \subj
and \compsl. As \citet[\page 171]{GSag2000a-u} point out, this is not a problem since overt signs that are
combined with heads by the Head-Complement Schema or the Head-Subject Schema have a \synsem value of
type \type{canonical} and hence could never be combined with heads having elements of
type \type{non-canonical} in their valence lists. \citet[\page 40]{GSag2000a-u} assume the Principle
of Canonicality, which is given in (\mex{1}):
\ea
Principle of Canonicality\is{principle!of Canonicality} (adapted from \citealt[\page 40]{GSag2000a-u}):\\
\type{sign} \impl \avm{
[ synsem & canonical ]}
\z
\largerpage
The principle ensures that full signs always have \synsem values of type \type{canonical}.
%   used for \ili{English} and \ili{French}. It allows
% non-canonical elements like gaps and clitics in the \subjl (see \citealt[Section~5.1.3]{GSag2000a-u}
% and \citealt[\page 50]{AG2007a-u}). This would be
% too permissive for \ili{Macushi} where only canonical elements should appear on the \subj and \compsl. Hence
% a further constraint is needed in addition to the constraints in (\mex{0}). The implication in
% (\mex{1}) ensures that both \subj and \comps contain elements of type \type{canonical} only:
% \ea
% \emph{word} \impl
% \avm{
%  	[ss|loc|cat & [subj   & \listOf{canonical}\\
%  		       comps  & \listOf{canonical} ] ] 
% }
% \z
% %
(\ref{ARP}) and (\mex{0}) together make sure that only the \textit{canonical} feature descriptions
on the \textsc{arg-st} can (and must) appear on the \textsc{subj} and \textsc{comps} lists of heads
that are used in further combinations.\footnote{
\citet[Section~5.1.3]{GSag2000a-u}  and \citet[\page 50]{AG2007a-u} make use of the fact that
gaps are admitted in the \subjl to account for \emph{that} trace effects in \ili{English} and the \emph{qui}/\emph{que}
distinction in \ili{French} relative clauses. However, these gaps are just used to distinguish
sentences with subject gaps from sentences without subject gaps. The verbs with gapped subjects
never combine with them via the Head-Subject Schema.
} %Thus,
% the constraints on \emph{word} disallow any \textit{non-canonical} argument from being mapped to
% either of these valence lists. 
This, then, captures the idea that affixal arguments, which are of type \type{non-canonical}, are generally inert in the combinatorics of the syntax proper: they saturate an argument slot and that argument slot is no longer available for other (at least direct) syntactic combination. 

So, returning to the \ili{Macushi} word \textit{ikoneka'p\^{\i}uya} `I made it.' from (\ref{mac a}),
the relevant partial lexical description is given in (\ref{made it}):
% 
\ea
\label{made it}
Lexical item for \emph{ikoneka'p\^{\i}uya} `I made it.':
\avm{
	[ss|loc &	[cat &	[head & verb \\
				 subj & < > \\
				 comps & < > \\
                                 arg-st & <[\type{non-canonical}]$_{\1}$, [\type{non-canonical}]$_{\2}$ >] \\ 
			 cont &	[rels &	<[\type*{make-rel}
					actor & \1 \\
					underdgoer & \2 ]> ] ]  ]
}
% changed arg1 and arg2 to act and und
\z
%
In (\ref{made it}), the word's \textsc{arg-st} list is comprised of two \textit{non-canonical} feature descriptions (corresponding to the maker argument with index \ibox{1} and the made argument, with index \ibox{2}). Yet, by the Argument Realization Principle, the \textsc{subj} and \textsc{comps} lists are empty. Consequently, \textit{ikoneka'p\^{\i}uya} `I made it.', can be a clause by itself -- as it is in (\ref{mac a}) -- because it requires no other valents (that is, it has a ``saturation'' level on a par with a clause) and it is headed by a verb (just like a clause is). 

%\largerpage[-1]
The specification of an empty list also contributes to ruling out examples like (\ref{mac b}) with a
conominal. On the standard HPSG view of how valence is managed, no element with an empty valence
list can combine with any possible valence-saturating syntactic entity, like an NP.\footnote{To
truly rule out the NP from combining with the verb in Figure~\ref{no conominal}, the NP would also
need to not match any nonlocal requirements of the verb, since otherwise the combination in
Figure~\ref{no conominal} could be an instance of the Filler Head Schema. See \crossrefchapterw{udc} for an overview
of analyses of nonlocal dependencies in HPSG.} Thus, the grammar would correctly not license a tree like in Figure~\ref{no conominal}.\footnote{The tree in Figure~\ref{no conominal}, as well as other trees in this chapter, only provides the relevant attribute-value pairs, suppressing the geometry of features found in more articulated feature descriptions.}
%
\begin{figure}
\centering
\begin{forest}
sm edges
[{\raisebox{-1.3ex}[\height][0pt]{*}}
	[NP[uur\^{\i}ya;me.\textsc{erg},tier=word,roof]] 
	[\avm{
		[head & verb \\
		subj & < >   \\
		comps & < > ] 
	} 
[ikoneka'p\^{\i}uya;I.made.it, tier=word]]]
\end{forest}
\caption{A tree of an illicit conominal--pro-index combination}
\label{no conominal}
\end{figure}

\largerpage
Overall, the Argument Realization Principle-mandated non-mapping of the \textit{non-canonical} \textsc{arg-st} list members to either the \textsc{subj} or the \textsc{comps} list captures the key behavior found in the pro-indexing type of argument indexing: the argument indexes occur in complementary distribution with any conominal.    
 
\subsection{Indexing co-occurring with conominals} 

Even though the pro-index type of argument index has a more straightforward analysis, this type is
not the most common in the world's languages. Rather, the most common type of argument indexing
appears to be the one where the argument indexing affix(es) can co-occur with a conominal, but
do(es) not have to.  In \citegen{haspelmath13}
%\addpages the whole paper is about terminology, Section 4.3
terms, this is the \isi{cross-index} type.\footnote{The behavior of cross-indexes is canonical for so-called ``pro-drop\is{pro-drop@\emph{pro}-drop}'' languages, a term arising from the transformational syntax tradition (particularly from \citealt[\page 28, Section~4.3]{Chomsky81a}), but now with wider currency.} A language exhibiting this type of behavior is \ili{Basque}, as evident from the example in (\ref{basque}):    
%
\begin{samepage}
\begin{exe}
\ex\label{basque} \ili{Basque} [eus] \citep[98]{laka96} \\
\gll Zuk niri liburua saldu d-i-da-zu. \\
\textsc{2sg.erg} \textsc{1sg.dat} book.\textsc{def} sold \textsc{3sg.abs}-\textsc{aux}-\textsc{1sg.dat}-\textsc{2sg.erg} \\
\glt `You have sold me the book.' 
\end{exe} 
\end{samepage}
%
Though \textit{zuk} `you', \textit{niri} `me', or even \textit{liburua}  `the book' would not need to be present for the grammaticality of this sentence, this sentence (and language) exhibits cross-index behavior because, even though these conominals are present, the argument indexing affixes on, in this case, the auxiliary \textit{didazu} `\textsc{3sg.abs}:\-\textsc{aux}:\-\textsc{1sg.dat}:\-\textsc{2sg.erg}' still occur. 

Unlike the pro-indexes, there is no current standard HPSG analysis of cross-indexes.\footnote{This may, in part, be a consequence of the standard way of managing predicate-argument relations in the syntax in HPSG: this management strategy is resource-sensitive -- namely, once something is ``cancelled'' off a \textsc{subj} or \textsc{comps} list, it no longer appears on any subsequent (higher) lists and cannot be used for other syntactic purposes. However, Section~\ref{other poss for cross} will discuss some HPSG approaches where the management strategy is not so resource-sensitive.} Nevertheless, there are some possible approaches. I detail two in some depth here -- what I will call the underspecification analysis and what I refer to as the direct syntax approach -- and mention some other options near the end of the section.

\subsubsection{Underspecification for cross-indexes}

\largerpage
On the underspecification analysis, the lexical descriptions of argument index-containing words would have underspecified feature descriptions on their \textsc{arg-st} lists, corresponding to their argument indexes. These would then resolve depending on the syntactic context. Which portions of the feature description would be underspecified is a bit flexible (at least, in the abstract) and depends on whether the analyst thought the ``agreement'' (argument indexing) was more formal or semantic in nature (see \crossrefchapteralt{agreement} for a more thorough discussion of what is involved here). For the sake of illustration purposes, I will employ a more semantic approach below.

Let us consider a word, like the \ili{Basque} auxiliary \textit{dut} `\textsc{aux:3.abs:1sg.erg}', that has a third-person singular absolutive argument index. Such a word might just be specified, by the constraints on the various lexical types of \ili{Basque}, as in (\ref{unsp dut}):\footnote{The lexical descriptions associated with \textit{dut} in (\ref{unsp dut})--(\ref{sp dut 2}) all have a further argument -- the verbal expression associated with the auxiliary -- suppressed in these descriptions (with ellipses) because such a verbal argument (and its interaction with the other arguments) is not the focus of the analysis here.}
%
\begin{exe}
\ex\label{unsp dut}
\avm{
[ ss|loc|cat|arg-st & <[\type{synsem}],  [\type*{synsem}
				        loc|cont|ind & 3sg ], \ldots > ]
}
\end{exe}
%
To be consistent with (\ref{unsp dut}), the second \textsc{arg-st} list member just needs to be something that is semantically a third person. Therefore, the second \textsc{arg-st} list member could ultimately resolve to a \textit{non-canonical} feature description, as in (\ref{sp dut 1}): 
%
\begin{exe}
\ex\label{sp dut 1}
\avm{
[ss|loc|cat|arg-st & < [\type{synsem}], [\type*{non-canonical}
			      loc|cont|ind & 3sg ], \ldots > ]
}
\end{exe}
%
This resolution would be forced when no conominal is present (if this \textit{synsem} on the \textsc{arg-st} list resolved to the \textit{canonical} type and a conominal was not present, the \textsc{comps} list would illicitly not be emptied). The analysis would, in this condition, be identical to that of the pro-indexes provided in Section~\ref{pro-indexes}.

However, the second \textsc{arg-st} list member could also ultimately resolve to a \textit{canonical} feature description, as in (\ref{sp dut 2}):   
%
%
\begin{exe}
\ex\label{sp dut 2}
\avm{
[ss|loc|cat|arg-st & <[\type{synsem}], [\type*{canonical}
			     loc|cont|ind & 3sg ], \ldots > ]
}
\end{exe}
%
This resolution would be forced when a conominal is present (otherwise, the conominal could not be syntactically licensed). Thus, the analysis, in this condition, is like an instance of obligatorily co-present conominal and argument index (a \isi{gramm-index} in Haspelmath's terms). 

As the discussion above indicates, there is a certain portion of this analysis that is not lexically mandated: the precise resolution of the argument depends on the specific syntactic expressions appearing in a particular clause. This analysis is also of the dual-nature type discussed by \citet{haspelmath13}: the argument index is treated as a pro-index when it has no conominal and it is treated as gramm-index when a conominal is present. Other frameworks employ a similar analysis (LFG\indexlfg does, for instance -- see \citealt[Chapter~8]{BATW2015a}). Haspelmath criticizes this approach for positing two distinct structural types for a single kind of affix; though, in the analysis above, it does not seem that the structural types are that radically different (observe that just one underspecified lexical description is associated with a given affixed form). Still, we might want to at least consider other options -- and, in keeping with the tendency for multiple different approaches to be found within HPSG, there are some.  

\subsubsection{The ``direct syntax'' approach to cross-indexes}

Another approach to cross-indexes, proposed for \ili{Oneida} in \citet{KM15}, takes the view that, in at least some languages, argument indexes always stand for arguments and the combination of a conominal and a verb with argument indexing is more purely semantically mediated and akin to a nominal expression combining with an already saturated narrow clause.\footnote{This analysis is perhaps the closest any HPSG analysis comes to the so-called Pronominal Argument Hypothesis \citep{jelinek84}.} This approach Koenig \& Michelson have called the ``direct syntax approach'' (it is direct in the sense that the combinatorics are not mediated by any valence lists, which, arguably, is a bit more ``indirect''). 

As \citet{KM15} discuss in detail, it appears that \ili{Oneida} exhibits some interesting properties that make treating its argument indexing patterns in a (seemingly) rather different way much more plausible. For one, as shown in (\ref{oneida 1}), the verb indexes all its arguments morphologically (except inanimates, like `his axe' in (\ref{oneida 2})) -- often with portmanteau affixes, as in (\ref{oneida 1}) -- making the case that the argument indexes are the actual arguments much stronger. 
%
\begin{exe}
\ex\label{oneida 1} \ili{Oneida} [one] \citep[5]{KM15} \\
\gll wa-hiy-at\'{e}{\textperiodcentered}kw-a-ht-eʔ \\
\textsc{fact}-\textsc{1sg$>$3.m.sg}-flee-\textsc{lnk.v}-\textsc{caus}-\textsc{pnc} \\
\glt `I chased him away.' 
\end{exe} 
%
Second, the evidence is equivocal about whether the language has any selection that cannot be treated as semantic selection.

Thus, on Koenig \& Michelson's view (and in keeping with the terminology of the previous discussion): all the arguments correspond (at best) to \textit{non-canonical} elements on the \textsc{arg-st} list and thus there is never any head-argument combinations in the syntax. Any and all conominals then are licensed via index sharing of a nominal and an element on a \textsc{nonlocal} feature that Koenig \& Michelson call  \textsc{disloc} (see \citealt[39]{KM15} for discussion of why they consider this the best way to deal with  the \textsc{nonlocal} feature), as shown in Figure~\ref{nonlocal tree}, a tree of (\ref{oneida 2}):
\begin{exe}
\ex\label{oneida 2} \ili{Oneida} [one] \citep[17]{KM15} \\
\gll ʌ-ha-hyoʔthi{\textperiodcentered}y\'{a}t-eʔ laoto{\textperiodcentered}k\'{ʌ}{\textperiodcentered},\\
     \textsc{fut}-\textsc{3m.sg.a}-sharpen-\textsc{pnc} his.axe \\
\glt `He will sharpen his axe,'
\end{exe} 
%
\begin{figure}
\centering
\begin{forest}
sm edges
[%
\avm{
	[head & verb \\
	disloc & \{ \} ]
} 
	[%
	\avm{
		[head & verb \\
		disloc & \{\1\} ]
	}
		[ʌhahyoʔthi{\textperiodcentered}y\'{a}teʔ;he will sharpen, tier=word]]
	[NP$_{\ibox{1}}$ [laoto{\textperiodcentered}k\'{ʌ}{\textperiodcentered};his axe,tier=word,roof]]
]
\end{forest}
\caption{Licensing conominals on the direct syntax approach}
\label{nonlocal tree}
\end{figure}
 

\citegen{KM15} discussion suggests that the direct syntax type might represent an extreme, occurring only in the most \isi{polysynthetic} and non-configurational\is{non-configurationality} of languages, like \ili{Oneida} and its \ili{Iroquoian} kin. However, this claim remains an open question. Perhaps further study will reveal that this sort of analysis could profitably be employed in other kinds of languages.  

\subsubsection{Other possibilities for cross-indexes} \label{other poss for cross}

In addition to the analyses discussed in the previous two subsubsections, there are a few more conceptual avenues that might be explored for the analysis of cross-indexes, though it is not clear that they have been fully explored in the literature yet (which might raise some questions as to their viability). 

One route to explore would be to use lexical rules to create \textsc{arg-st} or the valence lists with feature descriptions corresponding to both the argument indexes \emph{and} the conominals (something similar was explored for ``clitics'' in various \ili{Romance} varieties by \citealt{monachesi05}). Such a lexical rule might look as in (\ref{lex rule}):
%
\ea
\label{lex rule}
\avm{
	[synsem|loc|cat|arg-st & \tag{1} <[\type{canonical}],	[\type*{non-canonical}
						 loc|cont|ind & \2] >]
	$\mapsto$} \\								 
\flushright	
\avm{[synsem|loc|cat & [ comps & list \+ <[\type*{canonical}
					           loc|cont|ind & \2 ]> \\
                                arg-st & \tag{1} ]]
}
% todo avm space after AVM is lost once \flushright is in place
\z
\medskip

%\largerpage
\noindent
This approach might be a way to loosen the resource sensitivity of the usual valence regime, though a proposal along these lines would need to take care in considering whether any changes would be needed in the statement of the Argument Realization Principle (and if so, what form they should take) and if there would be any undesirable consequences to allowing single semantic arguments to correspond to more than one syntactico-semantic element.    

Another route to consider would be to relax the resource sensitivity in the syntax, instead of in the information associated with single words. Given proposals like the one in \citet{Bender2008a} for non-cancellation of arguments (more detailed discussion of this proposal is in \crossrefchapterw[Section~\ref{sec-free-without-domains}]{order}\footnote{Also see like-minded proposals in \citet{Meurers99b} and \citet{Mueller2008a}.} to deal with apparent cases of discontinuous constituency, maybe something similar should also be explored for the cross-index type of argument indexing. 

Overall, there seems to be a need for more explorations into cross-index behavior cross-linguistically within HPSG. Certainly, the above discussion shows that, in fact, there is no shortage of possible analyses, but work remains to further determine which of these might be the best analysis, overall, or which of these might be best for which languages.

\section{Non-accusative alignments}
\label{ul:sec-non-accusative-alignments}

%\largerpage 
Another area (in fact, not so distant from argument indexing in function) where
understudied languages have enriched the general understanding of natural language morphosyntax is
in the area of (morphosyntactic) \isi{alignment}. Alignment concerns how the morphology of a
language (if not also its syntax) groups together (or ``aligns'') different arguments into (what
seem to be) particular grammatical relations (see \citealt{bicknich09} for an overview of
alignment).\footnote{Alignment can be explored both in \isi{head-marking} and
  \isi{dependent-marking} \citep{Nichols86a-u}; however, having already focused on a kind of
  head-marking strategy in the previous section, I will focus on the corresponding dependent-marking
  strategy in this section.} The most widespread alignment is the accusative one -- familiar from
ancient Indo"=European languages and conservative modern day ones -- where subjects of transitive
and intransitive verbs are treated differently from the objects of transitive verbs. Other
recognized alignments include \isi{ergative} (where subjects of transitive verbs are treated
differently from the subjects of intransitives, which, in turn, pattern with the direct objects of
transitives) (see \citealt{comrie78erg,plank79,dixon79,dixon94}, among others, for further
discussion), split-S/active (where semantic agents and patients are treated differently) (see
\citealt{klimov73,klimov74}; \citealt[Chapter~4]{dixon94}; \citealt{mithun91}; \citealt{wichdon08},
among others, for further discussion), tripartite (where subjects of transitive verbs, subjects of
intransitive verbs, and objects of transitive verbs are each treated differently)
\citep[39--40]{dixon94}, \ili{Austronesian} alignment\footnote{This kind of system is known by
  various different names other than \ili{Austronesian} alignment, including a symmetrical voice
  system, a Philippine-type voice system, or an \ili{Austronesian} focus system.} (where arguments
of various semantic roles can flexibly hold a privileged syntactic slot) (see
\citealt{schachter76,ross02,himmelmann05typchar} for more discussion), and hierarchical alignment
(where elements of higher discourse salience are treated differently from elements of lower
discourse salience) (see \citealt{jacqant14} for a good overview of what is involved).

Surveys from WALS \citep{comrieWALSalignfullNP,comrieWALSalignpro,siewierskaWALSalignvp} indicate that accusative alignment is common worldwide,\footnote{However, since the surveys focus more on coding patterns\is{coding properties} rather than behavioral patterns\is{behavioral properties} (coding and behavioral in the sense of \citew{keenan76subjects} -- ``coding'' related to morphological patterns or function words that signal a particular grammatical relation category; ``behavioral'' related to reference properties or patterning across clauses), it is possible that they underreport behavioral accusative patterns, even among languages that have so-called ``neutral'' coding patterns.} and this seems to be even more true of languages with large numbers of speakers. Of the top 25 most widely spoken languages at present, arguably only a collection of languages from the Indian subcontinent (\ili{Hindi-Urdu}, \ili{Marathi}, and \ili{Gujarati}) have non-accusative alignments, and even those are restricted to certain portions of their respective verbal systems (see \citealt[Chapter~7]{verbeke13} for more on the patterns in these and other Indo"=Aryan languages). Impressionistically, it seems that understudied languages do have a much stronger propensity for non-accusative alignments. 


Because the non-accusative alignments at least seem to be rather different than accusative alignment, it is an interesting question how a given framework might handle these kinds of systems. In the majority of this section I will focus on the analysis of ergative systems, as a proof of concept (see, however, \citealt{Drellishak2009a-u} for analyses of each of the non-accusative alignments, including the hierarchical type).\footnote{There is also some discussion of an HPSG analysis of the ergative-aligned case system of the Caucasian language \ili{Archi} -- similar, in some respects, to the \ili{Lezgian} examples I consider further on -- in \citet{borsley16}, though the focus of that paper is much more on \ili{Archi}'s argument indexing system rather than its case system.}

%\largerpage[2]
In dealing with the analysis of ergative systems, it will be useful to divide the discussion into two parts. First, I will consider how particular morphological forms within NPs are licensed in instances when they co-occur with their governing verb -- I will call this ``the licensing of case in the syntax'' (see also \crossrefchapteralt{case}). Second, I will consider how particular arguments come to be associated with particular morphological forms (whether realized or not) -- I will call this ``the licensing of case in linking''  (see also \crossrefchapteralt{arg-st}). This division is not commonly recognized in most other frameworks; however, it does present itself as a possible division within HPSG, due to the separate \textsc{arg-st} and valence lists. 

\subsection{The licensing of case in the syntax}

The licensing of case in the syntax within HPSG is as straightforward in non-accusative alignments
as it is in an accusative alignment system; the fundamentals are the same, regardless of
alignment. This comes about due to the use of feature value matching (also known as ``feature
unification'') for case licensing in the syntax.\footnote{Feature value matching does have some
  conceptual similarity to the ``feature checking'' approach to case found in more recent Minimalist
  work \citep{Chomsky91a-u,Chomsky93b-u,adger2000,Adger2010a,framgut06,pestor07}\iaddpages, though
  there are notable differences between the approaches, particularly that features are deleted in
  feature checking, but not in feature value
  matching. \crossrefchaptert[Section~\ref{minimalism:sec-deletion-of-features}]{minimalism} discuss
  problems that arise for feature checking approaches if values are needed more than once, \eg in
  free relative clauses. } The simple premise of feature value matching is that a value for a
particular feature possessed by an argument and a feature value required by its head (for that same
argument) must match. The nature of this analysis makes case licensing nearly identical -- excepting
the different values involved -- to the selection of part-of-speech categories.\footnote{Thus, to
  use terms more commonly associated with Mainstream Generative Grammar (i.e.\ work in
  Transformational Grammar, \eg work in Government \& Binding\indexgb and Minimalism\indexminimalism \citealt{Chomsky81a,Chomsky95a-u}), HPSG views case licensing as (a
  specific kind of) c-selection (in the sense of \citealt{Grimshaw79a-u}).}

%\largerpage[2]
To actually license case in the syntax with an ergative system, the key elements are (1)
a feature for nominal expressions (call it \textsc{case}) and (2) appropriate values for
\textsc{case}, like \textit{ergative} and \textit{absolutive}. Note that \textit{ergative} and
\textit{absolutive} are types, so they can be potentially grouped with other case values into
supertypes, like \textit{structural} cases or \textit{semantic} cases, if such groupings are
relevant (as was done for the first time in \citealt[\page 207]{HM94a}). With those features in
place, the rest of the analysis falls out through the larger theories of syntactic selection,
featural identities, and syntactic combination: certain heads will require [\textsc{case}
\textit{erg(ative)}] and [\textsc{case} \textit{abs(olutive)}] of their arguments. If certain
potential arguments are just single words, the values for \textsc{case} of these words will
straightforwardly match or not. If certain potential arguments consist of multiple words,
independent constraints on \textsc{head} value identity will ensure that the value for \textsc{case}
will be identical between the head daughter and overall phrase
\crossrefchapterp[\pageref{page-hfp}]{properties}; constraints on the syntactic combination then
ensure that the \textsc{case} values of the nominal expressions and the head requirements match.

To see this with an actual example, let us consider the \ili{Lezgian} sentence in (\ref{lezgian 2}): 
%
		\begin{samepage}
		\begin{exe}
		\ex \label{lezgian 2} \ili{Lezgian} [lez] \citep[287]{haspelmath93} \\
		\gll Aburu zun ajibda. \\
		\textsc{3pl.erg} \textsc{1sg.abs} shame.\textsc{fut} \\
		\trans `They will shame me.' 
		\end{exe} 
		\end{samepage}
%
The example in (\ref{lezgian 2}) could be analyzed with the tree in Figure~\ref{lezgian tree}.		

\begin{figure}
\centering
\begin{forest}
sm edges,
for tree={l sep=10 mm}
[%
\avm{
	[head & \1 \\
	subj & < > \\
	comps & < > ]	
}, 
	[%
	\avm{
		\2 [head &	[\type*{noun}
					case & erg ] \\
		subj & < > \\
		comps & < > ]
	}
		[aburu;they]] 
	[%
	\avm{
		[head & \1 \\
		subj & < \2 > \\
		comps & < > ] 
	}, edge label={node[midway,above,outer sep=0.5 mm]{H}}
		[%
		\avm{
			\3 [head &	[\type*{noun}
						case & abs ] \\
			subj & < > \\
			comps & < > ]
		}
			[zun;me]]
		[%
		\avm{
			[head & \1 verb \\
			subj & < \2 > \\
			comps & < \3 >  ]
		}, edge label={node[midway,above,outer sep=0.5 mm]{H}}
		[ajibda;will shame]]
	]
]	
\end{forest}
\caption{Analysis of the Lezgian example \emph{Aburu zun ajibda.} `They will shame me.'}
\label{lezgian tree}
\end{figure}

\largerpage
The tree in Figure~\ref{lezgian tree} consists of two head-argument combinations, and in fact, the tree has the same geometry as an accusative verb-final language (on standard assumptions about the constituency) -- indeed, as the HPSG analysis does not intrinsically tie the analysis of case with constituency, the geometry of clauses would, all else being equal, not differ based on alignment alone. The most idiosyncratic aspect of Figure~\ref{lezgian tree} is that the verb \textit{ajibda} `will shame' is one that requires an ergative--absolutive combination of arguments. Because the HPSG framework is feature-rich and formally rigorous in how feature values must be constrained within constituent structures, the licensing of case in the syntax in an HPSG analysis is very straightforward.    	

\subsection{The licensing of case in linking}
 
Lurking behind the most idiosyncratic aspect of Figure~\ref{lezgian tree} is the question of  how particular heads come to have their particular argument requirements. This is, in fact,  the question of how case is licensed in linking. As with other matters of non-accusative alignments, it seems that different alignments need not be treated in a wholly different fashion from each other: thus, the same kinds of analytic moves for accusatively aligned systems could be used for non-accusatively aligned systems. That being so, it is probably too hasty to assume that there is a one-size-fits-all solution for linking of case across all languages (regardless of alignment), as quite a few different factors appear to be important in different languages, among them at least verb class (that is, the classes related to the verbal lexical semantics), the semantic nature of the argument itself, the morphological form of the verb, and the subordination status of the clause headed by the verb (see, for example, discussion in \citealt{dixon94}).     

\largerpage[2]
In all known \isi{ergative} languages, the ergative--absolutive case pattern -- clearly indicating that the subjects of transitive verbs are not encoded like the subjects of intransitive verbs -- appears with ``primary transitive verbs'' (a term from \citealt{andrews85,andrews072nded}): predicates with the canonical meaning associated with transitive verbs where an initiating entity causes change in an undergoing entity. Given this basic generalization, a possible analysis of the arguments' case requirements with these ``primary transitive verbs'' would be through the constraint in (\ref{simple erg}):
%
\begin{exe}
\ex\label{simple erg}
\emph{trans-v-lxm} \impl\\
\avm{
[ss|loc & [ cat|arg-st & < [case & erg]$_{\1}$, [case & abs]$_{\2}$ > \+ list\\
            cont|key [\type*{act-und-rel}
                      act & \1 \\
		      und & \2 ] ] ]
}
%\itdobl{JP convinced me that actor and undergoer are standard as role labels. So we have this throughout the volume.}
\end{exe}
%
In (\ref{simple erg}), the \textit{transitive-verb-lexeme} (\textit{trans-v-lxm}) has an
\textsc{arg-st} list with both an ergative and an absolutive argument. Key to this result is that
the verb lexeme is associated with an \textit{actor-undergoer-relation} (\textit{act-und-rel}), and,
in fact, this is the value of the \textsc{key} feature in (\ref{simple erg}), encoding the
designated semantic relation relevant for case and linking (see \citealt{KD2006a-u} 
% this is the whole paper on KEY.
for more on the \textsc{key} feature). The \textit{act-und-rel} type designates semantic predicates with precisely
the denotation behind the notion of ``primary transitive verb'' (see \citealt[75--134]{Davis2001a-u}
for discussion of this type, other related types, and how these types fit into a hierarchy of
semantic relations). Provided that the constraint in (\ref{simple erg}) is the only argument
realization constraint to mention the ergative and absolutive cases, the ergative--absolutive
collection of arguments would only be available with verbs with this particular sort of
meaning.\footnote{Though to achieve more generality with the licensing of absolutive case, one might
  follow \citet[Chapter~7]{ball08thesis} and have separate linking constraints for absolutive and
  ergative case.} The overall linking constraint is placed on a \textit{trans-v-lxm}, so that (a)
the case requirements are stated once for all the different inflected forms of the verb and (b) so
these case requirements could also be inherited by other semantically appropriate verbs with even
more arguments than the two arguments that were mentioned explicitly in (\ref{simple erg}).

As alluded to above, in other case ``assignment'' situations, though, other factors beyond just the semantics of the verb can be relevant (certainly with any instance of ``\isi{split ergativity}'', among others). These could be still be treated with constraints with a similar format to (\ref{simple erg}), but if they needed to refer to, say, just past tense verb forms, the relevant linking constraint would almost assuredly need to reference information from the morphology (perhaps encoded as part of a \textsc{morph(ology)} attribute). Given the known claims about what non-accusative alignment can be sensitive to,  it seems likely that the sign-based architecture (where all linguistic areas of structure can interact in parallel) would enable the straightforward statement of case constraints based on the previously claimed generalization. And, in fact, having the possibilities of morphological form, semantics, and various syntactic properties easily available for an analysis could be useful as a means of testing and modeling which areas might be relevant in particular examples.

\largerpage[2]
Overall, there is a lot still to be done to better understand the intricate details of case and linking generally, but given the toolbox available in the HPSG framework (again, see \crossrefchapteralt{case} and \crossrefchapteralt{arg-st}), it seems like HPSG offers a lot of flexibility for better figuring out what linguistic elements are crucial for particular patterns and for encoding analyses that directly reference the interaction of these elements across different levels of structure.      

\section{Verb--subject--object constituent order}
\label{ul:sec-vso}

\is{word order!VSO|(}
Let us turn to another interesting phenomenon of understudied languages: Verb-Subject-Object (VSO) constituent order.\footnote{It would probably be clearer to refer to this order as Predicate--Agentive--Patientive order, because, as noted in the previous section, alignment and constituent order are, to a degree, disjoint. However, I will bow to tradition and use the terms verb, subject, and object and their abbreviations, V, S, and O.} VSO order appears to be the rarest of the more common orders. Various typology surveys (like \citealt{dryerWALSwordorder}) indicate that it is only found in about 8--10\% of the world's languages. Interestingly, a greater number of examples of languages with this order do come from the realm of understudied languages. Of the twelve understudied languages (a non-random sample\footnote{See footnote~\ref{list expln} for the rationale behind the choice of those twelve languages.}) mentioned in the introduction, five of them have VSO (or verb-initial with no strict ordering of S and O) order. Perhaps a bit more telling of the (apparent) understudied language bias to VSO order is that only one of the top 50 languages by native speakers -- \ili{Tagalog} -- reasonably clearly has VSO order.\footnote{\ili{Tagalog}'s distant \ili{Austronesian} relatives \ili{Indonesian}, \ili{Javanese}, and \ili{Sundanese}, along with \ili{Arabic} -- all four of these languages are also in the top 50 -- had VSO, historically, and each of these languages preserves some instances of VSO order. As is often the case when looking at word orders and languages, things are rarely as cut and dry as they might otherwise seem.} Interestingly, VSO order does occur in a number of languages as a non-dominant word order: for instance, it is found in a great many western European languages (\ili{English}, \ili{German}, \ili{French}, \ili{Spanish}, among others) as a common order in questions.
% \itdobl{This has a great potential of confusion. It takes me weeks to explain to students that German is SOV and V2. What you do here is what the WALS does and this causes confusion. The WALS simply counts patterns. This works reasonably well for most languages, but there are 15 or so V2 languages that express something by fronting the verb. This V2 stuff breaks all the counting. The Germanic languages are either SVO (Scandinavian, English) or (SOV German, Dutch, Afrikaans). V2 comes on top of this. So I would really ask you either not to list German here as language with an VSO pattern or, if you insist, point the readers to \citew[Section~2.1--2.2]{MuellerGermanic} where I discuss all this in length.} 

\largerpage[2]
In spite of its relative rarity as a dominant order, VSO order (as well as verb-initial order with flexible ordering of verbal dependents) poses some interesting challenges for frameworks that place some importance in constituency (as HPSG has done). Since the V and the O are not (normally) adjacent in VSO clauses, it is less than obvious that there is a constituent that groups them together (as a VP or a V$'$) in these languages. This contrasts with the more common Subject--Verb--Object and Subject--Object--Verb orders where a constituent that groups the V and O together is much more plausible, on surface adjacencies alone. A long-standing question across constituency-based frameworks is how to best characterize VSO order, both on its own and in the context of the other cross-linguistically attested and common order patterns.  

Analyses within Mainstream Generative Grammar have generally analyzed VSO as a derived order; all (or nearly all) of them (especially after the 1970s) have viewed VSO as a derived permutation of some constituent (or more) from a covert SVO order (see \citealt{clemenspolinsky17} for an overview of the analyses within this tradition). Some of the suggested HPSG analyses follow a similar line of analysis, and I will briefly touch on those proposals below. However, more HPSG analysts have generally taken VSO order as is, and so I spend more of this section discussing two surface-oriented VSO analyses in HPSG: what I call the flat structure analysis and what I call the binary branching head-initial analysis.  

\subsection{The analogues of verb movement in HPSG}

Interestingly, there is not one, but two styles of HPSG analyses that are roughly analogous to
Mainstream Generative Grammar's verb movement, commonly employed to derive VSO order. Both are
discussed in greater detail in \crossrefchaptert{order}, so my comments here will be somewhat
superficial and will center around verb-initiality. The first of the two uses the
\textsc{doubleslash} (\dsl{}\isfeat{dsl}) feature (see
\crossrefchapteralt[Section~\ref{sec-head-movement}]{order}) and so treats the initial verb as
involved in a special dependency that uses a mechanism for percolation of information that is
similar to the slash passing in nonlocal dependencies: information related to the initial verb is
passed through the constituent structure to the verb's downstairs position (a trace, with semantic
and syntactic structure, though no phonological realization). While this analysis has been explored
for \ili{Germanic} languages (see Figure~\ref{fig-did-kim-get-the-job-hm} in
\crossrefchapteralt{order} for a pictorial depiction of an analysis of an \ili{English} verb-initial
clause and \citew[Chapter~6]{MuellerGermanic} for an application to all Germanic V2 languages), I am
not aware that it has (yet) been seriously explored in the HPSG literature for any particular
verb-initial language (let alone for an understudied verb-initial language).

%\largerpage
The other verb-movement-like analysis uses constituent order domains and linearization (see
\crossrefchapteralt[Section~\ref{sec-domains}]{order}). On this analysis, the verb, while combined
with its complements at a low level, is constrained at the clausal level to be initial (see
\citealt{borsley06} for more discussion of this in a verb-initial context). This style of analysis
has been closely and carefully considered for \ili{Welsh} in work by Borsley (for example in
\citealt{Borsley89b,borsley95,Borsley2009a-u}), but time and again, it seems that Borsley suggests
that an analysis (at least for the basics of clausal structure) more in line with what is discussed
in Section~\ref{understudied-sec-flat} is to be preferred for \ili{Welsh}.

\largerpage
Given the rarity (and perhaps reluctance) -- noted above -- of HPSG researchers to analyze VSO order
as covertly SVO (or, more to the point, to recognize a constituent that groups together the V and O
within VSO structures), one might wonder why this has (hitherto) been so. Probably, HPSG's surface
orientation has played a role, as well as the fact that HPSG-internal considerations do not force or
strongly suggest positing a VP constituent. Furthermore, HPSG analysts have also carefully
considered how constituency tests might inform such structures. In exploring these, various HPSG
researchers (such as \citealt{borsley06} for \ili{Welsh} and \citealt[Chapter~3]{ball08thesis} for
\ili{Tongan}) have not found compelling evidence for positing a VP constituent in particular VSO
languages.\footnote{The undermotivated VP in \ili{Welsh} is probably just a VP headed by a finite
  verb, as \ili{Welsh} does give evidence for non-finite VPs \citep{BTW07}. In other languages, like
  \ili{Tongan}, the undermotivated VPs might include both finite and non-finite VPs. As has emerged
  in the study of verb-initial languages in several frameworks, these languages might not be as
  structurally uniform as the term ``verb-initial languages'' suggests.} For instance, Ball, in
looking at \ili{Tongan}, found that: putative VP-coordination ``over'' a subject is not possible; no
auxiliary or verb obviously subcategorizes for a verbal constituent that obviously excludes its
subject, nor do adverbial elements obviously\itdsecond{Lots of ``obviously''s. Just saying. Probably
  intended.} select for such a constituent; and, while ``VP-fronting'' and ``VP-ellipsis'' are
possible, they seem to involve NPs rather than VPs. While these facts do not definitively rule out a
VP (it is difficult to argue that anything is clearly absent), they suggest that not positing a VP
does not complicate the grammar of this kind of language. Undoubtedly, it would be interesting to
see what further explorations like these with more verb-initial languages might reveal. Still, the
VSO-as-covert-SVO analysis may lie on shakier grounds empirically than analyses within Mainstream
Generative Grammar have generally acknowledged and this, explicitly or implicitly, has led HPSG
analysts to explore other avenues in the analysis of VSO languages.%

\subsection{The flat structure analysis}
\label{understudied-sec-flat}

\largerpage[2]
The seemingly most common analysis of VSO languages in HPSG is the flat structure
analysis.\footnote{There are, in fact, several alternative flat structure analyses, differing
  slightly in how the head's valence features relate to the structure. Besides having the head
  combine with its subjects and complements simultaneously, as in the main text, one variant has all
  the arguments as complements and, thus, VSO order arises out of a head-complements
  structure. \citet{borsley95} suggests that different languages might utilize different variants:
  in particular, Borsley suggests that Syrian Arabic\il{Arabic!Syrian} uses the
  head-subject-complements combination while \ili{Welsh} uses the head-complements
  combination. Still other analysts (such as \citealt{ball08thesis,ball17}) assume just one valence
  feature \textsc{val} instead of \subj and \comps and are similar to the subjects as complements
  approach in combining all arguments with the head at once.} As its name suggests, the proposed structure is flat, with the verb, subject NP, and any complement NPs all being sisters within the same constituent. To license such a structure, one has to depart from rules that put just heads and complements or just heads and subjects together. The flat structure analysis instead makes use of what I call the Head-All-Valents Schema\is{schema!Head-All-Valents} (also sometimes called the Head-Subject-Complements Schema), given in (\ref{h-a-v rule}):   


\begin{samepage}
\begin{exe}
\ex \label{h-a-v rule}
Head-All-Valents Schema:\\
\emph{head-all-valents-phrase} \impl  \\
\avm{
[synsem|loc|cat & [subj & < > \\
	           comps & < >  ]   \\
 hd-dtr & [\type*{word}
           synsem|loc|cat & [subj & < \1 > \\
                             comps & < \2, \ldots, \tag{n} > ] ] \\
 non-hd-dtrs  & < [synsem \1], [synsem \2], \ldots, [synsem \tag{n}] > ]
} 
%\itdopt{Added \synsem before daughters, to keep things consistent with other chapters.}
\end{exe}
\end{samepage}

\noindent
Per its name, it licenses a fully saturated phrase comprising a head -- a single word -- and all its
valents (subject, object, and whatever else). This schema has not just been used for canonical VSO
clauses within HPSG, but other clause-level head-initial structures, including polar questions in
\ili{English}. Thus, this schema has a long pedigree in the HPSG literature (compare the schema in
(\ref{h-a-v rule}) with Schema 3 from \citealt[40]{ps2}; \textit{sai-ph} from
\citealt[36]{GSag2000a-u}; and \textit{aux-initial-cxt} from \citealt[188]{Sag2012a}).

%\largerpage 
To see an example using the Head-All-Valents Schema, let us consider example (\ref{KD}) from \ili{Kimaragang}:

\begin{samepage}
\begin{exe}
\ex \label{KD} \ili{Kimaragang} [kqr] \citep[7]{kroeger10} \\
\gll Minangalapak it kogiw do ratu.  \\
\textsc{pst.av.tr.}split \textsc{nom} orangutan \textsc{gen} durian \\
\trans `The orangutan split (open) a durian.'  
\end{exe}
\end{samepage}

\noindent
By the Head-All-Valents Schema (and appropriate inherited constraints concerning the featural identities of \textsc{head} values), a tree for (\ref{KD}) would be as in Figure~\ref{KD tree}. 
%
\begin{figure}
\centering
\begin{forest}
sm edges,
for tree={l sep=10 mm}
[%
\avm{
	[head & \1 \\
	subj & < > \\
	comps & < >  ]
}	 
	[%
	\avm{
		[head & \1 verb \\
		subj & < \2 > \\
		comps & < \3 > ]
	},  edge label={node[midway,above,outer sep=0.5 mm]{H}}
		[minangalapak;split]]
	[%
	\avm{ 
		\2 [head & noun \\
		subj & < > \\
		comps & < > ]
	}
		[it kogiw;\textsc{nom} orangtuan, roof ]]
	[%
	\avm{
		\3 [head & noun \\
		subj & < > \\
		comps & < > ]
	}
		[do ratu;\textsc{gen} durian, roof]] 
]
\end{forest}
%\caption{Tree of (\ref{KD}) in the flat structure analysis}
\caption{The flat structure analysis of Kimaragang \emph{Minangalapak it kogiw do ratu.} `The orangutan split (open) a durian.'}
\label{KD tree}
\end{figure}
%
To license the tree in Figure~\ref{KD tree}, we first should observe that the verb
\textit{minangalapak} `split' appears to require both a nominative and a genitive argument. With two
such nominal expressions fitting those requirements available, the Head-All-Valents Schema can put
all three of these elements -- the verb and two NPs -- together, and the resulting mother node's
\textsc{subj} and \textsc{comps} lists would be empty.

In spite of the flatness of Figure~\ref{KD tree}, the structure is like all head-nexus combinations
in HPSG: a head and (at least some of) its dependents. In fact, Figure~\ref{KD tree} is identical to
certain verb phrases headed by a ditransitive verb (on some HPSG analyses) -- just a verb and two
NPs. Furthermore, the flat nature of the structure is less of a concern than it would be under
c-command-based\is{c-command} proposals (which are the off-the-shelf analyses in Mainstream
Generative Grammar): binding relations in HPSG are not calculated from the configurations within the
tree, but from configurations on the \textsc{arg-st} list (see \crossrefchapteralt{binding}). Other
subject-object and agent-patient asymmetries (to the extent they exist) are likewise encoded in HPSG
analyses using non-configurational data structures and do not seem to be relevant for determining
constituency.

%\largerpage 
Additionally, assuming a flatter structure for VSO/verb-initial languages eases the analysis of
several other phenomena (especially versus a treatment of the same data with a VP constituent). In
verb-initial languages where the order of elements following the verb is flexible (as in
\ili{Tongan}, among others), having all arguments together with the verb as part of a single
constituent allows for such ``\isi{scrambling}'' to be analyzed with simple linear precedence
constraints within that constituent, rather than having to deal with different orders across a VP
boundary (see the analysis in \citealt[Chapter~3]{ball08thesis} for \ili{Tongan} and
\crossrefchapteralt{order} for other HPSG approaches to ``scrambling''). There are also a few
languages like \ili{Coast Tsimshian}, where morphological marking (somewhat surprisingly) on one
syntactic item refers to the next constituent over. An example of this phenomenon is given in the
\ili{Coast Tsimshian} sentence in (\ref{CT ex 1}), where the second line employs brackets to better
show which elements are related to which others:

%\newpage
\begin{samepage}
\begin{exe}
\ex\label{CT ex 1} \ili{Coast Tsimshian} (Sm'algy\underline{a}x) [tsi] \citep[32]{mulder94} \\
 Yagwat huumda duusa hoon. \\
\gll Yagwa-t huum-[da duus]-[a hoon] \\
 \textsc{cont}-\textsc{3.erg} smell-[\textsc{erg.cn} cat]-[\textsc{abs.cn} fish] \\
\trans `The cat is sniffing the fish.' 
\end{exe}
\end{samepage}

\noindent
It is far more straightforward to analyze the apparent sideways relationships when the interacting elements are sisters, rather than to manage the relationships across a VP boundary (and possibly other constituent boundaries) (see \citealt{ball11} for an in-depth look into this syntactic phenomenon in \ili{Coast Tsimshian} and an analysis of it).  

\subsection{The binary branching head-initial analysis} 

Another approach in HPSG to VSO structures takes the view that all verb-headed structures within the clause are maximally binary branching, but strongly head-initial. This approach still has a strong surface-orientation -- so it does not take the VSO order to be covertly SVO or SOV -- but does posit that more structure is present within a clause than on the flat structure analysis. 

On the binary branching head-initial analysis, VSO clauses are built out of several instances of a
single rule. The rule, which I call the Head-Valent Schema, is given in (\ref{h-v
  rule}):\footnote{The Head-Valent Schema here is designed to implement the Categorial Grammar
  analysis of \citet{keenan2000} in HPSG terms, and, as such, uses a single \textsc{valence} list,
  abbreviated \textsc{val}. So, for the discussion in this section, I will employ this slightly
  different feature geometry. Note that the configuration of Figure~\ref{KD tree 2} could also be
  achieved using the \textsc{subj} and \textsc{comps} lists found elsewhere in this chapter
  (although it requires two rules instead of just one). Another option would be to include the
  subjects among the complements as it is done for finite verbs in Welsh \parencites[\page 347]{Borsley89b}[\page
  117--118]{borsley95} and \ili{German} \citep[\page 295]{Pollard90a-Eng}. As has been a recurring theme throughout this chapter, many
  analyses are possible and more empirical work is needed to see which might be preferred.}  
%
\begin{exe}
\ex \label{h-v rule}
Head-Valent Schema (binary branching):\\
\type{head-valent-phrase} \impl  \\
\avm{
[synsem|loc|cat|val & \1  \\
 \punk{hd-dtr}{[synsem|loc|cat|val  < \2 > \+ \1 ]} \\
 \punk{non-hd-dtrs}{< [synsem \2 ] >} ]
}
\end{exe}
%
The rule in (\ref{h-v rule}) allows a head to combine with just one of its valents; in particular, the first one on its \textsc{val} list. This aspect of the ordering is crucial to ensure that the subject-NP-before-object-NP sequence is licensed.

Returning to the \ili{Kimaragang} example of (\ref{KD}), we can see how a structure licensed by the Head-Valent Schema in (\ref{h-v rule}) differs from a structure licensed by the Head-All-Valents Schema. The structure licensed by the Head-Valent Schema (and relevant inherited constraints) is given in Figure~\ref{KD tree 2}.

\begin{figure}
\centering
\begin{forest}
sm edges,
for tree={l sep=10 mm}
[%
\avm{
	[head & \1 \\
	val & < >  ]
} 
	[%
	\avm{
		[head & \1 \\
		val & < \3 > ]
	}, edge label={node[midway,above,outer sep=0.5 mm]{H}} 
		[%
		\avm{
			[head & \1 verb \\
			val & < \2{,} \3 > ]
		}, edge label={node[midway,above,outer sep=0.5 mm]{H}}
			[minangalapak;split ]]  
		[%
		\avm{
			\2 [head & noun \\
			val & < >  ]
		}
			[it kogiw;\textsc{nom} orangutan, roof ]]
	] 
	[%
	\avm{
		\3 [head & noun \\
		val & < >  ]
	}
		[do ratu;\textsc{gen} durian, roof]]
]
\end{forest}
%\caption{Tree of (\ref{KD}) in the binary branching head-initial analysis}
\caption{The binary branching head-initial analysis of the Kimaragang example in (\ref{KD}) \emph{Minangalapak it kogiw do ratu.} `The orangutan split (open) a durian.'}
\label{KD tree 2}
\end{figure}
%
Like in Figure~\ref{KD tree}, in Figure~\ref{KD tree 2}, the head verb requires a nominative and a genitive argument. However, instead of combining with both of these at the same time, the verb just combines with the initial nominative argument (\ibox{2}), leaving the genitive argument (\ibox{3}) to be passed up to the mother. At this second level of structure, the Head-Valent Schema again applies -- because there is still at least one element on the relevant head's \textsc{val} list -- integrating \ibox{3} into the structure. The rule is barred, correctly, from applying to the root node of the tree in Figure~\ref{KD tree 2}, as this root node has an empty \textsc{val} list and the Head-Valent Schema requires the head daughter to have at least one valent.   
 
 A noteworthy feature of the VSO binary branching head-initial analysis is its grouping of verb and the subject NP into a constituent. Exactly this sort of thing has been reported to occur in some verb-initial languages, like \ili{Malagasy} \citep{keenan2000}, suggesting the binary branching head-initial analysis might be preferable for such languages. In VSO languages without such evidence, it would seem that either the flat structure analysis or the binary branching head-initial analysis would be possible, all else being equal.  
 
If one is accustomed to seeing the trees from Mainstream Generative Grammar, the structure in Figure~\ref{KD tree 2} may still seem strange (notably, the structural prominence relationships between what seems to be the subject NP and what seems to be the object NP are reversed). Nevertheless, many of the same kinds of comments made for the flat structure analysis hold here as well. The structure in Figure~\ref{KD tree 2} is a just a series of head-argument structures, the most common kind of structure in HPSG. And, once again, the non-configurational approach to binding in HPSG renders any issues related to tree configuration and binding as irrelevant.    

Both approaches to VSO order discussed above do raise interesting questions about whether there are any underlying grammatical principles, processing preferences, or historically-driven outcomes behind the patterns. For the binary branching head-initial analysis, there is a question as to why the required order of combination goes from from least oblique to most oblique. A similar set of question can be leveled to the flat structure analysis: what inhibits a more constituent-rich structure? Why are flat structures licensed here and not elsewhere? To my knowledge, these questions have yet to be tackled within the HPSG literature, but they do seem to be reasonable next steps, in addition to better seeing which analyses are appropriate for which verb-initial languages. 
\is{word order!VSO|)}
	
\section{Wrapping up}

In general, HPSG practitioners have been fairly conservative in what they assume to be universal in
syntax: since there is no core assumption in HPSG that particular rich, innate, and universal class
of structures help children learn any language (Mainstream Generative Grammar's Universal
Grammar\indexug), proposals can be (and are) made that are agnostic as to universality. Even so, the
brief trip made in this chapter through argument indexing, non-accusative alignments, and
verb-initial constituent order found in understudied languages reveals that the more
\isi{dependency}-oriented portions of the framework -- in particular, the areas encoded in the
\textsc{subj}, \textsc{comps}, and \textsc{arg-st} lists -- are useful for the analysis of all three
of these areas, across different languages, and, thus, are candidates for universality\footnote{Or
  in the case of the \textsc{subj} and \textsc{comps} lists, a candidate for near-universality, as
  \citet{KM15} argue that \ili{Oneida} does not require such lists.} (though the current level of
understanding does not clearly point to them originating from either a rich language-specific part
of cognition or from general cognition). Furthermore, the explorations above show that the rich and
precise modeling using attribute-value matrices also allows for uniform sorts of analyses, even
though the details may differ.\footnote{Two projects within the HPSG community have explored
  in-depth how uniform particular HPSG analyses of different languages might be. The Grammar Matrix
  project \citep{BDFPS2010a-u} just starts from a common core and adds language-specific elements as
  needed; the CoreGram project \citep{MuellerCoreGram} actively tries to use the same sorts of data
  structures for as many languages as possible within the project. Both projects develop
  computer-processable grammars. For more on these projects and the relation between HPSG and
  computational linguistics in general see \crossrefchapterw{cl}.} While the precise attributes and
feature values may not completely be candidates for universality, they certainly aid in the
enterprise of exploring different analyses and determining what precisely must be said to capture
certain linguistic phenomena.

In addition to revealing some of the more uniform aspects of HPSG, the above discussion also reveals a certain flexibility in how the framework can be deployed -- several analyses might be possible and certain ones might be more appropriate for certain languages and not for others. Thus, on top of a uniform foundation, various languages and phenomena are open to be analyzed in their own terms, dependent on what the specific empirical facts reveal. This mesh of uniformity and parochiality in HPSG analyses seems to strike a good balance as grammarians try to capture the two (somewhat paradoxical) realities one finds when comparing across languages: languages are both surprisingly similar and surprisingly different.                 

\section*{\acknowledgmentsUS}

Thanks to Stefan Müller, Robert Borsley, and Emily Bender for comments on this chapter. I alone am responsible for any remaining shortcomings. Thanks also to the editors, especially to Jean-Pierre Koenig (who served as the editors' in-person representative when I was first approached to write this chapter), for suggesting that a chapter like this one exist in this handbook.  


{\sloppy

\printbibliography[heading=subbibliography,notkeyword=this]
}

\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->
