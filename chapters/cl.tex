\documentclass[output=paper,nonflat]{langsci/langscibook} 
\author{%
	Emily M.\ Bender\affiliation{University of Washington}%
	\lastand Guy Emerson\affiliation{University of Cambridge}
}
\title{Computational linguistics and grammar engineering}

% \chapterDOI{} %will be filled in at production

\abstract{%
	We discuss the relevance of HPSG for computational linguistics,
	and the relevance of computational linguistics for HPSG.
	% TODO
}



\IfFileExists{../localcommands.tex}{%hack to check whether this is being
compiled as part of a collection or standalone
  \input{../localpackages}
  \input{../localcommands}
  \togglepaper
}{}




\begin{document}

\maketitle

\label{chap-cl}

% Comments from Gerald:

%I assume that the contributors would agree with me that this handbook is probably the wrong place to go into great technical detail on mathematical or computational subjects, but there needs to be at least something there to serve as a starting point from which the reader could choose to jump into some of the papers cited.‎ A summary of the major trends and the high-level intuitions behind a lot of the technical terminology that readers will encounter in HPSG-related CL papers, for example, would both be most welcome.

\section{Introduction}

% First draft: Emily

From the inception of HPSG in the 1980s,
there has been a close integration between theoretical and computational work
\crossrefchapterp[for an overview, see]{evolution}.
%\citep{FIXME-CLobit-or-other}.
% at HP Labs in the joint work of Ivan Sag, Geoff Pullum, Tom Wasow, Mark Gawron, Carl Pollard and Dan Flickinger
% GE: I think this list of authors is too long for a first sentence.  Do we need to do more than refer to the other chapter?
% EMB 2018-09-28 sounds good!
In this chapter, we discuss computational work in HPSG,
starting with the infrastructure that supports it (both theoretical and practical) in Section~\ref{cl:infrastructure}.
Next we describe several existing large-scale projects which build HPSG or HPSG-inspired grammars
(see Section~\ref{cl:resources})
and the deployment of such grammars in applications including both those within linguistic research and otherwise
(see Section~\ref{cl:deployment}).
Finally, we turn to linguistic insights gleaned from broad-coverage grammar development
(see Section~\ref{cl:insight}).

% EMB 2018-07-26 That seems like a relatively weak intro, but putting it in as a place-holder for now. 

% EMB 2018-07-26 Also, I wonder if ``lessons for linguistics'' might not come across as condescending...
% GE 2018-07-31 Yes, maybe best to reword that somehow... ``Linguistic insights''?
% EMB 2018-09-28 Thanks.

\section{Infrastructure}
\label{cl:infrastructure}

% First draft: By subsection

\subsection{Theoretical considerations}
\label{cl:theoretical}

There are several properties of HPSG as a theory that make it well-suited to computational implementation. First, the theory is kept separate from the formalism: the formalism is expressive enough to encode a wide variety of possible theories. While some theoretical work does argue for or against the necessity of particular formal devices (e.g.\ the shuffle operator \citep{FIXME-Reape}), much of it proceeds within shared assumptions about the formalism. This is in contrast to work in the context of the Minimalist Program \citep{Chomsky93b-u}, where theoretical results are typically couched in terms of modifications to the formalism itself. From a computational point of view, the benefit of differentiating between theory and formalism is that it means that the formalism is relatively stable. That in turns enables the development and maintenance of software systems that target the formalism, for parsing, generation, and grammar exploration (see Section~\ref{cl:history} below for some examples).\footnote{There are implementations of Minimalism, notably \citet{FIXME-Stabler} and \citet{FIXME-Indianadiss}. However, writing an implementation requires fixing the formalism, and so these are unlikely to be useful for testing theoretical ideas as the theory moves on.}

A second important property of HPSG that supports a strong connection between theoretical and computational work is an interest in both so-called `core' and so-called `peripheral' phenomena. Most implemented grammars are built with the goal of handling naturally occurring text.\footnote{It is possible, but less common, to do implementation work strictly against testsuites of sentences constructed specifically to focus on phenomena of interest.} This means that they will need to handle a wide variety of linguistic phenomena not always treated in theoretical syntactic work \citep{FIXME-Baldwin-et-al-Beauty}. A syntactic framework that excludes research on `peripheral' phenomena as uninteresting provides less support for implementational work than does one, like HPSG or Construction Grammar, that values such topics
\crossrefchapterp[for a comparison of HPSG and Construction Grammar, see]{cxg}.

Finally, the type hierarchy characteristic of HPSG lends itself well to developing broad-coverage grammars which are maintainable over time \citep{FIXME-find-cite?}. The use of the type hierarchy to manage complexity at scale comes out of the work of \citet{Flickinger87} and others at HP labs in the project where HPSG was originally developed. The core idea is that any given constraint is (ideally) expressed only once on types which serve as supertypes to all entities that bear that constraint.\footnote{Originally this only applied to lexical entries in Flickinger's work. Now it also applies phrase structure rules, lexical rules, and types below the level of the sign which are used in the definition of all of these.} Such constraints might represent broad generalizations that apply to many entities or relatively narrow, indiosyncratic properties. By isolating any given constraint on one type (as opposed to repeating it in mutiple places), we build grammars that are easier to update and adapt in light of new data that require refinements to constraints. Having a single locus for each constraint also makes the types a very useful target for documentation \citep{FIXME:LTDB} and grammar exploration \citep{FIXME:typediff}. 


\subsection{Practical considerations}
\label{cl:prac}

The formalism of HPSG allows practical implementations,
since feature structures are well-defined data structures.
Furthermore, because HPSG is defined to be bi-directional,
an implemented grammar can be used for both parsing and generation.
In this section, we discuss how HPSG allows tractable algorithms,
which enables linguists to empirically test hypotheses,
and which also enables HPSG grammars to be used in a range of applications,
as we will see in Sections~\ref{cl:lang-doc} and~\ref{cl:downstream}, respectively.

\subsubsection{Computational complexity}
\label{cl:prac:comp}

One way to measure how easy or difficult it is to use a syntactic theory in practical computational applications
is to consider the \textit{computational complexity} of parsing and generation algorithms \cite{FIXME-Hopcroft-and-Ullman?}.
For example, we can consider how much computational time
a parsing algorithm needs to process a particular sentence.
For longer sentences, we would expect the amount of time to increase,
but the more complex the algorithm is,
the more quickly the amount of time increases.
If we consider sentences containing $n$~tokens,
we can find the average amount of time taken,
or the longest amount of time taken.
We can then increase~$n$, and see how the amount of time changes,
both in the average case, and in the worst case.

At first sight, analysing computational complexity
would seem to paint HPSG in a bad light,
because the formalism allows us to write grammars
which can be arbitrarily complex;
this means that the formalism can be called \textit{Turing-complete}
% EMB 2018-09-28 ``can be called'' is a bit odd here. Either it is or it isn't right?
\citep[Section~3.4]{Johnson88}.
However, as discussed in the previous section,
there is a clear distinction between theory and formalism.
Although the feature-structure formalism rules out the possibility of efficient algorithms
that could cope with any possible feature-structure grammar,
a particular theory (or a particular grammar) might well allow efficient algorithms.

The difference between theory and formalism
becomes clear when comparing HPSG to other computationally-friendly frameworks,
such as Combinatory Categorial Grammar (CCG),\footnote{%
	For an introduction, see \citet{steedman2011ccg}.
	For a comparison with HPSG, see \crossrefchaptert{cg}.
}
or Tree Adjoining Grammar \citep[TAG;][]{Joshi87a-u,SAJ88a-u}).
The formalisms of CCG and TAG inherently limit computational complexity:
for both of them, as the sentence length~$n$ increases,
worst-case parsing time is proportional to~$n^6$ \citep{Kasamietal1989}.
This is a deliberate feature of these formalisms,
which aim to be just expressive enough to capture human language,
and not any more expressive.
Building this kind of constraint into the formalism itself
highlights a different school of thought from HPSG.
Indeed, \citet{MuellerCoreGram} explicitly argues
in favor of developing linguistic analyses first,
and improving processing efficiency second.
As discussed above in Section~\ref{cl:theoretical},
separating the formalism from the theory
means that the formalism is stable, even as the theory develops.

% EMB 2018-09-28 Say something here about particular analytical choices that constrain the processing complexity? (The bit about the fact that the lists that we read from tend to be limited in length.)
% EMB 2018-09-28 Maybe also cite Flickinger 2000 (`On building a more efficient grammar using types') and Copestake et al work on improving generator efficiency.

It would beyond the scope of this chapter
to give a full review of parsing algorithms,
but it is instructive to give an example.
For grammars that have a context-free backbone
(we can express every derivation as a phrase-structure tree
plus constraints between mother and daughter nodes),
it is possible to adapt the standard chart-parsing algorithm for context-free grammars.
The basic idea is to parse ``bottom-up'' through the tree,
starting by finding analyses for each token in the input,
and then finding analyses for increasingly longer sequences of tokens,
until we reach the entire sentence.
The resulting algorithm is more computationally complex than for a context-free grammar
because we are dealing with feature structures, rather than nonterminal symbols.
While a context-free grammar allows a finite number of nonterminals,
a feature-structure grammar may allow a infinite number of possible feature structures.
For an HPSG grammar without recursive unary rules,
this algorithm has a worst-case complexity of exponential time.
This is less complex than for an arbitrary grammar
(which means that this class of grammars is \emph{not} Turing-complete),
but more complex than for CCG or TAG.
However, when parsing real corpora,
it turns out that the average-case complexity is much better than we might expect.
On the one hand, grammatical constructions do not generally combine in the worst-case way,
and on the other hand, when a grammar writer is confronted
with multiple possible analyses for a particular construction,
they may opt for the analysis that is more efficient for a particular parsing algorithm.
% TODO citations -- maybe look at evolution.tex, {Implementations and Applications of HPSG}
% GE: Maybe this last paragraph is too terse?
% EMB 2018-09-28: Yes, a little terse --- I think maybe starting with how the derivations are like and unlike CFG derivations and then going to `bottom-up' chart parsing (with citations!) would help. Also, the discussion of complexity has too many contrasts: infinite, but not Turing-complete, but not even that bad when parsing real corpora. It would also be good to point to some numbers for ``parsing real corpora'' ... Dan or Stephan or maybe Bec should have published some, I think.



\subsubsection{Parse ranking}
\label{cl:prac:rank}

For an ambiguous sentence,
a grammar gives multiple valid parses.
This is widely known, with attachment ambiguities and coordination ambiguities
being particularly well-known examples.
However, people are naturally very good at resolving ambiguity,
which means most ambiguity is not apparent, even to linguists.
It is only with the development of large-scale grammars that the sheer scale of ambiguity has become clear.
% EMB 2018-10-03 What's rref?
For example, \rref{cl:ex:ambig-simple}~might seem unambiguous,
but there is a second reading, where \textit{my favorite} is the topicalized object of \textit{speak},
which would mean that town criers generally speak the speaker's favorite thing (perhaps a language) clearly.
There is also a third, even more implausible reading, where \textit{my favorite town} is the topicalized object.
Such implausible readings don't easily come to mind,
but with increasingly long sentences, such ambiguities stack up very quickly.
For~\rref{cl:ex:ambig-many}, the first line of a newspaper article,\footnote{%
	\url{https://www.theguardian.com/science/2018/aug/22/offspring-of-neanderthal-and-denisovan-identified-for-first-time}
}
the 1214 version of the English Resource Grammar \citep{Flickinger2000a,Flickinger2011a-u}
gives TODO readings.

% EMB 2018-09-28 We should have the 2018 version to play with soon :)

\begin{exe}
\ex\label{cl:ex:ambig-simple}
My favorite town criers speak clearly.
\ex\label{cl:ex:ambig-many}
A small piece of bone found in a cave in Siberia has been identified
as the remnant of a child whose mother was a Neanderthal and father was a Denisovan,
a mysterious human ancestor that lived in the region.
\end{exe}

% EMB 2018-09-28 Editing a bit through here:
Where exploring ambiguity can be interesting for a linguist, typical practical applications require just one parse per input sentence and specifically the parse the best reflects the intended meaning, or only the top few parses (in case the one put forward as `best' might be wrong).
Thus what is required is a \textit{ranking} of the parses,
so that the application only uses the most highly-ranked parse,
or the top~$N$ parses.
Parse ranking is not usually determined by the grammar itself,
because of the difficulty of manually writing disambiguation rules.
Typically, a statistical system is used \citep{OFTM2004a-u-platte}.
First, a corpus is \textit{treebanked}:
for each sentence in the corpus,
an annotator (often the grammar writer) chooses the best parse,
out of all parses produced by the grammar.
The set of all parses for a sentence is often referred to as the \textit{parse forest},
and the selected best parse is often referred to as the \textit{gold standard}.
Given the gold parses for the whole corpus, a statistical system is trained
to predict the gold parse from a parse forest,
based on many features\footnote{%
	In the machine learning sense of ``feature'',
	not the feature-structure sense.
}
of the parse.
From the example in~\rref{cl:ex:ambig-simple},
we can see how a number of a different features all influence the preferred interpretation:
the likelihood of a construction (such as topicalization),
the likelihood of a valence frame (such as transitive \textit{speak}),
the likelihood of a collocation (such as \textit{town crier}),
the likelihood of a semantic relation (such as speaking a town),
and so on.
%TODO briefly explain MaxEnt parse ranking?
%TODO references...
% EMB 2018-09-28 For DELPH-IN/Redwoods treebanking: 

%% @inproceedings{Tou:Man:Oep:02,
%%   author = {Kristina Toutanova and Chris Manning and Stephan Oepen},
%%   title = {Parse Ranking for a Rich {HPSG} Grammar},
%%   booktitle = {Proceedings of The First Workshop on Treebanks and
%%                 Linguistic Theories (TLT2002)},
%%   address = {Sozopol, Bulgaria},
%%   year = 2002
%% }

%% @article{Tou:Man:Fli:Oep:05,
%%   title={Stochastic {HPSG} Parse Disambiguation using the {R}edwoods Corpus},
%%   author={Toutanova, Kristina and Manning, Christopher D. and Flickinger, Dan and Oepen, Stephan},
%%   journal={Research on Language \& Computation},
%%   volume={3},
%%   number={1},
%%   pages={83--105},
%%   year={2005},
%%   publisher={Springer}
%% }

%% @article{Oep:Fli:98,
%%   author = {Stephan Oepen and Daniel P. Flickinger},
%%   title = {Towards Systematic Grammar Profiling.
%%            {T}est Suite Technology Ten Years After},
%%   editor = {Robert Gaizauskas},
%%   journal = {Journal of Computer Speech and Language},
%%   volume = {12 (4) (Special Issue on Evaluation)},
%%   year = 1998,
%%   pages = {411$\,$--$\,$436}
%% }

%% @Inbook{Fli:Oep:Ben:17,
%%   author="Flickinger, Dan and Oepen, Stephan and Bender, Emily M.",
%%   editor="Ide, Nancy and Pustejovsky, James",
%%   title="Sustainable Development and Refinement of Complex Linguistic Annotations at Scale",
%%   bookTitle="Handbook of Linguistic Annotation",
%%   year="2017",
%%   publisher="Springer Netherlands",
%%   address="Dordrecht",
%%   pages="353--377",
%%   isbn="978-94-024-0881-2",
%%   doi="10.1007/978-94-024-0881-2_14",
%%   url="http://dx.doi.org/10.1007/978-94-024-0881-2_14"
%% }

%% % And for the idea  of discrimimants in treebanking:

%% @inproceedings{Carter:97,
%%   author = {David Carter},
%%   title = {The {T}ree{B}anker.
%%            {A} Tool for Supervised Training of Parsed Corpora},
%%   editors = {Dominique Estival and Alberto Lavelli 
%%              and Klaus Netter and Fabio Pianesi},
%%   booktitle = ENVGRAM:97,
%%   address = {Madrid, Spain},
%%   pages = {9--15},
%%   year = 1997
%% }




Because of the large number of possible parses,
it can be helpful to \textit{prune} the search space:
rather than ranking the full set of parses,
we can restrict attention to a smaller set of parses,
which hopefully includes the correct parse.
By carefully choosing how to restrict our attention,
we can drastically reduce processing time without hurting parsing accuracy.
One method, called \textit{supertagging},\footnote{%
	The name refers to \textit{part-of-speech tagging},
	which predicts a part-of-speech for each input token,
	from a relatively small set of part-of-speech tags.
	Supertagging is ``super'', in that it predicts detailed lexical entries, rather than simple tags.
}
exploits the fact that HPSG is a lexicalized theory: Choosing the correct lexical entry brings in rich information that can be exploited to rule out many possible parses.
Thus if the correct lexical entry can be chosen prior to parsing (e.g.\ on the basis of the prior and following words), the range of possible analyses the parser must consider is drastically reduced.
Although there is a chance that the supertagger will predict the wrong lexical entry,
using a supertagger can often improve parsing accuracy,
by ruling out parses that the parse-ranking model might incorrectly rank too high.
Supertagging was first applied to HPSG by \citet{matsuzaki2007supertag},
building on previous work for TAG \citep{bangalore1999supertag}
and CCG \citep{clark2004supertag}.
To allow multi-word expressions (such as \textit{by and large}),
where the grammar assigns a single lexical entry to multiple tokens,
\citet{dridan2013ubertag} has proposed an extension of supertagging, called \textit{ubertagging},
which jointly predicts both a segmentation of the input and supertags for those segments.
\citeauthor{dridan2013ubertag} manages to increase parsing speed by a factor of four,
while also improving parsing accuracy.

% EMB 2018-09-28 Doesn't Bec also bring in the lexical rules? Maybe that's too much detail for here.

\subsubsection{Semantic dependencies}
\label{cl:prac:dep}

%TODO diagrams
% - feature-structure MRS
% - nice MRS
% - DMRS
% - DM
% ``the cherry tree blossomed''

\begin{figure}
\centering
...
\caption{Comparison of MRS, DMRS, and DM.}
\label{cl:fig:dep}
\end{figure}

In practical applications of HPSG grammars,
the full derivation trees and the full feature structures are often unwieldy,
containing far more information than necessary for the task at hand.
It is therefore often desirable to extract a concise semantic representation.
%TODO check the Semantics chapter once it's written

In computational linguistics,
a popular approach to semantics is to represent the meaning of a sentence as a \textit{dependency graph},
as this enables the use of graph-based algorithms.\footnote{%
	In this section, we are concerned with \emph{semantic} dependencies.
	For \emph{syntactic} dependencies, see \crossrefchaptert{dg}.
}
Several types of dependency graph have been proposed
based on Minimal Recursion Semantics \citep[MRS;][]{CFPS2005a},
with varying levels of simplification.
The most expressive is Dependency Minimal Recursion Semantics \citep[DMRS;][]{copestake2009dmrs},
which is fully interconvertible with MRS.\footnote{%
	More precisely, there is a one-to-one correspondence between DMRS and MRS structures,
	if every predicate as a unique \textit{intrinsic variable}.
	As observed by \citet{oepen2006eds}, this allows a variable-free semantic representation,
	by replacing each reference to a variable with a reference to the corresponding predicate.
}
In contrast, Elementary Dependency Structures \citep[EDS;][]{oepen2006eds}
lose some scope information,
which, for many applications, is less important than predicate-argument structure.
Finally, Delph-in MRS Dependencies \citep[DM;][]{ivanova2012dm}
express predicate-argument structure purely in terms of the surface tokens,
without introducing any abstract predicates.
A comparison of MRS, DMRS, and DM is given in Figure~\ref{cl:fig:dep}.
The existence of such dependency graphs
has made it easier to use HPSG grammars in a number of practical tasks,
as we will see in Section~\ref{cl:downstream}.



\subsection{A brief history of HPSG grammar engineering}
\label{cl:history}

History: PAGE, VerbMobil, ?? % First draft: Emily

% GE: no need to repeat what Flickinger et al. cover in their chapter
% but probably a good idea to contrast grammar engineering and linguistic theorizing

Current platforms:
    \begin{itemize}
    \item LKB/ACE/PET/Agree
    \item Trale
    \item Other
    \end{itemize}


\section{Development of HPSG resources}
\label{cl:resources}

% First draft: DELPH-IN, Emily / non-DELPH-IN, Guy

\begin{itemize}
 \item  CoreGram
 \item  DELPH-IN consortium
    \begin{itemize}
    \item ERG
    \item Other large-ish grammars
    \item Grammar Matrix
    \end{itemize}
 \item Systems inspired by HPSG:
   \begin{itemize}
     \item Alpino
     \item Enju
   \end{itemize}
\end{itemize}

% Alpino cites from Gertjan:

%% Gosse Bouma, Gertjan van Noord, Robert Malouf. Alpino: Wide Coverage Computational Analysis of Dutch. In: Computational Linguistics in the Netherlands CLIN 2000.

%% Leonoor van der Beek, Gosse Bouma, Gertjan van Noord. Een brede computationele grammatica voor het Nederlands. Nederlandse Taalkunde, jaargang 7, 2002-4. [in Dutch]. 353--374. 

%% Gertjan van Noord. At Last Parsing Is Now Operational. In: Piet Mertens, Cedrick Fairon, Anne Dister, Patrick Watrin (editors): TALN06. Verbum Ex Machina. Actes de la 13e conference sur le traitement automatique des langues naturelles. Page 20--42.

%% Gertjan van Noord. Self-trained Bilexical Preferences to Improve Disambiguation Accuracy. In: Harry Bunt, Paola Merlo and Joakim Nivre (editors), Trends in Parsing Technology. Dependency Parsing, Domain Adaptation, and Deep Parsing. Springer Verlag. pp 183-200. 2010.

%% Barbara Plank and Gertjan van Noord. Dutch Dependency Parser Performance Across Domains. In: Proceedings of the 20th Meeting of Computational Linguistics in the Netherlands. 

%% Daniël de Kok and Barbara Plank and Gertjan van Noord. Reversible Stochastic Attribute-value Grammars. In: ACL 2011.

%% Gertjan van Noord, Robert Malouf. Wide Coverage Parsing with Stochastic Attribute Value Grammars. Draft. Improved version of:

%% Robert Malouf, Gertjan van Noord. Wide Coverage Parsing with Stochastic Attribute Value Grammars. In: IJCNLP-04 Workshop Beyond Shallow Analyses - Formalisms and statistical modeling for deep analyses. 

%% all are available from http://www.let.rug.nl/vannoord/papers/

\section{Deployment of HPSG resources}
\label{cl:deployment}

\subsection{Language documentation and linguistic hypothesis testing}
\label{cl:lang-doc}

Deployment for linguistic goals.

% King (2016) ``Theoretical linguistics and grammar engineering as mutually constraining disciplines''
% http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2016/headlex2016-king.pdf

\subsubsection{CoreGram}
% Guy

\subsubsection{Grammar Matrix}
% Emily

\subsubsection{AGGREGATION}
% Emily

\subsubsection{Derived resources: Redwoods-style treebanks}
%Emily


\subsection{Downstream applications}
\label{cl:downstream}
%Guy

Deployment for other tasks.
Large number of applications.
Focus on several important applications below.
See also \url{http://moin.delph-in.net/DelphinApplications}.


\subsubsection{Language teaching}

Redbird (McGraw-Hill)


\subsubsection{NLP tasks}

Information extraction

Summarisation

Machine translation



\subsubsection{Data for machine learning}

Unlike the previous sections,
not providing analyses, but providing data.
Not used at test time, only at training time.

Training deep learning systems
-- semantic parsing
-- skip over HPSG, go straight to semantic representations

Evaluation of deep learning systems
-- ShapeWorld
-- use a grammar to produce annotations


\subsection{Other?}

Alpino?

Enju?



\section{Linguistic Insights}
\label{cl:insight}

\begin{itemize}
    \item Ambiguity % Guy
    \item Long-tail phenomena (raising and control?) % Emily
    \item Scaling up (thematic roles) % Emily
    \item CLIMB methodology % Emily
\end{itemize}

\section{Summary}

\section*{Abbreviations}
\section*{Acknowledgements}

\printbibliography[heading=subbibliography,notkeyword=this] 
\end{document}
