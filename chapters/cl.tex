\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.5599868}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[26]
}{}




\author{Emily M.\ Bender\orcid{0000-0001-5384-6227}\affiliation{University of Washington} and Guy Emerson\orcid{0000-0002-3136-9682}\affiliation{University of Cambridge}}

\title{Computational linguistics and grammar engineering}


\abstract{We discuss the relevance of HPSG for computational linguistics, and the relevance of computational linguistics for HPSG, including: the theoretical and computational infrastructure required to carry out computational studies with HPSG; computational resources developed within HPSG; how those resources are deployed, for both practical applications and linguistic research; and finally, a sampling of linguistic insights achieved through HPSG-based computational linguistic research.}







\begin{document}

\maketitle

\label{chap-cl}

\section{Introduction}

%\largerpage
From the inception of HPSG in the 1980s, there has been a close integration between theoretical and
computational work (for an overview, see \crossrefchapteralt{evolution}).  In this chapter, we
discuss computational work in HPSG, starting with the infrastructure that supports it (both
theoretical and practical) in Section~\ref{cl:infrastructure}.  Next we describe several existing
large-scale projects which build HPSG or HPSG-inspired grammars (see Section~\ref{cl:resources}) and
the deployment of such grammars in applications including both those within linguistic research and
otherwise (see Section~\ref{cl:deployment}).  Finally, we turn to linguistic insights gleaned from
broad-coverage grammar development (see Section~\ref{cl:insight}).

\section{Infrastructure}
\label{cl:infrastructure}

\subsection{Theoretical considerations}
\label{cl:theoretical}

There are several properties of HPSG as a theory that make it well-suited to computational implementation.
First, the theory is kept separate from the formalism:
the formalism is expressive enough to encode a wide variety of possible theories.
While some theoretical work does argue for or against the necessity of particular formal devices
(\eg\ the \isi{shuffle} operator; \citealt{Reape94a}),
much of it proceeds within shared assumptions about the formalism.
This is in contrast to work in the context of the Minimalist Program\is{Minimalism} \citep{Chomsky95a-u},
where theoretical results are typically couched in terms of modifications to the formalism itself.
From a computational point of view, the benefit of differentiating between theory and formalism\is{formalism!differentiating from theory}
is that the formalism is relatively stable.
That enables the development and maintenance of software systems that target the formalism \citep{boguraev1988software},
such as software for \isi{parsing}, \isi{generation}, and \isi{grammar exploration}
(see Section~\ref{cl:resources} below for some examples).\footnote{%
	There are implementations of Minimalism, notably \citet{Stabler97a-u} and \citet{Herring:16}.
	Most recently, \citet{Torr2019a-u} developed a broad-coverage, \isi{treebank}-trained Minimalist parser.
	However, implementing a theory requires fixing the formalism,
	and so these implementations are unlikely to be useful for testing theoretical ideas
	if the formalism moves on.
	See \crossrefchaptert[Section~2.1]{minimalism} for further discussion.
}

A second important property of HPSG that supports a strong connection between theoretical and
computational work is an interest in both so-called ``core\is{core grammar}'' and so-called ``peripheral''\is{periphery} phenomena. Most implemented grammars are built with the goal of handling naturally occurring text.\footnote{It is possible, but less common, to do implementation work strictly against test suites of sentences constructed specifically to focus on phenomena of interest.} This means that they will need to handle a wide variety of linguistic phenomena not always treated in theoretical syntactic work \citep{Baldwin-et-al-05}. A syntactic framework that discounts research on ``peripheral'' phenomena as uninteresting provides less support for implementational work than does one, like HPSG or Construction Grammar\indexcxg, that values such topics (for a comparison of HPSG and Construction Grammar, see \crossrefchapteralt{cxg}).

\largerpage
Finally, the \isi{type hierarchy} characteristic of HPSG lends itself well
to developing broad-coverage grammars which are
maintainable\is{maintainability!of grammars} over time (see \citealt{Syg:Win:11}).
The use of the type hierarchy to manage complexity\is{complexity!of grammars} at scale
comes out of the project at \isi{HP Labs} where HPSG was originally developed
\citep{FPW85a,Flickinger87}.
The core idea is that any given constraint is (ideally) expressed only once,
on a type which serves as a supertype to all entities that bear that constraint.\footnote{%
    Originally this only applied to lexical\is{lexical entry} entries in Flickinger's work.
    Now it also applies to \isi{phrase structure rule}s, \isi{lexical rule}s,
    and types below the level of the sign which are used in the definition of all of these.
    See \crossrefchaptert[Section~\ref{evolution:sec-theoretical-developments}]{evolution}
    for further discussion.
}
Such constraints might represent broad generalizations that apply to many entities
or relatively narrow, idiosyncratic properties that apply to only a few.
By isolating any given constraint on one type (as opposed to repeating it in multiple places),
we build grammars that are easier to update and adapt in light of
new data that require refinements to constraints.
Having a single locus for each constraint also makes the types a very useful target
for documentation\is{documentation!of grammars} \citep{Hashimoto-etal:07}
and grammar exploration \citep{Letcher:18}. 


\subsection{Practical considerations}
\label{cl:prac}

\largerpage
HPSG allows practical implementations because it uses a well-defined formalism.\footnote{\label{cl:fn-feature-structure}%
    See \crossrefchapterw{formal-background} for further discussion.
    To clarify a potentially confusing terminological point,
    much theoretical work in HPSG, including \citet{ps2},
    distinguishes between fully resolved feature structures
    and possibly underspecified feature structure descriptions.\is{underspecification}\is{feature structure description}
    Much computational work, by contrast, operates entirely with partially specified feature structures,
    at both the level of grammar and the level of analyses licensed by the grammar.
    In keeping with this tradition, we use the term
    ``feature structure'' to refer to both fully specified and partially specified objects,
    and have no need for the term ``feature structure description''.
}
Furthermore, because HPSG is defined to be bi-directional,\is{bi-directionality}
an implemented grammar can be used for both \isi{parsing} and \isi{generation}.
In this section, we discuss how HPSG allows tractable algorithms,
which enables linguists to empirically test hypotheses
and which also enables HPSG grammars to be used in a range of applications,
as we will see in Sections~\ref{cl:lang-doc} and~\ref{cl:downstream}, respectively.

\subsubsection{Computational complexity}
\label{cl:prac:comp}

\is{complexity!computational|(}

One way to measure how easy or difficult it is
to use a syntactic theory in practical computational applications
is to consider the \textit{computational complexity}\footnote{%
	Computational complexity is related to
	the complexity hierarchy of language classes in \isi{formal language theory}.
	More complex language classes tend to require
	parsing and generation algorithms with higher computational complexity,
	as illustrated by the Chomsky Hierarchy \citep{Chomsky63a-u,hopcroft1969automata}
	and the Weir Hierarchy \citep{weir1992hierarchy}.
	However, this relationship is not exact.
	For example, the class of strictly local languages is a proper subset of the class of regular languages,
	but both classes can be parsed in linear time \citep{jaeger2012hierarchy}.
	Similarly, there are proper supersets of the class of context-free languages
	which do not require additional computational complexity \citep{boullier1999cubic}.
	\citet[Chapter 17]{MuellerGT-Eng3} discusses HPSG from the point of view of formal language theory.
}
of parsing and generation algorithms \citep{gazdar1985complexity}.
Computational complexity includes both how much memory
% The technical term should be `space complexity',
% but since we aren't discussing that here,
% it doesn't make sense to introduce the terminology.
and how much computational time\is{complexity!time}
a \isi{parsing algorithm} needs to process a particular sentence.\footnote{% 
  In this section, we only consider parsing
  algorithms, but a similar analysis can be done for generation
  \citep[\eg][]{carroll1999generate}.}
Considering parsing time, longer sentences will take longer to process,
but the more complex the algorithm is,
the more quickly the amount of processing time increases.
Parsing complexity can thus be measured by considering
sentences containing $n$~\isi{token}s,\is{sentence length}
and then increasing~$n$ to see how the amount of time changes.
This can be done based on the average amount of time
for sentences in a corpus (average-case complexity),\is{complexity!average-case}
or based on the longest amount of time
for all theoretically possible sentences (worst-case complexity).\is{complexity!worst-case}

At first sight, analyzing computational complexity
would seem to paint HPSG in a bad light,
because the formalism allows us to write grammars
which can be arbitrarily complex;
in technical terminology, the formalism is \textit{\isi{Turing-complete}}
\citep[Section~3.4]{Johnson88}.
% maybe also relevant:
% Barton, Berwick, Ristad (1987)
% ``Computational Complexity and Natural Language''
However, as discussed in the previous section,
there is a clear distinction between theory and formalism.
Although the HPSG formalism rules out the possibility of efficient algorithms
that could cope with any possible feature-structure grammar,
a particular theory (or a particular grammar) might well allow efficient algorithms.

\largerpage 
Keeping processing complexity manageable is handled differently in other computationally-friendly frameworks, such as Combinatory\indexccg Categorial Grammar (CCG),\footnote{%
	For an introduction, see \citet{steedman2011ccg}.
	For a comparison with HPSG, see \crossrefchaptert{cg}.
}
or Tree Adjoining Grammar \citep[TAG;][]{Joshi87a-u,SAJ88a-u}.
\is{Tree Adjoining Grammar (TAG)}
The formalisms of CCG and TAG inherently limit computational complexity:
for both of them, as the sentence length~$n$ increases,
worst-case\is{complexity!worst-case} parsing time is proportional to~$n^6$ \citep{Kasamietal1989}.
This is a deliberate feature of these formalisms,
which aim to be just expressive enough to capture human language,
and not any more expressive.
Building this kind of constraint into the formalism itself
highlights a different school of thought from HPSG.
Indeed, \citet[64]{MuellerCoreGram} explicitly argues
in favor of developing linguistic analyses first,
and improving processing efficiency second.
As discussed above in Section~\ref{cl:theoretical},
separating the formalism from the theory
means that the formalism is stable, even as the theory develops.

It would be beyond the scope of this chapter to give a full review of parsing
algorithms, but it is instructive to give an example.  For grammars that have a \isi{context-free
  backbone} (every analysis can be expressed as a phrase-structure tree plus constraints between
mother and daughter nodes), it is possible to adapt the standard \textit{parsing} algorithm
\citep{kay:1973} for context-free grammars.\is{context-free grammar (CFG)}\is{parsing!chart parsing}
% EMB 2019-02-21 We should have cite for chart-parsing. It looks like Kay 1973 is the originator of that idea, so putting that in for now.
The basic idea is to parse ``bottom-up'',
starting by finding analyses for each token in the input,
and then finding analyses for increasingly longer sequences of tokens (called \textit{spans}\is{span}),
until the parser reaches the entire sentence.

For a context-free grammar,\is{context-free grammar (CFG)}
there is a finite number of nonterminal symbols,
and each \isi{span} is analyzed as a subset of the nonterminals.
For a feature-structure grammar, each span must be analyzed as a set of feature structures,
which makes the algorithm more complicated.
In principle, a grammar may allow an infinite number of possible feature structures,
for example if it includes recursive\is{recursion} unary rules.
However, if we can bound the number of possible feature structures as $C$,
then the worst-case parsing time is proportional to $C^2 n^{\rho+1}$,\is{complexity!worst-case}
where $\rho$ is the maximum number of children in a phrase-structure rule
\citep[Section~3.2.3]{carroll1993parse}. 
This is less complex than for an arbitrary grammar
(which means that this class of grammars is \emph{not} \isi{Turing-complete}),
but $C$ may nonetheless be very large.

\largerpage
But is the number of possible feature structures bounded
in implemented HPSG grammars?
For \is{delphin consortium@\delphin\ Consortium}\delphin\ grammars (see Section~\ref{cl:delphin}),
the answer is yes.
Assuming a system without relational constraints,
the potential for unboundedness in the number of feature structures
stems from the potential for \isi{recursion} in feature paths:
a \isi{list} is a simple example,\footnote{%
	More precisely, in the standard implementation of a list as a feature structure,
	the type \textit{list}\istype{list} has two subtypes \textit{null}\istype{null} and \textit{non-empty-list},\istype{non-empty-list} and
	\textit{non-empty-list} has the features \textsc{first}\isfeat{first}
	and \textsc{rest},\isfeat{rest} where the value of \textsc{rest} is of type \textit{list}.
	This means that the value of \textsc{rest} can itself have the feature \textsc{rest}.
	See also \crossrefchapterw[\page \pageref{page-list-encoding}]{formal-background} on lists.
}
and as another example, the elements on a \textsc{comps} list
also include the feature \textsc{comps}.

However, in practice, such recursive paths do not need to be considered by the parsing algorithm.
For example, selecting \isi{head}s might place constraints on their \isi{complement}s' \isi{subject}s
(\eg\ in \isi{raising}/\isi{control} constructions),
but no further than that (\eg a complement's complement's subject).
Similarly, while \isi{list}s that are potentially unbounded in length are used in semantic representations,
these are never involved in constraining grammaticality.
The only lists that constrain grammaticality are valence lists\is{valence!list},
but in practical grammars these are never greater than length four or five.\footnote{%
	In part, this is because \delphin\ does not
	adopt proposals like the \textsc{deps}\isfeat{deps} list of \citet*{BMS2001a}.
	Furthermore, in many \is{delphin consortium@\delphin\ Consortium}\delphin\ grammars, including the \ili{English} Resource Grammar (ERG),\is{English Resource Grammar (ERG)}
	the \textsc{slash}\isfeat{slash} list cannot have more than one element.
	If unbounded valence lists or \textsc{slash} lists are required,
	such as to model \isi{cross-serial dependencies} (\citealp{Rentier94}; see also \crossrefchapteralp{complex-predicates}),
	the number of possible structures might still be bounded as a function of sentence length;
	this would allow us to bound worst-case parsing complexity,\is{complexity!worst-case}
	but it will be a higher bound.
}

When parsing real corpora,
it turns out that the average-case complexity\is{complexity!average-case} is much better than might be expected \citep{Carroll94}.
% TODO url in bib? https://aclweb.org/anthology/P94-1040
On the one hand, grammatical constructions do not generally combine in the worst-case way,
and on the other, when a grammar writer is confronted
with multiple possible analyses for a particular construction,
they may opt for the analysis that is more efficient for a particular parsing algorithm \citep{Flickinger2000a}.
To measure the efficiency of grammars and parsing algorithms in practice,
it can be helpful to use a \isi{test suite} composed of a representative sample of sentences \citep{OF98}.\is{complexity!computational|)}

% GE: maybe look at evolution.tex, {Implementations and Applications of HPSG}
% GE: Maybe this last paragraph is too terse?
% EMB 2018-09-28: Yes, a little terse
% --- I think maybe starting with how the derivations are like and unlike CFG derivations
% and then going to `bottom-up' chart parsing (with citations!) would help.
% Also, the discussion of complexity has too many contrasts:
% infinite, but not Turing-complete, but not even that bad when parsing real corpora.
% It would also be good to point to some numbers for ``parsing real corpora''
% ... Dan or Stephan or maybe Bec should have published some, I think.
% GE 2019-02-05 Updated and split into three paragraphs
% TODO citation with numbers for parsing real corpora
% EMB 2019-02-25 This would still be a nice to have, if we can get to it before the final version.


\subsubsection{Parse ranking}
\label{cl:prac:rank}
\is{parse ranking|(}

Various kinds of \isi{ambiguity} are well-known in linguistics
(such as modifier\is{ambiguity!modifier attachment} attachment and part-of-speech\is{ambiguity!part-of-speech} assignment),
to the point that examples like \rref{cl:ambig-telescope} are stock in trade:

\begin{exe}
\ex\label{cl:ambig-telescope} 
\begin{xlist}
\item I saw the kid with the telescope.
\item Visiting relatives can be annoying.
\end{xlist}
\end{exe}
% EMB 2019-02-25 Posted to facebook & twitter to try to find the original 
% cites for these. So far, it looks like they go back to the early 1960s 
% at least.


\noindent
A well-constructed grammar should be expected to return multiple
parses for each ambiguous sentence.

\largerpage
However, people are naturally very good at resolving \isi{ambiguity},
which means most ambiguity is not apparent, even to linguists.
It is only with the development of large-scale grammars that the sheer scale of ambiguity has become clear.
% EMB 2018-10-03 What's rref?
% GE 2018-11-27 an example in brackets (from localcommands.tex)
% (normally, I'd prefer \cref from cleveref, but we don't seem to be using that for this handbook)
For example, \rref{cl:ex:ambig-simple}~might seem unambiguous,
but there is a second reading, where \textit{my favorite} is the topicalized object of \textit{speak},
which would mean that town criers generally speak the speaker's favorite thing (perhaps a language) clearly.
There is also a third, even more implausible reading, where \textit{my favorite town} is the topicalized object.
Such implausible readings don't easily come to mind,
and in fact, the 2018 version of the \ili{English}\is{English Resource Grammar (ERG)} Resource Grammar \citep[ERG;][]{Flickinger2000a,Flickinger2011a-u}
gives a total of 21 readings for this sentence.
With increasingly long sentences, such ambiguities stack up very quickly.
For~\rref{cl:ex:ambig-many}, the first line of a newspaper article,\footnote{%
	\url{https://www.theguardian.com/science/2018/aug/22/offspring-of-neanderthal-and-denisovan-identified-for-first-time}, accessed 2019-08-16.}
the ERG gives 35,094 readings.

\begin{exe}
\ex\label{cl:ex:ambig-simple}
My favorite town criers speak clearly.
\ex\label{cl:ex:ambig-many}
A small piece of bone found in a cave in Siberia has been identified
as the remnant of a child whose mother was a Neanderthal and father was a Denisovan,
a mysterious human ancestor that lived in the region.
\end{exe}

% EMB 2018-09-28 Editing a bit through here:
% GE thanks!
\is{machine learning|(}\is{statistical model|(}
While exploring ambiguity can be interesting for a linguist,
typical practical applications require just one parse per input sentence
and specifically the parse that best reflects the intended meaning
(or only the top few parses, in case the one put forward as ``best'' is wrong).
Thus, what is required is a \textit{ranking} of the parses,
so that the application can only use the most highly-ranked parse,
or the top~$k$ parses.

\largerpage
Parse ranking is not usually determined by the grammar itself, because
of the difficulty of manually writing disambiguation
rules.\footnote{In fact, in earlier work, this task was undertaken by
  hand. One of the authors (Bender) had the job of maintaining rule
  weights in addition to developing the Jacy grammar \citep*{SBB2016a}
  at YY Technologies in 2001--2002. No systematic methodology for
  determining appropriate weights was available and the system was
  both extremely brittle (sensitive to any changes in the grammar) and
  next to impossible to maintain.}
% EMB 2019-02-21 Added the above footnote. TMI or fun historical detail?
% GE 2019-02-25 fun historical detail! I didn't know if there was anything to cite.
Typically, a statistical system is used \citep{Tou:Man:Shi:Fli:Oep:02,Tou:Man:Fli:Oep:05}.
First, a corpus is \textit{\isi{treebank}ed}:
for each sentence in the corpus,
an annotator (often the grammar writer) chooses the best parse,
out of all parses produced by the grammar.
The set of all parses for a sentence is often referred to as the \textit{\isi{parse forest}},
and the selected best parse is often referred to as the \textit{\isi{gold standard}} or \textit{\isi{gold parse}}.
Given the gold parses for the whole corpus, a statistical system is trained
to predict the gold parse from a parse forest,
based on many features\footnote{%
	In the machine-learning sense of \textit{feature},
	not the feature-structure sense.
}
of the parse.
From the example in~\rref{cl:ex:ambig-simple},
a number of different features all influence the preferred interpretation:
the likelihood of a construction (such as topicalization),
the likelihood of a \isi{valence} frame (such as transitive \textit{speak}),
the likelihood of a collocation (such as \textit{town crier}),
the likelihood of a semantic relation (such as speaking a town),
and so on.
% GE 2019-02-05:
% - any more detail for MaxEnt parse ranking?
% - any other references?

Because of the large number of possible parses,
it can be helpful to \textit{prune}\is{pruning} the search space:
rather than ranking the full set of parses,
ranking is restricted to a smaller set of parses.
Carefully choosing how to restrict the parser's attention 
can drastically reduce processing time without hurting parsing accuracy,
as long as the algorithm for selecting the subset
includes the correct parse sufficiently frequently.
One method, called \textit{\isi{supertagging}},\footnote{%
	The term \textit{supertagging}, coined by \citet{bangalore1999supertag},
	refers to \textit{\isi{part-of-speech tagging}},
	which predicts a part of speech for each input token,
	from a relatively small set of part-of-speech tags.
	Supertagging is ``super'' in that it predicts detailed lexical entries,
	rather than simple parts of speech.
}
exploits the fact that HPSG is a lexicalized\is{lexicalism} theory:
choosing the correct \isi{lexical entry} for each \isi{token} brings in rich information
that can be exploited to rule out many possible parses.
Thus if the correct lexical entry can be chosen prior to parsing
(\eg\ on the basis of the preceding and following words),
the range of possible analyses the parser must consider is drastically reduced.
Although there is a chance that the supertagger will predict the wrong lexical entry,
using a supertagger can often improve parsing accuracy
by ruling out parses that the parse-ranking model might incorrectly rank too high.
Supertagging was first applied to HPSG by \citet{matsuzaki2007supertag},
building on previous work for TAG\is{Tree Adjoining Grammar (TAG)} \citep{bangalore1999supertag}
and CCG\indexccg \citep{clark2004supertag}.
To allow \isi{multiword expression}s (such as \textit{by and large}),
where the grammar assigns a single lexical entry to multiple tokens,
\citet{dridan2013ubertag} proposes an extension of supertagging, called \textit{\isi{ubertagging}},
which jointly predicts both a \isi{segmentation}\is{tokenization} of the input and supertags for those segments.
\citeauthor{dridan2013ubertag} manages to increase parsing speed by a factor of four,
while also improving parsing accuracy.
% Pruning via chunking? Muszyńska http://aclweb.org/anthology/P16-3014

% EMB 2018-09-28 Doesn't Bec also bring in the lexical rules? Maybe that's too much detail for here.

Finally, in order to train these statistical systems,
we need to first annotate\is{annotation} a \isi{treebank}.
When there are many parses for a sentence,
it can be time-consuming to select the best one.
To efficiently use an annotator's time,
it can be helpful to use \textit{\isi{discriminant}s}:
properties which hold for some parses but not for others \citep{Carter:97}.
For example, \isi{discriminant}s might include
whether to analyze an ambiguous\is{ambiguity} token as a noun or a verb,
or where to attach a prepositional phrase.\is{ambiguity!modifier attachment}
This approach to treebanking also means that
annotations can be re-used when the grammar is updated
\citep{OFTM2004a-u,Fli:Oep:Ben:17}.
For more on treebanking, see Section~\ref{cl:lang-doc:treebanks}.%
\is{machine learning|)}\is{statistical model|)}\is{parse ranking|)}

% Self-training?
% e.g. for Alpino: 
%% Gertjan van Noord. Self-trained Bilexical Preferences to Improve Disambiguation Accuracy. In: Harry Bunt, Paola Merlo and Joakim Nivre (editors), Trends in Parsing Technology. Dependency Parsing, Domain Adaptation, and Deep Parsing. Springer Verlag. pp 183-200. 2010.


\subsubsection{Semantic dependencies}
\label{cl:prac:dep}

\is{dependency!semantic|(}

In practical applications of HPSG grammars,
the full phrase-structure trees and the full feature structures are often unwieldy,
containing far more information than is necessary for the task at hand.
It is therefore often desirable to extract a concise \isi{semantic representation}.
%TODO check the Semantics chapter once it's written

%\largerpage[-1]
In computational linguistics, a popular approach to semantics
is to represent the meaning of a sentence as a \textit{\isi{dependency graph}},
as this enables the use of graph-based algorithms.\footnote{%
	In this section, we are concerned with \emph{semantic} dependencies.\is{dependency!semantic}
	For \emph{syntactic} dependencies,\is{dependency!syntactic} see \crossrefchaptert{dg}.
	Some practical applications of HPSG use syntactic dependencies
	(including many applications of the \isi{Alpino} grammar, discussed in Section~\ref{cl:other:alpino}).
}
Several types of dependency graph have been proposed
based on Minimal Recursion Semantics \citep[MRS;][]{CFPS2005a},\is{Minimal Recursion Semantics (MRS)}
with varying levels of simplification.
\citet{oepen2006eds} observe that if every predicate has a unique
\textit{\isi{intrinsic argument}}, an MRS can be converted to a variable-free semantic representation\is{semantic representation!variable-free}\is{logical variable}
by replacing each reference to a variable with a reference to the corresponding predicate.
They present \isi{Elementary Dependency Structures (EDS)}:
semantic graphs which maintain \isi{predicate-argument structure} but discard some \isi{scope} information.
(For many applications, scope information is less important than predicate-argument structure.)
\citet{copestake2009dmrs} builds on this idea
to create a more expressive graph-based representation called
\isi{Dependency Minimal Recursion Semantics (DMRS)},
which is fully interconvertible with MRS.\footnote{%
	More precisely, for DMRS and MRS to be fully interconvertible,
	every predicate (except for \isi{quantifier}s) must have an \isi{intrinsic argument},
	and every variable must be the intrinsic argument of exactly one predicate.
} This expressivity is achieved by adding annotations on the edges to
indicate scope information.
Finally, \delphin\ MRS Dependencies \citep[DM;][]{ivanova2012dm}\is{delphin MRS Dependencies@\delphin\ MRS Dependencies (DM)}
express \isi{predicate-argument structure} purely in terms of the surface \isi{token}s,
without introducing any abstract predicates.\is{predicate!abstract}

%\largerpage
For example, the \isi{English Resource Grammar (ERG)}
produces the MRS representation in~(\ref{cl:fig:mrs-feat})
for the sentence \emph{The cherry tree blossomed}.
For simplicity, we have omitted some details, including
features such as \textsc{number} and \textsc{tense},
individual constraints (\textsc{icons}),\isfeat{icons}
and the use of difference lists.
By convention, \is{delphin consortium@\delphin\ Consortium}\delphin\ predicates beginning with an
underscore correspond to a \isi{lexical item},\is{predicate!lexical} and have a three-part format,
consisting of a lemma, a part-of-speech tag, and (optionally) a sense.
Predicates without an initial underscore are abstract predicates.\is{predicate!abstract}
The \textit{qeq}\is{QEQ} constraints (\isi{equality modulo quantifiers})
are scopal\is{scope} relationships, where \isi{quantifier}s may possibly intervene
(for details, see \citealt{CFPS2005a} or \crossrefchapteralt{semantics}).

\begin{exe}
	\ex\label{cl:fig:mrs-feat}
	\scalebox{.95}{
\avm{
	[\type*{mrs}
		hook &	[\type*{hook}
			ltop & \1 \\
			index & \2 ] \\
		rels & <[\type*{relation}
				pred & \_the\_q \\
				lbl & \3 \\
				arg0 & \4 \\
				rstr & \5 \\
				body & \6 ],
			[\type*{relation}
				pred & compound \\
				lbl & \7 \\
				arg0 & \8 \\
				arg1 & \4 \\
				arg2 & \9 ],
			[\type*{relation}
				pred & udef\_q \\
				lbl & \tag{10} \\
				arg0 & \9 \\
				rstr & \tag{11} \\
				body & \tag{12} ], \\
			[\type*{relation}
				pred & \_cherry\_n\_1 \\
				lbl & \tag{13} \\
				arg0 & \9 ],
			[\type*{relation}
				pred & \_tree\_n\_of \\
				lbl & \7 \\
				arg0 & \4 ],
			[\type*{relation}
				pred & \_blossom\_v\_1 \\
				lbl & \1 \\
				arg0 & \2 \\
				arg1 & \4 ] 
				\\ > \\
		hcons & <[\type*{qeq}
				harg & \5 \\
				larg & \7 ],
			[\type*{qeq}
				harg & \tag{11} \\
				larg & \tag{13} ] > ]
}
}
\end{exe}

\noindent
For readability, it can be easier to express an MRS
in a more abstract mathematical form, as shown in~(\ref{cl:fig:mrs-abs}).
This is equivalent to the feature structure in~(\ref{cl:fig:mrs-feat}).

\begin{exe}
	\setlength{\abovedisplayskip}{1ex}
	\setlength{\belowdisplayskip}{1ex}
	\ex\label{cl:fig:mrs-abs}
	\begin{minipage}[c]{.6\textwidth}
	\vspace*{-1\baselineskip}
	\begin{align*}
	    \textsc{index}\colon & e_1 \\
		l_1\colon & \textit{\_the\_q}\left(x_1,h_1,h_2\right), h_1 \,\textsc{qeq}\, l_4 \\
		l_2\colon & \textit{udef\_q}\left(x_2,h_3,h_4\right), h_3 \,\textsc{qeq}\, l_3 \\
		l_3\colon & \textit{\_cherry\_n\_1}\left(x_2\right) \\
		l_4\colon & \textit{\_tree\_n\_of}\left(x_1\right), \textit{ compound}\left(e_2,x_1,x_2\right) \\
		\textsc{ltop}, l_5\colon & \textit{\_blossom\_v\_1}\left(e_1,x_1\right)
	\end{align*}
	\end{minipage}
\end{exe}

The corresponding DMRS\is{Dependency Minimal Recursion Semantics (DMRS)}
representation is shown in~(\ref{cl:fig:dmrs}).
This captures all of the information
in the MRS\is{Minimal Recursion Semantics (MRS)} in (\ref{cl:fig:mrs-abs}).
Predicates\is{predicate} are represented as nodes,
while \isi{semantic role}s and scopal\is{scope} constraints are represented as directed edges,
called \textit{dependencies} or \textit{links}.
Each dependency has two labels.
The first is an \isi{argument label},
such as \textsc{arg1},\isfeat{arg1} \textsc{arg2},\isfeat{arg2}
or \textsc{rstr}\isfeat{rstr} (the restriction of a \isi{quantifier}).
The second is a scopal constraint,
such as \textsc{qeq}\is{QEQ},\footnote{%
	An alternative notation is to write
	\textsc{/h}\is{/H} instead of \textsc{/qeq}.\is{/QEQ}
}
\textsc{eq}\is{EQ} (the linked nodes share a label\is{label (MRS)} in the MRS,
which is generally true for modifiers),
or \textsc{neq}\is{NEQ} (the linked nodes don't share a label).

\begin{exe}
	\ex\label{cl:fig:dmrs}
	\small
	\begin{tikzpicture}[node distance=7mm and 11mm, inner sep=1.5mm, thick, baseline=(ch.base)]
		\node (ch) {\itshape \_cherry\_n\_1};
		\node[right=of ch] (cm) {\itshape compound};
		\node[right=of cm] (tr) {\itshape \_tree\_n\_of};
		\node[right=of tr] (b)  {\itshape \_blossom\_v\_1};
		\node[above=of ch] (u)  {\itshape udef\_q};
		\node[above=of tr] (th) {\itshape \_the\_q};
		\draw[->] (cm) -- (ch) node[midway, above] {\textsc{arg2/neq}};
		\draw[->] (cm) -- (tr) node[midway, above] {\textsc{arg1/eq}};
		\draw[->] (b) -- (tr) node[midway, above] {\textsc{arg1/neq}};
		\draw[->] (u) -- (ch) node[midway, right, yshift=1mm] {\textsc{rstr/qeq}};
		\draw[->] (th) -- (tr) node[midway, right, yshift=1mm] {\textsc{rstr/qeq}};
		\node[anchor=north, align=center, inner sep=0pt] at (b.south) {\textsc{ltop}\\\textsc{index}};
	\end{tikzpicture}%
\end{exe}

Finally, the corresponding DM representation
is shown in~(\ref{cl:fig:dm}).\is{delphin MRS Dependencies@\delphin\ MRS Dependencies (DM)}
This is a simplified version of MRS,\is{Minimal Recursion Semantics (MRS)}
where all nodes are tokens in the sentence.
Some abstract predicates are dropped (such as \textit{udef\_q}),
while others are converted to dependencies (such as \textit{compound}).
Some scopal information is dropped (such as \textsc{eq} vs.\ \textsc{neq}).
The label \textsc{bv}\is{BV} stands for the ``\isi{bound variable}'' of a \isi{quantifier},
equivalent to the \textsc{rstr/qeq}\isfeat{rstr}\is{QEQ} of DMRS.

\begin{exe}
	\ex\label{cl:fig:dm}
	\begin{tikzpicture}[node distance=15mm, inner sep=1.5mm, thick, baseline=(ch.base)]
		\node (th) {\itshape the\vphantom{y}};
		\node[right=of th] (ch) {\itshape cherry};
		\node[right=of ch] (tr) {\itshape tree\vphantom{y}};
		\node[right=of tr] (b) {\itshape blossomed\vphantom{y}};
		\draw[->] (ch) -- (tr) node[midway, above] {\textsc{compound}};
		\draw[->] (b) -- (tr) node[midway, above] {\textsc{arg1}};
		\draw[->] (th) to[out=90, in=90, looseness=.4] node[midway, above, inner sep=1mm] {\textsc{bv}} (tr);
		\node[anchor=north, inner sep=0pt] at (b.south) {\textsc{top}};
	\end{tikzpicture}
\end{exe}


The existence of such dependency graph formalisms,
as well as \isi{software} packages to manipulate such graphs (\eg\ \citealt{ivanova2012dm}, \citealt{copestake2016pydmrs}, \citealt{Her:Kuh:Oep:19}, or \isi{PyDelphin}\footnote{\url{https://github.com/delph-in/pydelphin/}, accessed 2019-08-16.}),
has made it easier to use HPSG grammars in a number of practical tasks,
as we will discuss in Section~\ref{cl:downstream}.\is{dependency!semantic|)}


%% \subsection{A brief history of HPSG grammar engineering}
%% \label{cl:history}

%% History: PAGE, VerbMobil, ?? % First draft: Emily

%% % GE: no need to repeat what Flickinger et al. cover in their chapter
%% % but probably a good idea to contrast grammar engineering and linguistic theorizing

%% % EMB 2018-12-21 Maybe everything we were going to put here is ending up
%% % in the discussions below?

%% Current platforms:
%%     \begin{itemize}
%%     \item LKB/ACE/PET/Agree
%%     \item Trale
%%     \item Other
%%     \end{itemize}


\section{Development of HPSG resources}
\label{cl:resources}


In\is{software|(} this section we describe various projects that have developed computational
resources on the basis of or inspired by HPSG.  As we will discuss in Section~\ref{cl:deployment}
below, such resources can be used both in \isi{linguistic hypothesis testing} as well as in various
practical applications\is{application}.  The intended purpose of the resources influences the form that they
take.  The CoreGram Project (Section~\ref{cl:coregram}) and Babel (Section~\ref{cl:other:babel})
primarily target linguistic hypothesis testing, the Alpino and Enju parsers
(Section~\ref{cl:other:alpino} and~\ref{cl:other:enju}) primarily target practical applications, and
the \delphin\ Consortium (Section~\ref{cl:delphin}) attempts to balance these two goals.


\subsection{CoreGram}
\label{cl:coregram}

%\largerpage
\enlargethispage{3pt}
\is{CoreGram|(}%
\is{grammar engineering!multilingual|(}%
The CoreGram\footnote{
	\url{https://hpsg.hu-berlin.de/Projects/CoreGram.html}, accessed 2021-06-11.
} Project aims to produce large-scale HPSG grammars,
which share a common ``core'' grammar \citep{MuellerCoreGram}.
% https://hpsg.hu-berlin.de/~stefan/Pub/coregram.pdf
% https://hpsg.hu-berlin.de/~stefan/Pub/coregram-brief.pdf
At the time of writing, large grammars have been produced for \ili{German} \citep{MuellerLehrbuch1},
\ili{Danish} \citep{MOeDanish}, \ili{Persian} \citep{MG2010a}, \ili{Maltese}
\citep{MuellerMalteseSketch}, and \ili{Mandarin Chinese} \citep{ML2013a}.  Smaller grammars are also
available for \ili{English}, \ili{Yiddish}, \ili{Spanish}, \ili{French}, and \ili{Hindi}.

All grammars are implemented in the \isi{TRALE} system \citep{MPR2002a-u,Penn2004a-u}, which
accommodates a wide range of technical devices proposed in the literature, including phonologically
\isi{empty element}s, \isi{relational constraint}s, implications with \isi{complex antecedent}s, and
cyclic\is{cyclic feature description} feature structures.  It also accomodates \isi{macro}s and an expressive morphological
component.\is{morphology} \citet{MelnikHandWritten} observes that, compared to other platforms like
the LKB\indexlkb (see Section~\ref{cl:delphin} below), this allows grammar engineers to directly
implement a wider range of theoretical proposals.

An important part of CoreGram is the sharing of grammatical constraints across grammars.  Some
general constraints hold for all grammars, while others hold for a subset of the grammars, and some
only hold for a single grammar.  \citet{MuellerCoreGram} describes this as a
``bottom-up\is{linguistic universal!bottom-up exploration} approach with cheating'' (p.\,43): the
aim is to analyze each language on its own terms (hence ``bottom-up''), but to re-use analyses from
existing grammars if possible (hence ``with cheating'').
% on ``bottom-up'', cite Haspelmath, Dryer, Croft?
% p43 of https://hpsg.hu-berlin.de/~stefan/Pub/coregram.pdf
The use of a core\is{core grammar} set of constraints is motivated not just for practical reasons,
but also for theoretical ones.  By developing multiple grammars in parallel, analyses can be
improved by \isi{cross-linguistic comparison}.  The constraints encoded in the core grammar can be
seen as a hypothesis about the structure of human language,
% GE 2019-02-25 aN hypothesis?
% I definitely don't say that, but I'm happy to follow your spelling!
% We currently also have ``a history'' and ``a high-dimensional vector'' elsewhere
as we will discuss in Section~\ref{cl:lang-doc:coregram}.

CoreGram grammar development aims to incrementally increase \isi{coverage} of each language.  To
measure progress, grammars are evaluated against \isi{test suite}s: collections of sentences each
annotated with a \isi{grammaticality judgment} \citep{ONK97a,Mueller2004f}.  This allows a
grammarian to check for unexpected side effects when modifying a grammar and to avoid situations
when implementing an analysis of one phenomenon would break the analysis of another phenomenon.
This is particularly important when modifying a constraint that is used by several grammars.  To
help achieve these aims, grammar development is supported by a range of software tools, including
the \isi{test suite} tool \itsdb (\citealp{Oepen:01}; see also Section~\ref{cl:delphin}),
and the graphical debugging tool \isi{Kahina} \citep{DER2010a-u,DER2013a}.\is{CoreGram|)}

\subsection{The \delphin\ Consortium}
\label{cl:delphin}

\is{delphin consortium@\delphin\ Consortium|(}%
The \delphin\footnote{\delphin\ stands for DEep Linguistic Processing in \textsc{Hpsg} INitiative;
  see \url{http://www.delph-in.net}, accessed 2019-08-16.} Consortium was established in 2001 to
facilitate the development of large-scale, linguistically motivated HPSG grammars for multiple
languages, in tandem with the software required for developing them and deploying them in practical
applications. At the time when \delphin\ was founded, the ERG\is{English Resource Grammar
  (ERG)}\il{English} \citep{Flickinger2000a,Flickinger2011a-u} had already been under development
for eight years, and the \verbmobil project \citep{Wahlster2000a-ed} had also spurred the
development of grammars for \ili{German}\is{GG (German Grammar)} \citep[GG;][]{MK2000a,Crysmann2003b} and
\ili{Japanese}\is{Jacy} \citep*[Jacy;][]{SBB2016a}. \isi{Project DeepThought}
\citep*{Callmeier-etal:2004} was exploring methodologies for combining deep\is{deep processing} and
\isi{shallow processing} in practical applications\is{application} across multiple languages. This inspired
the development of the LinGO \isi{Grammar Matrix} \citep*{BFO2002a-u}, which began as a \isi{core
  grammar}, consisting of constraints hypothesized to be cross-linguistically useful, abstracted out
of the ERG with reference to Jacy and GG. The goal of the Grammar Matrix is to serve as a starting
point for the development of new grammars, making it easy to reuse what has been learned in the
development of existing grammars. In the years since, it has been extended to include
``libraries''\is{Grammar Matrix!library} of analyses of cross-linguistically variable phenomena
\citep[e.g.,][]{Drellishak2009a-u,BDFPS2010a-u}.

\delphin\ provides infrastructure (version control repositories, mailing lists, annual meetings) and
an emphasis on \isi{open-source} distribution of resources, both of which support the collaboration
of a global network of researchers working on interoperable components. Such components include
repositories of linguistic knowledge, that is, both grammars and \isi{meta-grammar}s (including the
Matrix and \isi{CLIMB}, \citealt{Fokkens:14}); processing engines that apply that knowledge for
\isi{parsing} and \isi{generation} (discussed further below); software for supporting the
development of grammar documentation\is{documentation!of grammars}
\citep[e.g.,][]{Hashimoto-etal:07}, software for creating \isi{treebank}s
(\citealt{OFTM2004a-u,Packard:15}; see also Section~\ref{cl:lang-doc:treebanks} below), \isi{parse
  ranking} models trained on these treebanks (\citealt{Tou:Man:Fli:Oep:05}; see also
Section~\ref{cl:prac:rank} above), and software for \isi{robust processing}, \ie\ using the
knowledge encoded in the grammars to return analyses for sentences even if the grammar deems them
ungrammatical \citep{W11-2923,buys2017parse,chen2018parse}.

A key accomplishment of the \delphin\ Consortium is the standardization of a \isi{formalism} for the
declaration of grammars \citep{Copestake:02:CLE}, a formalism for the \isi{semantic representation}s
\citep{CFPS2005a}, and file formats for the storage and interchange of
% GE 2019-01-17 I presume CFPS2005a is a suitable reference?
% EMB 2019-02-21 Yes, though I think the discussion on MrsRfc is also 
% relevant --- I guess there's not really anything citable there though.
grammar outputs (\eg\ \isi{parse forest}s, as well as the results of \isi{treebank}ing;
\citealt{Oepen:01,OFTM2004a-u}).  These standards facilitate the development of multiple different
\isi{parsing} and \isi{generation} engines which can all process the same grammars, including, so
far, the LKB\indexlkb \citep{Copestake2002a}, \isi{PET} \citep{callmeier00},
\isi{ACE},\footnote{\url{http://sweaglesw.org/linguistics/ace/}, accessed 2019-08-16.}  and
\isi{Agree} \citep{Slayden2012a-u}; of multiple software systems for processing bulk grammar output,
like \itsdb \citep{Oepen:01}, \is{art
  (software)}art,\footnote{\url{http://sweaglesw.org/linguistics/libtsdb/art.html}, accessed
  2019-08-16.} and \isi{PyDelphin}\footnote{\url{https://github.com/delph-in/pydelphin/}, accessed
  2019-08-16.};
% GE 2019-01-17 PyDelphin doesn't have an obvious citation...
% maybe best to refer to systems by name?
and of multilingual downstream systems which can be adapted to additional languages by plugging in
different grammars.  These tools and standards have in turn helped support a thriving community of
users who furthermore accumulate and share information about best practices.
\citet[234]{MelnikHandWritten} credits this community and the tools it has developed as a key factor
that makes grammar engineering with the \delphin\ ecosystem more accessible to HPSG linguists,
compared to other platforms like \isi{TRALE} (see Section~\ref{cl:coregram} above).

The \delphin\ community maintains research interests in both linguistics and practical
applications. The focus on linguistics means that \delphin\ grammarians strive to create grammars
which capture linguistic generalizations\is{generalization} and model \isi{grammaticality}. This, in turn, leads
to grammars with lower \isi{ambiguity} than one finds with \isi{treebank}-trained grammars and,
importantly, grammars which produce well-formed strings in \isi{generation}. The focus on
practical applications\is{application} leads to several kinds of additional research goals. Practical
applications require \isi{robust processing}, which in turn requires methods for handling unknown
words \citep[e.g.,][]{chartmapping}, methods for managing \isi{extra-grammatical mark-up} in text
such as in Wikipedia pages \citep[e.g.,][]{FOY2010a-u}, and strategies for processing inputs that
are ungrammatical, at least according to the grammar (\eg\ \citealp{W11-2923}; see also
Section~\ref{cl:downstream:data}).  Processing large quantities of text motivates performance
innovations, such as \isi{supertagging} or \isi{ubertagging} (e.g.,
\citealp{matsuzaki2007supertag,dridan2013ubertag}; see also Section~\ref{cl:prac:rank}) to speed up
processing times.\is{processing time} Naturally occurring text can include very long sentences which
can run up against processing limits. Supertagging helps here, too, but other strategies include
\textit{\isi{sentence chunking}}, which is the task of breaking a long sentence into smaller ones
without loss of meaning \citep{muszynska:2016:ACL-SRW}.
%and pre-processors that provide partial
%information about constituent boundaries to narrow the search space.
% EMB 2019-02-21 Took the above out because I can't find a DELPH-IN citation for it.
% GE 2019-02-25 Ewa Muszyńska is working on this
% I thought about mentioning this earlier in the chapter but have only left a comment so far
% http://aclweb.org/anthology/P16-3014
% EMB 2019-02-25 Oh cool -- not actually what I was thinking of, but definitely
% relevant here, so I've added it! 
Working with real-world text (rather than curated test suites designed for linguistic research only)
requires the integration of external components such as \isi{morphological analyzer}s
\citep[e.g.,][]{Marimon2013a-u} and \isi{named entity recognizer}s
\citep[e.g.,][]{L06-1115,Sch:Usz:Fed:08}. As described in Section~\ref{cl:prac:rank}, working with
real-world applications requires \isi{parse ranking} \citep[e.g.,][]{Tou:Man:Fli:Oep:05}, and
similarly ranking of generator outputs (known as \textit{\isi{realization ranking}};
\citealt[e.g.,][]{Velldal:09}). Finally, research on embedding \isi{broad-coverage grammar}s in
practical applications\is{application} inspires work towards making sure that the \isi{semantic
  representation}s can serve as a suitable interface for external components
\citep[e.g.,][]{flickinger2005sem}.
% EMB 2018-12-21 I can probably put in citations for all of those
% things. Worth it, or too busy?
% GE 2019-01-29 Probably okay like this,
% as long as those citations are discussed elsewhere in the chapter
% EMB 2019-02-21 On my read-through, I decided they really should be there and
% that I shouldn't try to cram that all into one sentence! So, fixed.
These efforts are also valuable from a strictly linguistic point of view, \ie\ one not concerned with practical applications.  First, the broader the \isi{coverage} of a grammar, the more linguistic phenomena it can be used to explore.  Second, external constraints on the form of semantic representations provide useful guide points in the development of semantic analyses.%
% EMB 2018-12-21 I don't have a cite for that one --- just personal
% experience...
\is{delphin consortium@\delphin\ Consortium|)}%
\is{grammar engineering!multilingual|)}


\subsection{Other HPSG and HPSG-inspired broad-coverage grammars}
\label{cl:othergrammars}

\subsubsection{Alpino}
\label{cl:other:alpino}

\is{Alpino|(}%
Alpino\footnote{%
	\url{http://www.let.rug.nl/vannoord/alp/Alpino/}, accessed 2019-08-16.
}
is a broad-coverage grammar\is{broad-coverage grammar} of \ili{Dutch}
\citep{BvNM2001a-u,vannoord2005alpino,vannoord2006alpino}.
% https://www.let.rug.nl/vannoord/papers/alpino.pdf
% https://www.let.rug.nl/vannoord/papers/wcpsavg.pdf
% http://talnarchives.atala.org/TALN/TALN-2006/taln-2006-invite-002.pdf
The main motivation is practical: to provide coverage and accuracy
comparable to state-of-the-art parsers for \ili{English}.
Nonetheless, it also includes theoretically interesting analyses,
such as for \isi{cross-serial dependencies} \citep{BvN98a}.
In addition to using hand-written rules,
lexical information (such as subcategorization frames)\is{valence} has also been extracted from two existing lexicons,
\isi{Celex} \citep{baayen1995celex}
and \isi{Parole} \citep{kruyt1997parole}.

Alpino produces syntactic dependency graphs,\is{dependency!syntactic}
following the annotation format of the \isi{Spoken \ili{Dutch} Corpus} \citep{oostdijk2000corpus}.
These dependencies are constructed directly in the feature-structure formalism,
exploiting the fact that a feature structure can be formalized as a \isi{directed acyclic graph}.
Each lexical entry encodes a partial dependency graph,
and these graphs are composed through phrase structure rules
to give a dependency graph for a whole sentence.

Although these dependencies differ from the semantic dependencies\is{dependency!semantic} discussed in Section~\ref{cl:prac:dep},
a common motivation is to make the representations easier to use in practical applications\is{application}.
To harmonize with other computational work on \isi{dependency parsing},
\citet{Bou:Van:17} have also produced a mapping from this format
to Universal Dependencies \citep[UD;][]{Niv:Mar:Gin:16},\is{Universal Dependencies (UD)}
as discussed in Section~\ref{cl:lang-doc:treebanks} below.
Alpino uses a \isi{statistical model}\is{machine learning} trained on a dependency \isi{treebank},
and in fact the same statistical model can be used in both \isi{parsing} and \isi{generation}
\citep{dekok2011reversible}.\is{Alpino|)}

\subsubsection{Enju}
\label{cl:other:enju}

\is{Enju|(}%
Enju\footnote{%
	\url{http://www.nactem.ac.uk/enju/}, accessed 29 August 2019.
} \citep{MNT2005a-u}
is a broad-coverage grammar of \ili{English},
semi"=automatically acquired from the \isi{Penn Treebank} \citep{Mar:San:Mar:93}.
This approach aims to reduce the cost of writing a grammar
by leveraging existing resources.
The basic idea is that, by viewing Penn Treebank trees as partial specifications of HPSG analyses,
it is possible to infer lexical entries.

\citeauthor{MNT2005a-u} converted the relatively flat trees in the Penn Treebank to \isi{binary-branching} trees,
and percolated \isi{head information} through the trees.
They also had to convert analyses for certain constructions,
including subject-\isi{control} verbs, \isi{auxiliary verb}s, \isi{coordination}, and extracted arguments.\is{extraction}
Each converted tree can then be combined with a small set of hand-written HPSG schemata,
to induce a lexical entry\is{lexical entry!induction of} for each word in the sentence.

The development of Enju has focused on performance in practical applications,
and the grammar is supported by an efficient parser \citep{tsuruoka2004enju,matsuzaki2007supertag},
using a probabilistic model for feature structures \citep{MT2008a-u}.
Enju has been used in a variety of NLP tasks, as will be discussed in Section~\ref{cl:downstream:nlp}.\is{Enju|)}

\subsubsection{Babel}
\label{cl:other:babel}

\isi{Babel} is a broad-coverage grammar of \ili{German} \citep{Babel,Mueller99a}.
One interesting feature of this grammar is that
it makes extensive use of \isi{discontinuous constituent}s \citep{Mueller2004b}.
Although this makes the worst-case parsing complexity\is{complexity!worst-case} much worse,
parsing speed doesn't seem to suffer in practice.
This mirrors the findings of \citet{Carroll94},
discussed in Section~\ref{cl:prac:comp} above.

% Notes from Stefan:
%% I have a candidate for being listed under "other". In 93 I started to
%% work on an HPSG implementation implemented in Prolog. The Babel system
%% had the largest \ili{German} grammar at the time and the processing system was
%% the fastest. It was orders of magnitute better than what IBM had and
%% people like Wahlster were impressed that one guy could do both grammar
%% and system, something IBM spent one or more Millions on.





\section{Deployment of HPSG resources}
\label{cl:deployment}

There are several different ways in which computational resources based on HPSG are used.  In
Section~\ref{cl:lang-doc}, we first consider applications furthering linguistic research, including
both language documentation and linguistic hypothesis testing.  Then, in
Section~\ref{cl:downstream}, we consider applications outside of linguistics.


\subsection{Language documentation and linguistic hypothesis testing}
\label{cl:lang-doc}

\is{language documentation|(}%
\is{linguistic hypothesis testing|(}%
As described by \citet[\page 439]{Mueller99a}, \citet{Bender2008c}, and \citet{BFO2011a-u},
grammar engineering\,---\,that is, the building of grammars in software\,---\,is
an essential technique for testing linguistic hypotheses at scale. By
``at scale'', we mean both against large quantities of data and as
integrated models of language that handle multiple phenomena at
once. In this section, we review how this is done in the CoreGram
and Grammar Matrix projects for cross-linguistic hypothesis testing,
and in the AGGREGATION project in the context of language
documentation.\footnote{Grammar engineering\is{grammar engineering} is not specific to HPSG and
in fact has a history going back to at least the early 1960s \citep{Kay:63,ZFHW65a,Petrick65a-u,FBDPM71a-u}
and modern work in grammar engineering includes work in many different frameworks, such as Lexical Functional Grammar\indexlfg \citep{BKNS99a-ed}, Combinatory Categorial
Grammar\indexccg \citep{BCPW2007a}, Grammatical Framework\is{Grammatical Framework (GF)} \citep{Ranta:09}, and others.
For reflections on grammar engineering for linguistic hypothesis testing
in LFG, see \citet{BKNS99a-ed} and \citet{King:16}.}
%
% King (2016) ``Theoretical linguistics and grammar engineering as mutually constraining disciplines''
% http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2016/headlex2016-king.pdf


\subsubsection{CoreGram}
\label{cl:lang-doc:coregram}
% Guy

As described in Section~\ref{cl:coregram},
the \isi{CoreGram} project develops grammars for a diverse set of languages,
and shares constraints across grammars in a bottom-up fashion, so that more
similar languages share more constraints. There are constraints
shared across all of the grammars in the project which can be seen
as a hypothesis about properties shared by all languages.
Whenever the CoreGram project expands to cover a new language,
it can be seen as a test of this hypothesis.

For example, the most general constraint set
allows a language to have V2 word order\is{word order!V2}
(as exemplified by \ili{Germanic} languages),
but rules out verb"=penultimate word order\is{word order!verb"=penultimate},
as discussed by \citet[\page 45--46]{MuellerCoreGram}
(see also \crossrefchapteralt{order} on constituent order and \crossrefchapteralt{udc} on nonlocal dependencies).
It also includes constraints for \isi{argument structure} and \isi{linking}
(see \crossrefchapteralt{arg-st}),
as well as for \isi{information structure}
(see \crossrefchapteralt{information-structure}).



\subsubsection{Grammar Matrix}
% Emily
\label{cl:lang-doc:gmcs}

\is{Grammar Matrix|(}%
\is{delphin consortium@\delphin\ Consortium|(}%
As noted in Section~\ref{cl:delphin}, the LinGO \isi{Grammar Matrix} \citep{BFO2002a-u,BDFPS2010a-u}
was initially developed in the context of \isi{Project DeepThought} with the goal of speeding up the
development of \delphin-style grammars for additional languages.  It consists of a shared \isi{core
  grammar} and a series of ``libraries''\is{Grammar Matrix!library} of analyses for
cross-linguistically variable phenomena.  Both of these constitute linguistic hypotheses: the
constraints are hypothesized to be cross-linguistically useful.  However, in the course of
developing grammars based on the Matrix for specific languages, it is not uncommon to find reasons
to refine the core grammar. The libraries, in turn, are intended to cover the attested range of
variation for the phenomena they model.  Languages that are not covered by the analyses in the
libraries provide evidence that the libraries need to be extended or refined.

\largerpage
Grammar Matrix grammar development is less tightly coordinated than that of \isi{CoreGram} (see
Section~\ref{cl:coregram}): in the typical use case, grammar developers start from the Grammar
Matrix, but with their own independent copy of the Matrix core grammar. This impedes somewhat the
ability of the Matrix to adapt to the needs of various languages (unless grammar developers report
back to the Matrix developers).  On the other hand, the Matrix libraries\is{Grammar Matrix!library}
represent an additional kind of linguistic hypothesis testing: each library on its own represents
one linguistic phenomenon, but the libraries must be interoperable with each other. This is the
cross-linguistic analogue of how monolingual implemented grammars allow linguists to ensure that
analyses of different phenomena are interoperable (\citealt[439--440]{Mueller99a};
\citealt{Bender2008c}): the Grammar Matrix customization system\is{Grammar Matrix!customization
  system}\is{meta-grammar} allows its developers to test cross-linguistic libraries of analyses for
interactions with other phenomena \citep{BFO2011a-u,Bender:16}. Without computational support (\ie\
a computer keeping track of the constraints that make up each analysis, compiling them into specific
grammars, and testing those grammars against test suites), this problem space would be too complex
for exploration.\is{Grammar Matrix|)}\is{delphin consortium@\delphin\ Consortium|)}


\subsubsection{AGGREGATION}
\label{cl:lang-doc:aggr}
% Emily

\is{AGGREGATION|(}%
In many ways, the most urgent need for computational support for linguistic hypothesis testing is
the description of \isi{endangered language}s. Implemented grammars can be used to process
transcribed but unglossed text in order to find relevant examples more quickly, both of phenomena
that have already been analyzed and of phenomena that are as yet not well-understood.\footnote{This
  methodology of using an implemented grammar as a sieve to sift the interesting examples out of
  corpora is demonstrated for \ili{English} by \citet{Baldwin-et-al-05}.}  Furthermore,
\isi{treebank}s constructed from implemented grammars can be tremendously valuable additions to
language documentation (see Section~\ref{cl:lang-doc:treebanks} below). However, the process of
building an implemented grammar is time-consuming, even with the start provided by a multilingual
grammar engineering project like \isi{CoreGram}, \isi{ParGram}
\citep{BDKMR02a-u,Kin:For:Kuh:But:05}, the \isi{GF Resource Grammar Library} \citep{Ranta:09}, or
the \isi{Grammar Matrix}.

This is the motivation for the
AGGREGATION\footnote{\url{http://depts.washington.edu/uwcl/aggregation/}, accessed 2019-08-16.}
project, which starts from two observations: (1) descriptive linguists produce extremely rich
annotations on data in the form of \isi{interlinear glossed text (IGT)}; and (2) the Grammar
Matrix's libraries are accessed through a customization system which elicits a grammar specification
in the form of a series of choices describing either high-level \is{typological property}typological
properties or specific constraints on \isi{lexical class}es and \isi{lexical rule}s.  The goal of
AGGREGATION is to automatically produce such grammar specifications on the basis of information
encoded in IGT, to be used by the Grammar Matrix customization system to produce language-particular
grammars. AGGREGATION uses different approaches for different linguistic subsystems. For example, it
learns\is{machine learning}\is{statistical model} \isi{morphotactics}\is{morphology} by observing
\isi{morpheme} order in the training data, and how to group affixes together into \isi{position
  class}es based on measures of overlap of stems they attach to \citep{Wax:14,Zamaraeva:17}. For
many kinds of syntactic information, it leverages syntactic structure projected from the translation
line (\ili{English}, easily parsed with current tools) through the gloss line (which facilitates
aligning the language and translation lines) to the language line
\citep{Xia:Lew:07,Georgi:16}. Using this projected information, the AGGREGATION system can detect
\isi{case frame}s for verbs, \isi{word order} patterns, etc.\
\citep{Ben:Goo:Cro:Xia:13,Zam:How:Ben:19}.\footnote{The \isi{TypeGram} project \citep{Hel:Bee:14} is
  in a similar spirit. TypeGram provides methods of creating HPSG grammars by encoding
  specifications of valence and inflection in particularly rich IGT and then creating grammars based
  on those specifications.}%
\is{AGGREGATION|)}\is{software|)}


\subsubsection{Treebanks and sembanks}
\label{cl:lang-doc:treebanks}
%Emily

\is{treebank|(}\is{sembank|(}% 
Annotated corpora are a particularly valuable type of resource that
can be derived from HPSG grammars.  Two important kinds are treebanks and sembanks.  A
\textit{treebank} is a collection of text where each sentence is associated with a syntactic
representation.  A \textit{sembank} has semantic representations (in some cases in addition to the
syntactic ones). Treebanks and sembanks can be used for linguistic research, as the analyses allow
for more detailed structure-based searches\is{corpus search!structure-based} for phenomena of
interest \citep{Rohde:05,Gho:Bir:10,Kou:Oep:14}.\footnote{The \isi{WeSearch} interface of
  \citet{Kou:Oep:14} can be accessed at \url{http://wesearch.delph-in.net/deepbank/search.jsp}
  (accessed 2019-08-16).}  In the context of language documentation and description, searchable
treebanks\is{treebank!searchable} can also be a valuable addition, helping readers connect prose
descriptions of linguistic phenomena to multiple examples in the corpus
\citep{Ben:Gho:Bal:Dri:12}. In natural language processing, treebanks and sembanks are critical
source material for training parsers (see Sections~\ref{cl:prac:rank} and~\ref{cl:downstream:data}).

Traditional treebanks are created by doing a certain amount of automatic processing on corpus data,
including possibly \isi{chunking} or context-free grammar\is{context-free grammar (CFG)} parsing,
and then hand-correcting the result \citep{Mar:San:Mar:93,Ban:Bon:Cai:13}.  While this approach is a
means to encode human insight about linguistic structure for later automatic processing, it is both
inefficient and potentially error-prone. The \isi{Alpino} project (\citealt{vanderbeek2002alpino};
see also Section~\ref{cl:other:alpino} above) addresses this by first parsing the text with a
broad-coverage HPSG-inspired grammar of \ili{Dutch} and then having annotators\is{annotation} select
among the parses.  The selection process is facilitated by allowing the annotators to mark
constituent\is{constituent!boundary} boundaries and to mark \is{lexical entry}lexical entries as
correct, possibly correct, or wrong.  These constraints reduce the search space for the parser and
consequently also the range of analyses the annotator has to consider before choosing the best
one. A facility for adding one-off lexical entries to handle misspellings, for example, helps
increase grammar \isi{coverage}.  Disambiguation\is{ambiguity}\is{disambiguation} is handled with
the aid of \isi{discriminant}s, as discussed in Section~\ref{cl:prac:rank} above. Finally, the
annotators may further edit analyses deemed insufficient. Though the underlying grammar is based on
HPSG, the treebank stores \isi{dependency graph}s instead.  The Alpino parser was similarly used to
construct the \isi{Lassy Treebanks} of written \ili{Dutch} \citep{van:bou:van:13}.  In more recent
work, these dependency representations have been mapped to the \isi{Universal Dependencies (UD)}
annotation standards \citep{Niv:Mar:Gin:16} to produce a UD treebank for \ili{Dutch}
\citep{Bou:Van:17}.

The \isi{Redwoods} project \citep{OFTM2004a-u} also produces grammar-driven treebanks, in this case
for \ili{English} and without any post-editing of the selected analyses.\footnote{There are also
  Redwoods-style treebanks for other languages, including the \isi{Hinoki Treebank} of
  \ili{Japanese} \citep{bond:etal:2004} and the \isi{Tibidabo Treebank} of \ili{Spanish}
  \citep{marimon:2015}.}  As with Alpino, this is done by first parsing the corpus with the grammar
and calculating the discriminants for each parse forest.  After annotation, the treebanking software
stores not only the final full HPSG analysis that was selected, but also the decisions the
annotator\is{annotation} made about each \isi{discriminant}.  Thus when the grammar is updated, for
example to refine the semantic representations, the corpus can be reparsed and the decisions
replayed, leaving only a small amount of further annotation work to be done to handle any additional
\isi{ambiguity} introduced by the grammar update.  The activity of treebanking in turn provides
useful insight into grammatical analyses, including sources of spurious
ambiguity\is{ambiguity!spurious} and phenomena that are not yet properly handled, and thus informs
and spurs on further grammar development.  A downside to strictly grammar-based treebanking is that
only items for which the grammar finds a reasonable parse can be included in the treebank. For many
applications, this is not a drawback, so long as there are sufficient and sufficiently varied
sentences that do receive analyses.

Finally, there are also automatically annotated\is{annotation!automatic} treebanks, which use a statistical parse-ranking model to select the best parse, instead of using a human annotator.  These are not as reliable as manually annotated treebanks, but they can be considerably larger.  \isi{WikiWoods}\footnote{\url{http://moin.delph-in.net/WikiWoods}, accessed 2019-08-16.}  covers 55 million sentences of \ili{English} (900 million tokens).  It was produced by \citet{flickinger2010wikiwoods} and \citet{solberg2012wikiwoods} from the July 2008 dump of the full \ili{English} \isi{Wikipedia}, using the ERG and PET, with parse ranking trained on the manually treebanked subcorpus \isi{WeScience} \citep{ytrestol2009wescience}.  As with the \isi{Redwoods} treebanks, WikiWoods is updated with each release of the ERG.%
%
\is{treebank|)}%
\is{sembank|)}%
%
\is{language documentation|)}%
\is{linguistic hypothesis testing|)}



\subsection{Downstream applications}
\label{cl:downstream}
%Guy

\largerpage
%\enlargethispage{3pt}
\is{application|(}%
In this section, we discuss the use of HPSG grammars for practical tasks.  There is a large number
of applications, and we focus on several important ones here.  In Section~\ref{cl:downstream:edu},
we cover educational applications where a grammar is used directly.  In
Section~\ref{cl:downstream:nlp}, we cover cases where a grammar is used to provide features to help
solve tasks in Natural Language Processing (NLP).  Finally, in Section~\ref{cl:downstream:data}, we
cover situations where a grammar is used to provide data for machine learning systems.\footnote{The
  \delphin\ community maintains an updated list of applications of \delphin\ \isi{software} and
  resources at \url{http://moin.delph-in.net/DelphinApplications} (accessed 2019-08-16).}



\subsubsection{Education}
\label{cl:downstream:edu}

\is{application!education|(}%
Precise syntactic analyses can be useful in \isi{language teaching},
in order to automatically identify errors and give feedback to the student.
In order to model common mistakes,
a grammar can be extended with so-called \textit{\isi{mal-rule}s}.
A mal-rule is like a normal rule, in that it licenses a construction,
and can be treated the same during parsing. However, given a parse,
the presence of a mal-rule indicates that the student needs to be given feedback
\citep{Ben:Fli:Oep:04,flickinger2013error,morgadodacosta2016error}.
A large-scale system implementing this kind of computer-aided teaching has been developed
by the \isi{Education Program for Gifted Youth} at Stanford University,
using the ERG\is{English Resource Grammar (ERG)} \citep{suppes2014teach}.
This system has reached tens of thousands of elementary and middle school children,
and has been found to improve the school results of underachieving children.
% Redbird (McGraw-Hill)

Another way to use an implemented grammar is to automatically produce teaching materials.  Given a \isi{semantic representation}, a grammar can generate\is{generation} one or more sentences.  \citet{Flickinger:17} uses the ERG\is{English Resource Grammar (ERG)} to produce practice exercises for a student learning \isi{first-order logic}.  For each exercise, the student is presented with an \ili{English} sentence and is supposed to write down the corresponding first-order logical form.\is{logical form}\is{first-order logic} By using a grammar, the system can produce syntactically varied questions and automatically evaluate\is{evaluation!automatic} the student's answer.%
\is{application!education|)}


\subsubsection{NLP tasks}
\label{cl:downstream:nlp}

\largerpage
\is{Natural Language Processing (NLP)|(}%
\is{statistical model|(}%
\is{machine learning|(}%
Much NLP work focuses on specific \textit{tasks}, where a system is presented with some input and
required to produce an output, with a clearly-defined metric to determine how well the system
performs.  HPSG grammars have been used in a range of such tasks, where the syntactic and semantic
analyses provide useful features.

\textit{Information retrieval}\is{information retrieval} is the task of finding relevant documents
for a given query.  For example, \citet{schaefer2011acl} present a tool for searching the ACL
Anthology, using the ERG.\is{English Resource Grammar (ERG)} \textit{Information
  extraction}\is{information extraction} is the task of identifying useful facts in a collection of
documents.  For example, \citet{reiplinger2012glossary} aim to identify definitions of technical
concepts from \ili{English} text, in order to automatically construct a glossary.  They find that
using the ERG reduces noise in the candidate definitions.  \citet{miyao2008protein} aim to identify
protein-protein interactions in the \ili{English} biomedical literature, using
\isi{Enju}.\is{biomedical NLP}

For these tasks, some linguistic phenomena are particularly important, such as
\isi{negation}\is{negation|(} and hedging\is{hedge} (including \isi{adverb}s like \textit{possibly},
\isi{modal}s like \textit{may}, and verbs of speculation\is{verb!of speculation} like
\textit{suggest}).  When it comes to identifying facts asserted in a document, a clause that has
been negated or hedged should be treated with caution.  \citet{mackinlay2012biomed} consider the
biomedical domain, evaluating on the \isi{BioNLP 2009 Shared Task}
\citep{kim2009task},\is{biomedical NLP} where they outperform previous approaches for
\isi{negation}, but not for \isi{speculation}.  \citet{velldal2012specneg} consider negation and
speculation in biomedical text, evaluating on the CoNLL 2010 Shared Task \citep{farkas2010task},
where they outperform previous approaches.  \citet{packard2014neg} propose a general-purpose method
for finding the scope of negation\is{scope!of negation} in an MRS, evaluating on the *SEM 2012
Shared Task \citep{morante2012task}.  They find that transforming the output of the ERG\is{English
  Resource Grammar (ERG)} with a relatively simple set of rules achieves high performance on this
\ili{English} dataset, and combining this approach with a purely statistical system outperforms
previous approaches.  \citet{zamaraeva2018pathology} use the ERG for \isi{negation} detection and
then use that information to refine the (machine-learning) features in a system that classifies
\ili{English} pathology reports, thereby improving system performance.\is{biomedical NLP} A common
finding from these studies is that a system using the output of the ERG tends to have high
\isi{precision} (items identified by the system tend to be correct) but low \isi{recall} (items are
often overlooked by the system).  One reason for low recall is that the grammar does not cover all
sentences in natural text.  As we will see in Section~\ref{cl:downstream:data}, recent work on
\isi{robust parsing} may help to close this \isi{coverage} gap.

Negation\is{negation} resolution is also included in \citeauthor{oepen2017extrinsic}'s
(\citeyear{oepen2017extrinsic}) \isi{Shared Task on Extrinsic Parser Evaluation}.  As mentioned in
Section~\ref{cl:prac:dep}, \isi{dependency graph}s can provide a useful tool in NLP tasks, and this
shared task aims to evaluate the use of dependency graphs (both semantic and syntactic) for three
downstream applications: biomedical information extraction,\is{information extraction!biomedical}
negation resolution, and fine-grained opinion analysis.\is{opinion analysis} Some participating
teams use DM\is{delphin MRS Dependencies@\delphin\ MRS Dependencies (DM)} dependencies
%\footnote{DM \delphin\ MRS dependencies; see (\ref{cl:fig:dm}).} 
\citep{schuster2017dep,chen2017dep}.  The results of this shared task suggest that, compared to
other dependency representations, DM is particularly useful for negation resolution.\is{negation|)}

Another task where dependency graphs have been used is \textit{\isi{summarization}}.  Most existing
work on this task focuses on so-called \textit{extractive
  summarization}:\is{summarization!extractive} given an input document, a system forms a summary by 
extracting short sections of the input.  This is in contrast to \textit{abstractive
  summarization},\is{summarization!abstractive} where a system generates new text based on the input
document.  Extractive summarization is limited, but widely used because it is easier to implement.
However, \citet{fang2016summarise} show how a wide-coverage grammar like the ERG\is{English Resource
  Grammar (ERG)} makes it possible to implement an abstractive summarizer with state-of-the-art
performance.  After \isi{parsing} the input document into logical propositions, the summarizer
prunes the set of propositions using a cognitively inspired model.  A summary is then generated
based on the pruned set of propositions.  Because no text is directly extracted from the input
document, it is possible to generate a more concise summary.

\largerpage[2]
Finally, no discussion of NLP tasks would be complete without including \textit{\isi{machine translation}}.  A traditional grammar-based approach uses three grammars: a grammar for the \isi{source language}, a grammar for the \isi{target language}, and a \textit{\isi{transfer grammar}}, which converts semantic representations for the source language to semantic representations for the target language \citep{OVL2007a-u,bond2011deep}.  Translation proceeds in three steps: parse the source sentence, transfer the semantic representation, and generate a target sentence.  The transfer grammar is needed both to find appropriate lexical items and also to convert semantic representations when languages differ in how an idea might be expressed.  The difficulty in writing a transfer grammar that is robust enough to deal with arbitrary input text means that statistical systems might be preferred.  \citet{horvat2017translate} explores the use of statistical techniques, skipping over the transfer stage: a target-language sentence is generated directly from a semantic representation for the source language.  \citet{goodman2018translate} explores the use of statistical techniques within the paradigm of parsing, transferring, and generating.%
%TODO expand?
\is{application|)}
%TODO check:
% http://www.let.rug.nl/vannoord/papers/
% https://www.let.rug.nl/~vannoord/Lassy/
% http://heartofgold.dfki.de/Publications_Applications.html
% http://www.nactem.ac.uk/enju/



\subsubsection{Data for machine learning}
\label{cl:downstream:data}

In Section~\ref{cl:downstream:nlp},
we described how HPSG grammars can be directly incorporated into NLP systems.
Another use of HPSG grammars in NLP
is to generate data on which a statistical system can be trained.

For example, one limitation of using an HPSG grammar in an NLP system
is that the grammar is unlikely to cover all sentences in the data
\citep{flickinger2012deepbank}.
One way to overcome this \isi{coverage} gap
is to train a statistical system to produce the same output as the grammar.
The idea is that the trained system will be able
to generalize to sentences that the grammar does not cover.
\citet{oepen2014semeval}, \citet{oepen2015semeval}, and \citet{Oep:Abe:Haj:19}
present shared tasks on semantic dependency parsing,\is{dependency parsing!semantic}
including both DM dependencies\is{delphin MRS Dependencies@\delphin\ MRS Dependencies (DM)}
and \isi{Enju} \isi{predicate-argument structure}s.
As of 2015, the best-performing systems in these shared tasks
could already produce dependency graphs almost as accurately as grammar-based parsers
(for sentences where the grammar has coverage).
Similarly, \citet{buys2017parse} develop a parser
for EDS\is{Elementary Dependency Structures (EDS)}
and DMRS\is{Dependency Minimal Recursion Semantics (DMRS)}
which performs almost as well as a grammar-based parser,
but has full \isi{coverage}, and can run 70 times faster.

\largerpage
In fact, in more recent work, the difference in performance has been effectively closed.
\citet{chen2018parse} consider parsing to EDS and DMRS graphs,
and actually achieve slightly higher accuracy with their system,
compared to a grammar-based parser.
Unlike the previous statistical approaches,
\citeauthor{chen2018parse} do not just train on the desired dependency graphs,
but also use information in the phrase-structure trees.
They suggest that using this information allows their system
to learn compositional rules mirroring composition\is{compositionality} in the grammar,
which thereby allows their system to generalize better.
%TODO numbers in the above paragraphs?

Another application of HPSG-derived dependency graphs
is for \textit{\isi{distributional semantics}}.
Here, the aim is to learn the meanings of words from a corpus,
exploiting the fact that the context of a word tells us something about its meaning.
This is known as the \textit{\isi{distributional hypothesis}},
an idea with roots in \isi{American structuralism} \citep{harris1954distribution}
and \isi{British lexicology} \citep{firth1951collocation,firth1957company}.
Most work on distributional semantics learns a \textit{\isi{vector space model}},
where the meaning of each word is represented as a point in a high-dimensional vector space
(for an overview, see \citealt{erk2012vector} and \citealt{clark2015vector}).
However, \citet{emerson2018functional} argues that
vector space models cannot capture various aspects of meaning,
such as logical structure, and phenomena like polysemy.
Instead, \citeauthor{emerson2018functional} presents a distributional model
which can learn \isi{truth-conditional semantics},
using a parsed corpus like \isi{WikiWoods} (see Section~\ref{cl:lang-doc:treebanks}).
This approach relies on the semantic analyses given by a grammar,
as well as the infrastructure to parse a large amount of text.

Finally, there are also applications which use grammars not to parse,
but to generate.\is{generation}
\citet{kuhnle2018shapeworld} consider the task of
\textit{visual question answering},\is{visual question answering (VQA)}
where a system is presented with an image and a question about the image,
and must answer the question.
This task requires \isi{language understanding},
\isi{reference resolution}, and \isi{grounded reasoning},
in a way that is relatively well-defined.
However, for many existing datasets, there are biases in the questions
which mean that high performance can be achieved without true language understanding.
%TODO references
For this reason, there is increasing interest in \isi{artificial dataset}s,
which are controlled to make sure that high performance requires true understanding.
\citeauthor{kuhnle2018shapeworld} present \isi{ShapeWorld},
a configurable system for generating artificial data.
The system generates an abstract representation of a scene
(colored shapes in different configurations),
and then generates an image and a caption based on this representation.
The use of a broad-coverage grammar is crucial
in allowing the system to be configurable
and scale across a variety of syntactic constructions.%
\is{machine learning|)}%
\is{statistical model|)}%
\is{Natural Language Processing (NLP)|)}

%TODO Alex's more recent papers?
% https://arxiv.org/pdf/1809.03044.pdf
% https://arxiv.org/pdf/1812.11737.pdf


\section{Linguistic insights}
\label{cl:insight}

\largerpage
In Section~\ref{cl:lang-doc} above, we described multiple ways in which computational methods can be
used in the service of linguistic research, especially in testing linguistic hypotheses. Here, we
highlight a few ways in which grammar engineering work in HPSG has turned up linguistic insights
that had not previously been discovered through non-computational means.\footnote{For similar
  reflections from the point of view of LFG,\indexlfg see \citew{King:16}.}

\subsection{Ambiguity} %Guy
\is{ambiguity|(}

As discussed in Section~\ref{cl:prac:rank},
the scale of ambiguity has become clear now that
broad-coverage precision grammars are available.\is{broad-coverage grammar}
By taking both \isi{coverage} and \isi{precision} seriously,
it is possible to investigate it on a large scale,
quantifying the sources of ambiguity and the information needed to resolve it.
For example, \citet{Tou:Man:Shi:Fli:Oep:02,Tou:Man:Fli:Oep:05}
found that in the \isi{Redwoods} \isi{treebank} (3rd Growth),
roughly half of the ambiguity was lexical,\is{ambiguity!lexical} and half syntactic.\is{ambiguity!syntactic}
% Baseline accuracy: .227 ~ choosing out of 4.41 parses
% Accuracy given lexical tags: .488 ~ choosing out of 2.05 parses, roughly square root of above
They also showed how combining sources of information
(such as both semantic and syntactic information)
is important for resolving ambiguity,
and argue that using multiple kinds of information in this way
is consistent with probabilistic approaches in psycholinguistics.\is{ambiguity|)}

\subsection{Long-tail phenomena} % Emily
\is{periphery|(}

One of the strengths of HPSG as a theoretical framework is that it allows for
the analysis of both ``core\is{core grammar}'' and ``peripheral''\is{periphery} phenomena within a single, integrated model.
Indeed, by treebanking large corpora,
it becomes possible to investigate the extent to which
a particular phenomenon could be considered ``core'' or ``peripheral'' within a language.
Furthermore, by implementing large-scale grammars across a range of languages,
it also becomes possible to investigate the extent to which
a phenomenon could be considered ``core'' or ``peripheral'' across languages \citep{MuellerKernigkeit}.

\largerpage
\begin{sloppypar}
In fact, when working with actual data and large-scale grammars,
it quickly becomes apparent just how long the long-tail of ``peripheral'' phenomena is. Furthermore,
the sustained development of broad-coverage linguistic resources makes it possible to bring into
view more and more low"=frequency phenomena (or low"=frequency variations on relatively
high-frequency phenomena). A case in point is the range of \isi{raising} and \isi{control}
\isi{valence} frames found in the ERG\is{English Resource Grammar (ERG)}
\citep{Flickinger2000a,Flickinger2011a-u}. As of the 2018 release, the ERG\il{English} includes over 60 types
for raising and control predicates, including \isi{verb}s, \isi{adjective}s, and \isi{noun}s,
many of which are not otherwise discussed in the syntactic literature. These include such
low"=frequency types as the one for \textit{incumbent}, which requires an expletive \textit{it}
subject, an obligatory \textit{(up)on} PP complement, and an \isi{infinitival VP} complement,
and which establishes a control relation between the object of \textit{on} and the VP's missing subject:\footnote{%
	Our thanks to Dan Flickinger for this example.
}
\end{sloppypar}

\begin{exe}
\ex\label{cl:incumbent} 
It is incumbent on you to speak plainly.
\end{exe}%
%
\is{periphery|)}

\subsection{Analysis-order effects}
\is{analysis order!effect of|(}

Grammar engineering means making analyses
specific and then being able to build on them. This has both benefits
and drawbacks: on the one hand, it means that additional grammar
engineering work can build directly on the results of previous
work. It also means that any additional grammar engineering work is
constrained by the work it is building on.  \citet{Fokkens:14} observes
this phenomenon and notes that it introduces artifacts: the form an
implemented grammar takes is partially the result of the order in
which the grammar engineer considered phenomena to implement. This is
probably also true for non-computational work, as theoretical ideas
developed with particular phenomena (and, indeed, languages) in mind
influence the questions with which researchers approach additional
phenomena. \citeauthor{Fokkens:14} proposes that the methodology of
\textit{\isi{meta-grammar}} engineering can be used to address this problem: using
her \isi{CLIMB} methodology, rather than deciding between analyses of a
given phenomenon without input from later-studied phenomena, the
grammar engineer can maintain multiple competing analyses through time
and break free, at least partially, of the effects of the timeline of
grammar development. The central idea is that the grammar writer develops
a \isi{meta-grammar}, like the Grammar Matrix customization system\is{Grammar Matrix!customization system} (see Section~\ref{cl:lang-doc:gmcs}),
but for a single language. This customization system maintains
alternate analyses of particular phenomena which are invoked via grammar
specifications so the different versions of the grammar can be compiled
and tested.\is{analysis order!effect of|)}


\section{Summary}

In this chapter, we have attempted to illuminate the landscape of
computational work in HPSG. We have discussed how HPSG as a theory
supports computational work, described large-scale computational
projects that use HPSG, highlighted some applications of implemented
grammars in HPSG, and explored ways in which computational work can
inform linguistic research. This field is very active and our overview is
necessarily incomplete. Nonetheless, it is our hope that the pointers
and overview provided in this chapter will serve to help interested readers
connect with ongoing research in computational linguistics using HPSG.

% EMB 2019-02-21 Okay, that's kinda uninspired, but the ``summary'' is
% no longer empty, at least.

% St. Mü. =:-)

%\section*{Abbreviations}
\section*{\acknowledgmentsUS}

We would like to thank Stephan Oepen for helpful comments on an early
draft of this chapter, Stefan Müller for detailed comments as
volume editor and Elizabeth Pankratz for careful copy editing.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->
