\documentclass[output=paper,nonflat]{langsci/langscibook} 
\author{%
	Emily M.\ Bender\affiliation{University of Washington}%
	\lastand Guy Emerson\affiliation{University of Cambridge}
}
\title{Computational linguistics and grammar engineering}

% \chapterDOI{} %will be filled in at production

\abstract{%
	We discuss the relevance of HPSG for computational linguistics,
	and the relevance of computational linguistics for HPSG.
	% TODO
}



\IfFileExists{../localcommands.tex}{%hack to check whether this is being
compiled as part of a collection or standalone
  \input{../localpackages}
  \input{../localcommands}
  \togglepaper
}{}




\begin{document}

\maketitle

\label{chap-cl}

% Comments from Gerald:

%I assume that the contributors would agree with me that this handbook is probably the wrong place to go into great technical detail on mathematical or computational subjects, but there needs to be at least something there to serve as a starting point from which the reader could choose to jump into some of the papers cited.‎ A summary of the major trends and the high-level intuitions behind a lot of the technical terminology that readers will encounter in HPSG-related CL papers, for example, would both be most welcome.

\section{Introduction}

% First draft: Emily

From the inception of HPSG in the 1980s,
there has been a close integration between theoretical and computational work
\crossrefchapterp[for an overview, see]{evolution}.
%\citep{FIXME-CLobit-or-other}.
% at HP Labs in the joint work of Ivan Sag, Geoff Pullum, Tom Wasow, Mark Gawron, Carl Pollard and Dan Flickinger
% GE: I think this list of authors is too long for a first sentence.  Do we need to do more than refer to the other chapter?
% EMB 2018-09-28 sounds good!
In this chapter, we discuss computational work in HPSG,
starting with the infrastructure that supports it (both theoretical and practical) in Section~\ref{cl:infrastructure}.
Next we describe several existing large-scale projects which build HPSG or HPSG-inspired grammars
(see Section~\ref{cl:resources})
and the deployment of such grammars in applications including both those within linguistic research and otherwise
(see Section~\ref{cl:deployment}).
Finally, we turn to linguistic insights gleaned from broad-coverage grammar development
(see Section~\ref{cl:insight}).

% EMB 2018-07-26 That seems like a relatively weak intro, but putting it in as a place-holder for now. 

% EMB 2018-07-26 Also, I wonder if ``lessons for linguistics'' might not come across as condescending...
% GE 2018-07-31 Yes, maybe best to reword that somehow... ``Linguistic insights''?
% EMB 2018-09-28 Thanks.

\section{Infrastructure}
\label{cl:infrastructure}

\subsection{Theoretical considerations}
\label{cl:theoretical}

There are several properties of HPSG as a theory that make it well-suited to computational implementation. First, the theory is kept separate from the formalism: the formalism is expressive enough to encode a wide variety of possible theories. While some theoretical work does argue for or against the necessity of particular formal devices (e.g.\ the shuffle operator \citep{Reape94a}), much of it proceeds within shared assumptions about the formalism. This is in contrast to work in the context of the Minimalist Program \citep{Chomsky93b-u}, where theoretical results are typically couched in terms of modifications to the formalism itself. From a computational point of view, the benefit of differentiating between theory and formalism is that it means that the formalism is relatively stable. That in turns enables the development and maintenance of software systems that target the formalism, for parsing, generation, and grammar exploration (see Section~\ref{cl:history} below for some examples).\footnote{There are implementations of Minimalism, notably \citealt{Stabler97a-u} and \citealt{Herring:16}. However, writing an implementation requires fixing the formalism, and so these are unlikely to be useful for testing theoretical ideas as the theory moves on.}

A second important property of HPSG that supports a strong connection between theoretical and computational work is an interest in both so-called `core' and so-called `peripheral' phenomena. Most implemented grammars are built with the goal of handling naturally occurring text.\footnote{It is possible, but less common, to do implementation work strictly against testsuites of sentences constructed specifically to focus on phenomena of interest.} This means that they will need to handle a wide variety of linguistic phenomena not always treated in theoretical syntactic work \citep{Baldwin-et-al-04,Baldwin-et-al-05}. A syntactic framework that excludes research on `peripheral' phenomena as uninteresting provides less support for implementational work than does one, like HPSG or Construction Grammar, that values such topics
\crossrefchapterp[for a comparison of HPSG and Construction Grammar, see]{cxg}.

Finally, the type hierarchy characteristic of HPSG lends itself well to developing broad-coverage grammars which are maintainable over time (see \citealt{Syg:Win:11}). The use of the type hierarchy to manage complexity at scale comes out of the work of \citet{Flickinger87} and others at HP labs in the project where HPSG was originally developed. The core idea is that any given constraint is (ideally) expressed only once on types which serve as supertypes to all entities that bear that constraint.\footnote{Originally this only applied to lexical entries in Flickinger's work. Now it also applies phrase structure rules, lexical rules, and types below the level of the sign which are used in the definition of all of these.} Such constraints might represent broad generalizations that apply to many entities or relatively narrow, indiosyncratic properties. By isolating any given constraint on one type (as opposed to repeating it in mutiple places), we build grammars that are easier to update and adapt in light of new data that require refinements to constraints. Having a single locus for each constraint also makes the types a very useful target for documentation \citep{Hashimoto-etal:07} and grammar exploration \citep{Letcher:18}. 


\subsection{Practical considerations}
\label{cl:prac}

The formalism of HPSG allows practical implementations,
since feature structures are well-defined data structures.
Furthermore, because HPSG is defined to be bi-directional,
an implemented grammar can be used for both parsing and generation.
In this section, we discuss how HPSG allows tractable algorithms,
which enables linguists to empirically test hypotheses,
and which also enables HPSG grammars to be used in a range of applications,
as we will see in Sections~\ref{cl:lang-doc} and~\ref{cl:downstream}, respectively.

\subsubsection{Computational complexity}
\label{cl:prac:comp}

One way to measure how easy or difficult it is to use a syntactic theory in practical computational applications
is to consider the \textit{computational complexity} of parsing and generation algorithms \citep{gazdar1985complexity}.
For example, we can consider how much computational time
a parsing algorithm needs to process a particular sentence.
For longer sentences, we would expect the amount of time to increase,
but the more complex the algorithm is,
the more quickly the amount of time increases.
If we consider sentences containing $n$~tokens,
we can find the average amount of time taken,
or the longest amount of time taken.
We can then increase~$n$, and see how the amount of time changes,
both in the average case, and in the worst case.

At first sight, analysing computational complexity
would seem to paint HPSG in a bad light,
because the formalism allows us to write grammars
which can be arbitrarily complex;
in technical terminology, the formalism is \textit{Turing-complete}
\citep[Section~3.4]{Johnson88}.
However, as discussed in the previous section,
there is a clear distinction between theory and formalism.
Although the feature-structure formalism rules out the possibility of efficient algorithms
that could cope with any possible feature-structure grammar,
a particular theory (or a particular grammar) might well allow efficient algorithms.

The difference between theory and formalism
becomes clear when comparing HPSG to other computationally-friendly frameworks,
such as Combinatory Categorial Grammar (CCG),\footnote{%
	For an introduction, see \citet{steedman2011ccg}.
	For a comparison with HPSG, see \crossrefchaptert{cg}.
}
or Tree Adjoining Grammar \citep[TAG;][]{Joshi87a-u,SAJ88a-u}).
The formalisms of CCG and TAG inherently limit computational complexity:
for both of them, as the sentence length~$n$ increases,
worst-case parsing time is proportional to~$n^6$ \citep{Kasamietal1989}.
This is a deliberate feature of these formalisms,
which aim to be just expressive enough to capture human language,
and not any more expressive.
Building this kind of constraint into the formalism itself
highlights a different school of thought from HPSG.
Indeed, \citet{MuellerCoreGram} explicitly argues
in favor of developing linguistic analyses first,
and improving processing efficiency second.
As discussed above in Section~\ref{cl:theoretical},
separating the formalism from the theory
means that the formalism is stable, even as the theory develops.

% EMB 2018-09-28 Say something here about particular analytical choices that constrain the processing complexity? (The bit about the fact that the lists that we read from tend to be limited in length.)
% EMB 2018-09-28 Maybe also cite Flickinger 2000 (`On building a more efficient grammar using types') and Copestake et al work on improving generator efficiency.
% TODO

It would be beyond the scope of this chapter
to give a full review of parsing algorithms,
but it is instructive to give an example.
For grammars that have a context-free backbone
(we can express every derivation as a phrase-structure tree
plus constraints between mother and daughter nodes),
it is possible to adapt the standard chart-parsing algorithm for context-free grammars.
The basic idea is to parse ``bottom-up'', % through the tree
starting by finding analyses for each token in the input,
and then finding analyses for increasingly longer sequences of tokens,
until we reach the entire sentence.
The resulting algorithm is more computationally complex than for a context-free grammar,
because we are dealing with feature structures, rather than nonterminal symbols.
While a context-free grammar allows a finite number of nonterminals,
a feature-structure grammar may allow a infinite number of possible feature structures.
For an HPSG grammar without recursive unary rules,
this algorithm has a worst-case complexity of exponential time.
This is less complex than for an arbitrary grammar
(which means that this class of grammars is \emph{not} Turing-complete),
but more complex than for CCG or TAG.

However, when parsing real corpora,
it turns out that the average-case complexity is much better than we might expect \citep{Carroll94}.
On the one hand, grammatical constructions do not generally combine in the worst-case way,
and on the other hand, when a grammar writer is confronted
with multiple possible analyses for a particular construction,
they may opt for the analysis that is more efficient for a particular parsing algorithm.
\citet{OF98} describe the use of test-suites for profiling
the time efficiency of a grammar and parsing algorithm,
when running on a representative sample of text.

% TODO citations -- maybe look at evolution.tex, {Implementations and Applications of HPSG}
% GE: Maybe this last paragraph is too terse? [edit: now split into two]
% EMB 2018-09-28: Yes, a little terse
% --- I think maybe starting with how the derivations are like and unlike CFG derivations
% and then going to `bottom-up' chart parsing (with citations!) would help.
% Also, the discussion of complexity has too many contrasts:
% infinite, but not Turing-complete, but not even that bad when parsing real corpora.
% It would also be good to point to some numbers for ``parsing real corpora''
% ... Dan or Stephan or maybe Bec should have published some, I think.
% TODO




\subsubsection{Parse ranking}
\label{cl:prac:rank}

Various kinds of ambiguity are well-known in linguistics
(such as modifier attachment and part-of-speech assignment),
to the point that examples like \rref{cl:ambig-telescope} are stock in trade:

\begin{exe}
\ex\label{cl:ambig-telescope} 
\begin{xlist}
\item I saw the kid with the telescope.
\item Visiting relatives can be annoying.
\end{xlist}
\end{exe}

\noindent
A well-constructed grammar should be expected to return multiple
parses for each ambiguous sentence.

However, people are naturally very good at resolving ambiguity,
which means most ambiguity is not apparent, even to linguists.
It is only with the development of large-scale grammars that the sheer scale of ambiguity has become clear.
% EMB 2018-10-03 What's rref?
% GE 2018-11-27 an example in brackets (from localcommands.tex)
% (normally, I'd prefer \cref from cleveref, but we don't seem to be using that for this handbook)
For example, \rref{cl:ex:ambig-simple}~might seem unambiguous,
but there is a second reading, where \textit{my favorite} is the topicalized object of \textit{speak},
which would mean that town criers generally speak the speaker's favorite thing (perhaps a language) clearly.
There is also a third, even more implausible reading, where \textit{my favorite town} is the topicalized object.
Such implausible readings don't easily come to mind,
and in fact, the 2018 version of the English Resource Grammar \citep{Flickinger2000a,Flickinger2011a-u}
gives a total of 21 readings for this sentence.
With increasingly long sentences, such ambiguities stack up very quickly.
For~\rref{cl:ex:ambig-many}, the first line of a newspaper article,\footnote{%
	\url{https://www.theguardian.com/science/2018/aug/22/offspring-of-neanderthal-and-denisovan-identified-for-first-time}
}
the ERG gives 35,094 readings.

\begin{exe}
\ex\label{cl:ex:ambig-simple}
My favorite town criers speak clearly.
\ex\label{cl:ex:ambig-many}
A small piece of bone found in a cave in Siberia has been identified
as the remnant of a child whose mother was a Neanderthal and father was a Denisovan,
a mysterious human ancestor that lived in the region.
\end{exe}

% EMB 2018-09-28 Editing a bit through here:
% GE thanks!
While exploring ambiguity can be interesting for a linguist,
typical practical applications require just one parse per input sentence
and specifically the parse that best reflects the intended meaning
(or only the top few parses, in case the one put forward as `best' might be wrong).
Thus, what is required is a \textit{ranking} of the parses,
so that the application can only use the most highly-ranked parse,
or the top~$N$ parses.

Parse ranking is not usually determined by the grammar itself,
because of the difficulty of manually writing disambiguation rules.
Typically, a statistical system is used \citep{Tou:Man:Shi:Fli:Oep:02,Tou:Man:Fli:Oep:05}.
First, a corpus is \textit{treebanked}:
for each sentence in the corpus,
an annotator (often the grammar writer) chooses the best parse,
out of all parses produced by the grammar.
The set of all parses for a sentence is often referred to as the \textit{parse forest},
and the selected best parse is often referred to as the \textit{gold standard}.
Given the gold parses for the whole corpus, a statistical system is trained
to predict the gold parse from a parse forest,
based on many features\footnote{%
	In the machine-learning sense of ``feature'',
	not the feature-structure sense.
}
of the parse.
From the example in~\rref{cl:ex:ambig-simple},
a number of a different features all influence the preferred interpretation:
the likelihood of a construction (such as topicalization),
the likelihood of a valence frame (such as transitive \textit{speak}),
the likelihood of a collocation (such as \textit{town crier}),
the likelihood of a semantic relation (such as speaking a town),
and so on.
%TODO briefly explain MaxEnt parse ranking?
%TODO references...
% EMB 2018-09-28 For DELPH-IN/Redwoods treebanking: 
% GE thanks!

Because of the large number of possible parses,
it can be helpful to \textit{prune} the search space:
rather than ranking the full set of parses,
we can restrict attention to a smaller set of parses,
which hopefully includes the correct parse.
By carefully choosing how to restrict the parser's attention,
we can drastically reduce processing time without hurting parsing accuracy.
One method, called \textit{supertagging},\footnote{%
	The name refers to \textit{part-of-speech tagging},
	which predicts a part-of-speech for each input token,
	from a relatively small set of part-of-speech tags.
	Supertagging is ``super'', in that it predicts detailed lexical entries, rather than simple tags.
}
exploits the fact that HPSG is a lexicalized theory:
choosing the correct lexical entry brings in rich information
that can be exploited to rule out many possible parses.
Thus if the correct lexical entry can be chosen prior to parsing
(e.g.\ on the basis of the prior and following words),
the range of possible analyses the parser must consider is drastically reduced.
Although there is a chance that the supertagger will predict the wrong lexical entry,
using a supertagger can often improve parsing accuracy,
by ruling out parses that the parse-ranking model might incorrectly rank too high.
Supertagging was first applied to HPSG by \citet{matsuzaki2007supertag},
building on previous work for TAG \citep{bangalore1999supertag}
and CCG \citep{clark2004supertag}.
To allow multi-word expressions (such as \textit{by and large}),
where the grammar assigns a single lexical entry to multiple tokens,
\citet{dridan2013ubertag} has proposed an extension of supertagging, called \textit{ubertagging},
which jointly predicts both a segmentation of the input and supertags for those segments.
\citeauthor{dridan2013ubertag} manages to increase parsing speed by a factor of four,
while also improving parsing accuracy.
% Pruning via chunking? Muszyńska http://aclweb.org/anthology/P16-3014

% EMB 2018-09-28 Doesn't Bec also bring in the lexical rules? Maybe that's too much detail for here.

Finally, in order to train these statistical systems,
we need to first annotate a treebank.
When there are many parses for a sentence,
it can be time-consuming to select the best parse.
To efficiently use an annotator's time,
it can be helpful to use \textit{discriminants},
properties which hold for some parses but not for others \citep{Carter:97}.
For example, discriminants might include
whether to analyse an ambiguous token as a noun or a verb,
or where to attach a prepositional phrase.
This approach to treebanking also means that
annotations can be re-used when the grammar is updated
\citep{OFTM2004a-u,Fli:Oep:Ben:17}.

% Self-training?
% e.g. for Alpino: 
%% Gertjan van Noord. Self-trained Bilexical Preferences to Improve Disambiguation Accuracy. In: Harry Bunt, Paola Merlo and Joakim Nivre (editors), Trends in Parsing Technology. Dependency Parsing, Domain Adaptation, and Deep Parsing. Springer Verlag. pp 183-200. 2010.


\subsubsection{Semantic dependencies}
\label{cl:prac:dep}

\begin{figure}
\centering
\begin{subfigure}{\textwidth}
	\centering
	\begin{avm}
	[\asort{mrs}
		hook & [\asort{hook}
			ltop & @1 \\
			index & @2 ] \\
		rels & \Bigg\langle\, % Manual < for easier linebreak
			[\asort{relation}
				pred & \_the\_q \\
				lbl & @3 \\
				arg0 & @4 \\
				rstr & @5 \\
				body & @6 ],
			[\asort{relation}
				pred & compound \\
				lbl & @7 \\
				arg0 & @8 \\
				arg1 & @4 \\
				arg2 & @9 ],
			[\asort{relation}
				pred & udef\_q \\
				lbl & @{10} \\
				arg0 & @9 \\
				rstr & @{11} \\
				body & @{12} ],
\\ & \hphantom{\Bigg\langle}\,  % line break for long list
			[\asort{relation}
				pred & \_cherry\_n\_1 \\
				lbl & @{13} \\
				arg0 & @9 ],
			[\asort{relation}
				pred & \_tree\_n\_of \\
				lbl & @7 \\
				arg0 & @4 ],
			[\asort{relation}
				pred & \_blossom\_v\_1 \\
				lbl & @1 \\
				arg0 & @2 \\
				arg1 & @4 ] \;\Bigg\rangle \\
		hcons & \Bigg\langle\,
			[\asort{qeq}
				harg & @5 \\
				larg & @7 ],
			[\asort{qeq}
				harg & @{11} \\
				larg & @{13} ] \;\Bigg\rangle ]
	\end{avm}
	\caption{MRS, as a feature structure.}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\centering
	\setlength{\abovedisplayskip}{1ex}
	\setlength{\belowdisplayskip}{1ex}
	\begin{gather*}
		\textsc{index} : e_1 \\
		\begin{aligned}
			l_1 &: \textit{\_the\_q}(x_1,h_1,h_2),\; h_1 \,\textsc{qeq}\, l_4 \\
			l_2 &: \textit{udef\_q}(x_2,h_3,h_4),\; h_3 \,\textsc{qeq}\, l_3 \\
			l_3 &: \textit{\_cherry\_n\_1}(x_2) \\
			l_4 &: \textit{\_tree\_n\_of}(x_1), \textit{compound}(e_2,x_1,x_2) \\
			\textsc{ltop}\quad l_5 &: \textit{\_blossom\_v\_1}(e_1,x_1)
		\end{aligned}
	\end{gather*}
	\caption{MRS, abstractly.}
\end{subfigure}
\caption{%
	Minimal Recursion Semantics (MRS)
	representation for the sentence \emph{The cherry tree blossomed},
% EMB 2018-12-17 I approve of this example sentence!
	as produced by the English Resource Grammar (ERG).
	For simplicity, we have omitted some details, including
	features such as number and tense,
	individual constraints (ICONS),
	and the use of difference lists.
}
\label{cl:fig:mrs}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{\textwidth}
	\centering
	\begin{tikzpicture}[node distance=8mm and 12mm, inner sep=2mm, thick]
		\node (ch) {\it \_cherry\_n\_1};
		\node[right=of ch] (cm) {\it compound};
		\node[right=of cm] (tr) {\it \_tree\_n\_of};
		\node[right=of tr] (b) {\it \_blossom\_v\_1};
		\node[above=of ch] (u) {\it udef\_q};
		\node[above=of tr] (th) {\it \_the\_q};
		\draw[->] (cm) -- (ch) node[midway, above] {\textsc{arg2/neq}};
		\draw[->] (cm) -- (tr) node[midway, above] {\textsc{arg1/eq}};
		\draw[->] (b) -- (tr) node[midway, above] {\textsc{arg1/neq}};
		\draw[->] (u) -- (ch) node[midway, right, yshift=1mm] {\textsc{rstr/qeq}};
		\draw[->] (th) -- (tr) node[midway, right, yshift=1mm] {\textsc{rstr/qeq}};
		\node[anchor=north, align=center, inner sep=0pt] at (b.south) {\textsc{ltop}\\\textsc{index}};
	\end{tikzpicture}%
	\vspace*{2mm}%
	\caption{
		Dependency Minimal Recursion Semantics (DMRS)
		is as expressive as MRS.
		Predicates are represented as nodes,
		while semantic roles and scopal constraints are represented as links.
	}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\centering
	\vspace*{3mm}
	\begin{tikzpicture}[node distance=15mm, inner sep=2mm, thick]
		\node (th) {\it the};
		\node[right=of th] (ch) {\it cherry};
		\node[right=of ch] (tr) {\it tree};
		\node[right=of tr] (b) {\it blossomed};
		\draw[->] (ch) -- (tr) node[midway, above] {\textsc{compound}};
		\draw[->] (b) -- (tr) node[midway, above] {\textsc{arg1}};
		\draw[->] (th) to[out=90, in=90, looseness=.4] node[midway, above, inner sep=1mm] {\textsc{bv}} (tr);
		\node[anchor=north, inner sep=0pt] at (b.south) {\textsc{top}};
	\end{tikzpicture}
	\vspace*{2mm}
	\caption{
		\delphin\ MRS Dependencies (DM)
		are a simplified version of MRS.
		Nodes are the tokens in the sentence (rather than abstract predicates),
		and some scopal information is dropped.
	}
\end{subfigure}
\caption{%
	MRS-based dependency graphs for the sentence
	\emph{The cherry tree blossomed},
	based on the MRS given in Figure~\ref{cl:fig:mrs}.
}
\label{cl:fig:dep}
\end{figure}

In practical applications of HPSG grammars,
the full derivation trees and the full feature structures are often unwieldy,
containing far more information than necessary for the task at hand.
It is therefore often desirable to extract a concise semantic representation.
%TODO check the Semantics chapter once it's written

In computational linguistics,
a popular approach to semantics is to represent the meaning of a sentence as a \textit{dependency graph},
as this enables the use of graph-based algorithms.\footnote{%
	In this section, we are concerned with \emph{semantic} dependencies.
	For \emph{syntactic} dependencies, see \crossrefchaptert{dg}.
}
Several types of dependency graph have been proposed
based on Minimal Recursion Semantics \citep[MRS;][]{CFPS2005a},
with varying levels of simplification.
The most expressive is Dependency Minimal Recursion Semantics \citep[DMRS;][]{copestake2009dmrs},
which is fully interconvertible with MRS.\footnote{%
	More precisely, there is a one-to-one correspondence between DMRS and MRS structures,
	if every predicate as a unique \textit{intrinsic variable}.
	As observed by \citet{oepen2006eds}, this allows a variable-free semantic representation,
	by replacing each reference to a variable with a reference to the corresponding predicate.
}
In contrast, Elementary Dependency Structures \citep[EDS;][]{oepen2006eds}
lose some scope information,
which, for many applications, is less important than predicate-argument structure.
Finally, \delphin\ MRS Dependencies \citep[DM;][]{ivanova2012dm}
express predicate-argument structure purely in terms of the surface tokens,
without introducing any abstract predicates.
A comparison of MRS, DMRS, and DM is given in Figures \ref{cl:fig:mrs} and~\ref{cl:fig:dep}.
The existence of such dependency graph formalisms,
as well as software packages to manipulate such graphs \citep{copestake2016pydmrs},
has made it easier to use HPSG grammars in a number of practical tasks,
as we will discuss in Section~\ref{cl:downstream}.

% Syntactic dependencies?  e.g. in Alpino?




%% \subsection{A brief history of HPSG grammar engineering}
%% \label{cl:history}

%% History: PAGE, VerbMobil, ?? % First draft: Emily

%% % GE: no need to repeat what Flickinger et al. cover in their chapter
%% % but probably a good idea to contrast grammar engineering and linguistic theorizing

%% % EMB 2018-12-21 Maybe everything we were going to put here is ending up
%% % in the discussions below?

%% Current platforms:
%%     \begin{itemize}
%%     \item LKB/ACE/PET/Agree
%%     \item Trale
%%     \item Other
%%     \end{itemize}


\section{Development of HPSG resources}
\label{cl:resources}

In this section we describe various projects that have developed computational resources on the basis of or inspired by HPSG.
As we'll discuss in Section~\ref{cl:deployment} below,
such resources can be used both in linguistic hypothesis testing as well as in various practical applications.
The intended purpose of the resources influences the form that they take.
The CoreGram Project (section~\ref{cl:coregram:1}) primarily targets linguistic hypothesis testing,
the Enju and Alpino parsers (section~\ref{cl:othergrammars}) primarily target practical applications,
and the \delphin\ Consortium (section \ref{cl:delphin}) attempts to balance these two goals.


\subsection{CoreGram}
\label{cl:coregram:1}

The CoreGram\footnote{
	\url{https://hpsg.hu-berlin.de/Projects/CoreGram.html}
} Project
aims to produce large-scale HPSG grammars,
which share a common `core' grammar \citep{MuellerCoreGram}.
% https://hpsg.hu-berlin.de/~stefan/Pub/coregram.pdf
% https://hpsg.hu-berlin.de/~stefan/Pub/coregram-brief.pdf
At the time of writing, large grammars have been produced for
German \citep{},
Danish \citep{},
Persian \citep{},
Maltese \citep{},
and Mandarin \citep{}.
Smaller grammars are also available for English, Yiddish, Spanish, French, and Hindi.

All grammars are implemented
in the TRALE system \citep{MPR2002a-u,Penn2004a-u},
which accommodates a wide range of technical devices proposed in the literature, including:
phonologically empty elements,
relational constraints,
implications with complex antecedents,
cyclic feature structures,
and macros.
It also allows an expressive morphological component.

An important part of CoreGram is the sharing of grammatical constraints across grammars.
Some general constraints hold for all grammars,
while others hold for a subset of the grammars,
and some only hold for a single grammar.
\citet{MuellerCoreGram} describes this as a ``bottom-up approach with cheating''\,---\,the aim is to analyse each language on its own terms (hence ``bottom-up''),
but to re-use analyses from existing grammars, if possible (hence ``with cheating'').



\subsection{The \delphin\ Consortium}
\label{cl:delphin}

The \delphin\footnote{This stands for DEep Linguistic
  Processing in \textsc{Hpsg} INitiative; see \url{http://www.delph-in.net}} Consortium
was established in 2001 to facilitate the development of large-scale,
linguistically motivated HPSG grammars for multiple languages in tandem
with the software required for developing them and deploying them in
practical applications. At the time that \delphin\ was founded, the
English Resource Grammar
\citep[ERG;][]{Flickinger2000a,Flickinger2011a-u} had been under
development already for 8 years and the Verbmobil project
\citep{Wahlster:00} had also spurred the development of grammars for
German \citep[GG;][]{Mue:Kap:00,Crysmann2003b} and Japanese
\citep[Jacy;][]{SBB2016a}. Project DeepThought
\citep{Callmeier-etal:2004} was exploring methodologies for combining
deep and shallow processing in pratical applications across multiple
languages. This inspired the development of the LinGO Grammar Matrix
\citep{BFO2002a-u}, which began as a core grammar, consisting of
constraints hypothesized to be cross-linguistically useful, abstracted
out of the ERG with reference to Jacy and GG. The goal of the Grammar
Matrix is to serve as a starting point for the development of new
grammars making it easy to reuse what has been learned in the
development of existing grammars. In the years since, it has been
extended to include `libraries' of analyses of cross-linguistically
variable phenomena
\citep[\eg][]{Drellishak2009a-u,BDFPS2010a-u}.

\delphin\ provides infrastructure (version control repositories,
mailing lists, annual meetings) and an emphasis on open-source
distribution of resources. Both of these support the collaboration of a
global network of researchers working on interoperable
components. These include repositories of linguistic knowledge, that
is, both grammars and meta-grammars (including the Matrix and CLIMB
\citep{Fokkens:14}); processing engines that apply that
knowledge for parsing and generation (discussed further below);
software for supporting the development of grammar documentation
\citep[\eg][]{Hashimoto-etal:07}, software for creating treebanks
(\citealt{OFTM2004a-u,Packard:15}; see also section~\ref{cl:treebanks} below), 
and parse ranking models trained on them
(\citealt{Tou:Man:Fli:Oep:05}; see also section~\ref{cl:prac:rank} above), and
software for robust processing, \ie\ using the knowledge encoded in
the grammars to return analyses for sentences even if the grammar
deems them ungrammatical
\citep{jigsaw,buys2017parse,chen2018parse}.

A key accomplishment of the \delphin\ Consortium is the standardization
of a formalism for the declaration of grammars
\citep{Copestake:02:CLE}, a formalism for the semantic representations
\citep{CFPS2005a}, and file formats for the storage and interchange of
% GE 2019-01-17 I presume CFPS2005a is a suitable reference?
grammar outputs (\eg\ the forest that results from parsing a forest,
as well as the results of treebanking \citep{Oepen:01,OFTM2004a-u}).
These standards facilitate the development of multiple different
parsing and generation engines which can all process the same grammars
(including, so far, the LKB \citep{Copestake2002a}, PET
\citep{callmeier00},
ACE,\footnote{\url{http://sweaglesw.org/linguistics/ace/}} and Agree
\citep{Slayden2012a-u}), of multiple software systems for processing
bulk grammar output (\itsdb\ \citep{Oepen:01}, art\footnote{FIXME: Find URL}, PyDelphin\footnote{%
	\url{https://github.com/delph-in/pydelphin/}
})
% GE 2019-01-17 PyDelphin doesn't have an obvious citation...
% maybe best to refer to systems by name?
and of multilingual downstream systems which can be adapted to additional
languages by plugging in different grammars.

The \delphin\ community maintains research interests in both
linguistics and practical applications. The focus on linguistics means
that \delphin\ grammarians strive to create grammars which capture
linguistic generalizations and model grammaticality. This, in turn,
leads to grammars with lower ambiguity than one finds with
treebank-trained grammars and, importantly, grammars which produce
well-formed strings in generation. The focus on practical applications
leads to several kinds of research goals: (1) robustness measures such
as techniques for handling unknown words and extra-grammatical mark-up
in text (\eg\ Wikipedia pages) and strategies for processing inputs
that are ungrammatical (at least according to the grammar); (2)
performance innovations, such as supertagging or ubertagging to speed
up processing times and pre-processors that provide partial
information about constituent boundaries to narrow the search space;
(3) the integration of external components such as morphological
analyzers and named entity recognizers; (4) parse and realization
ranking; and (5) work towards making sure that the semantic
representations can serve as a suitable interface for external
components. 
% EMB 2018-12-21 I can probably put in citations for all of those
% things. Worth it, or too busy?
% GE 2019-01-29 Probably okay like this,
% as long as those citations are discussed elsewhere in the chapter
From a linguistic point of view, these efforts are also valuable.
First, the broader the coverage of a grammar,
the more linguistic phenomena it can be used to explore.
Second, external constraints on the form of semantic representations provide
useful guide points in the development of semantic analyses. 
% EMB 2018-12-21 I don't have a cite for that one --- just personal
% experience...


\subsection{Other HPSG and HPSG-inspired broad-coverage grammars}
\label{cl:othergrammars}

\subsubsection{Alpino}
\label{cl:other:alpino}

Alpino\footnote{%
	\url{http://www.let.rug.nl/vannoord/alp/Alpino/}
}
is a broad-coverage grammar of Dutch \citep{BvNM2001a-u,vannoord2005alpino,vannoord2006alpino}.
% https://www.let.rug.nl/vannoord/papers/alpino.pdf
% https://www.let.rug.nl/vannoord/papers/wcpsavg.pdf
% http://talnarchives.atala.org/TALN/TALN-2006/taln-2006-invite-002.pdf

Produces syntactic dependencies.
Although these differ from the semantic dependencies discussed in section~\ref{cl:prac:dep},
a common motivation is to make the representations easier to use in practical applications.

The same statistical model is used in both parsing and generation \citep{dekok2011reversible}.



\subsubsection{Enju}
\label{cl:other:enju}

Enju\footnote{%
	\url{http://www.nactem.ac.uk/enju/}
}
is a broad-coverage grammar of English,
semi-automatically acquired from the Penn Treebank \citep{MNT2005a-u}.
% 2008 https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2008.34.1.35

%TODO http://www.nactem.ac.uk/enju/


\subsubsection{Babel}
\label{cl:other:babel}

Babel is a broad-coverage grammar of German \citep{Babel,Mueller99a}.
One interesting feature of this grammar is that
it makes extensive use of discontinuous constituents \citep{Mueller2004b}.
Although this makes the worst-case parsing complexity much worse,
parsing speed doesn't seem to suffer in practice.
This mirrors the findings of \citet{Carroll94},
discussed in section~\ref{cl:prac:comp} above.

% Notes from Stefan:
%% I have a candidate for being listed under "other". In 93 I started to
%% work on an HPSG implementation implemented in Prolog. The Babel system
%% had the largest German grammar at the time and the processing system was
%% the fastest. It was orders of magnitute better than what IBM had and
%% people like Wahlster were impressed that one guy could do both grammar
%% and system, something IBM spent one or more Millions on.





\section{Deployment of HPSG resources}
\label{cl:deployment}

\subsection{Language documentation and linguistic hypothesis testing}
\label{cl:lang-doc}

Deployment for linguistic goals.

% King (2016) ``Theoretical linguistics and grammar engineering as mutually constraining disciplines''
% http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2016/headlex2016-king.pdf

\subsubsection{CoreGram}
\label{cl:coregram}
% Guy

\subsubsection{Grammar Matrix}
% Emily

As noted in Section~\ref{cl:delphin}, the LinGO Grammar Matrix
\cite{BFO2002a-u,BDFPS2010a-u} was initially developed in
the context of Project DeepThought with the goal of speeding up the
development of \delphin-style grammars for additional languages. It
consists of a shared core grammar and a series of `libraries' of
analyses for cross-lingusitically variable phenomena. Both of these
constitute linguistic hypotheses: The constraints in the constraints
in the core grammar are hypothesized to be cross-linguistically
useful. However, in the course of developing grammars based on the
Matrix for specific languages, it is not uncommon to find reasons to
refine the core grammar. The libraries, in turn, are intended to 
cover the attested range of variation for the phenomena they model.
Languages that are not covered by the analyses in the libraries provide
evidence that the libraries need to be extended or refined. 

Grammar Matrix grammar development is less tightly coordinated than
that of CoreGram (see section~\ref{cl:coregram}): in the typical use case,
grammar developers start from the Grammar Matrix, but with their own
independent copy of the Matrix core grammar. This impedes somewhat the
ability of the Matrix to adapt to the needs of various languages
(unless grammar developers report back to the Matrix developers).  On
the other hand, the Matrix libraries represent an additional kind of
linguistic hypothesis testing: each library on its own represents one
linguistic phenomenon, but the libraries must be interoperable with
each other. This is the cross-linguistic analogue of how monolingual
implemented grammars allow linguists to ensure that analyses of
different phenomena are interoperable
\citep{Mueller99a,Bender2008c-Short}: the Grammar Matrix customization
system allows its developers to test cross-linguistic libraries of
analyses for interactions with other phenomena
\citep{BFO2011a-u,Bender:16}. Without computational
support\,---\,\ie\ a computer keeping track of the constraints that
make up each analysis, compiling them into specific grammars, and
testing those grammars against test suites\,---\,this problem space
would be too complex for exploration.

\subsubsection{AGGREGATION}
% Emily

In many ways, the most urgent need for computational support for
linguistic hypothesis testing is the description of endangered
languages. Implemented grammars can be used to process transcribed but
unglossed text in order to find relevant examples more quickly, both
of phenomena that have already been analyzed and of phenomena that are
as yet not well-understood.\footnote{This methodology of using an
  implemented grammar as a sieve to sift the interesting examples out
  of corpora is demonstrated for English by \cite{Baldwin-et-al-05}.}
Furthemore, treebanks constructed from implemented grammars can be
tremendously valuable additions to language documentation (see
section~\ref{cl:treebanks} below). However, the process of building an
implemented grammar is time-consuming, even with the start provided by
a multilingual grammar engineering project like CoreGram or the
Grammar Matrix.

This is the motivation for the AGGREGATION project. AGGREGATION starts
from two observations: (1) Descriptive linguists produce extremely
rich annotations on data in the form of interlinear glossed text
(IGT); and (2) the Grammar Matrix's libraries are accessed through a
customization system which elicits a grammar specification in the form
of a series of choices describing either high-level typological
properties or specific constraints on lexical classes and lexical
rules.  The goal of AGGREGATION is to automatically produce such
grammar specifications on the basis of information encoded in IGT, to
be used by the Grammar Matrix customization system to produce
language-particular grammars. AGGREGATION uses different approaches
for different linguistic subsystems. For example, it learns
morphotactics by observing morpheme order in the training data, and
the grouping affixes together into position classes based on measures
of overlap of stems they attach to \citep{Wax:14,Zamaraeva:17}. For
many kinds of syntactic information, it leverages syntactic structure
projected from the translation line (English, easily parsed with
current tools) through the gloss line (which facilitates aligning the
language and translation lines) to the language line
\citep{Xia:Lew:07,Georgi:16}. Using this projected information, the
AGGREGATION system can detect case frames for verbs, word order
patterns, etc.\ \citep{Ben:Goo:Cro:Xia:13,Zam:How:Ben:19}.

\subsubsection{Derived resources: Treebanks and Sembanks}
\label{cl:treebanks}
%Emily

A particularly valuable type of resource that can be derived from HPSG
grammars are treebanks and sembanks. A treebank is a collection of
text where each sentence is associated with a syntactic
representation. A sembank has semantic representations (in some cases
in addition to the syntactic ones). Treebanks and sembanks can be used
for linguistic research, as the analyses allow for more detailed
phenomenon-based searches \citep{FIXME-tgrep,FIXME-fangorn}. In the
context of language documentation and description, searchable
treebanks can also be a valuable addition, helping readers connect
prose descriptions of linguistic phenomena to multiple examples in the
corpus \citep{Ben:Gho:Bal:Dri:12}. In natural language processing,
treebanks and sembanks are critical source material for training
stochastic and neural parsers \citep{FIXME}.

Traditional treebanks are created by doing a certain amount of
preprocessing on data, including possibly chunking or CFG parsing, and
then hand-correcting the result \citep{FIXME-PTB,FIXME-AMR}. While this
approach is a means to encode human insight about linguistic structure
for later automatic processing, it is both inefficient and potentially
error-prone. The Redwoods project addresses these limitations by
producing grammar-driven treebanks \citep{OFTM2004a-u}. This is done by
first parsing the corpus with the grammar. For each sentence that has
at least one parse, the treebanking software stores a \emph{parse
  forest} or collection of derivations representing the range of
analyses found by the grammar. The parse forest is used to calculate
\emph{discriminants} \ie\ properties true of some but not all trees in
the parse forest \citep{Carter:97}. The manual annotation task, then,
involves selecting among the discriminants. There are typically far
fewer discriminants than trees and there are dependencies between
discriminants so that annotation can be done by specifically choosing
only a small minority of them per item. Furthermore, the task of
choosing discriminants is much easier to do consistently than that of
creating annotations at the level of detail that a grammar-produced
representation can have \citep{Bender2015LayersOI}. Finally, the
treebanking software stores not only the final tree that was selected,
but also the decisions the annotator made about each
discriminant. Thus when the grammar is updated to include for example
a refinement to the semantic representations, the corpus can be
reparsed and the decisions replayed, leaving only a small amount of
further annotation work to be done to handle any additional ambguity
introduced. The activity of treebanking in turn provides useful
insight into grammatical analyses, including sources of spurious
ambiguity and phenomena that are not yet properly handled and thus
informs and spurs further grammar development.
% EMB 2019-01-31: Some of this is covered above in parsing. We should
% see what's really needed in both places and then streamline.

A downside to grammar-based treebanking is that only items for which the
grammar finds a reaonsable parse can be included in the treebank. For many
applications, this is not a drawback so long as there is sufficient and sufficiently varied sentences that do receive analyses. 

* Alpino Dependency Treebank \citep{vanderbeek2002alpino}.
* Universal Dependencies.


\subsection{Downstream applications}
\label{cl:downstream}
%Guy

In this section, we discuss
the use of HPSG grammars for practical tasks.
There is a large number of applications,
and we focus on several important applications here.
Within the \delphin\ community,
a regularly updated list of applications is available online.\footnote{%
	\url{http://moin.delph-in.net/DelphinApplications}
}


\subsubsection{Education}
\label{cl:downstream:edu}

Precise syntatic analyses can be useful in language teaching,
in order to automatically identify errors and give feedback to the student.
In order to model common mistakes,
a grammar can be extended with so-called ``mal-rules''.
A mal-rule is like a normal rule, in that it licenses a construction,
and can be treated the same during parsing
-- however, given a parse,
the presence of mal-rules indicates that the student needs to be given feedback
\citep{Ben:Fli:Oep:04,flickinger2013error,morgadodacosta2016error}.
A large scale system implementing this kind of computer-aided teaching has been developed
by the Education Program for Gifted Youth at Stanford University,
using the ERG \citep{suppes2014teach}.
This system has reached tens of thousands of elementary and middle school children,
and has been found to improve the school results of underachieving children.
% Redbird (McGraw-Hill)

Another way to use a precision grammar is to automatically produce teaching materials.
Given a semantic representation,
a grammar can generate one or more sentences.
\citet{Flickinger:17} uses the ERG to produce practice exercises for a student learning first-order logic.
For each exercise, the student is presented with a natural language sentence,
and is supposed to write down the corresponding first-order logical form.
By using a grammar, the system can produce syntactically varied questions,
and automatically evaluate the student's answer.


\subsubsection{NLP tasks}
\label{cl:downstream:nlp}

HPSG grammars have been used in a range of tasks
in Natural Language Processing (NLP).

\citet{oepen2017extrinsic} shared task,
evaluating dependency representations for three downstream applications:
biomedical event extraction, negation resolution, and fine-grained opinion analysis.
Some systems use DM (Schuster et al.)...

Information extraction.
\citet{schaefer2011acl}.
\citet{reiplinger2012glossary}.
\citet{miyao2008protein} Enju, protein-protein interaction.

Some phenomena are particularly important, such as negation and speculation.
\citet{packard2014neg} negation, general purpose.
More specifically...
\citet{velldal2012specneg} biomedical, negation and speculation.
\citet{mackinlay2012biomed} biomedical event extraction, negation and speculation.
\citet{zamaraeva2018pathology} scope of negation for pathology reports.

Summarization.
\citet{fang2016summarise}

Machine translation.
Parse, transfer, generate.
\citep{OVL2007a-u,bond2011deep}.
\citet{goodman2018translate}.
Statistical without transfer,
\citet{horvat2017translate}.

Distributional semantics.
\citet{emerson2018functional}.

%TODO check http://www.let.rug.nl/vannoord/papers/



\subsubsection{Data for machine learning}
\label{cl:downstream:data}

In section~\ref{cl:downstream:nlp},
we described how HPSG grammars can be used to tackle a number of NLP tasks.
Another use of HPSG grammars in NLP
is to generate data, on which a statistical system can be trained.

Training deep learning systems
-- semantic parsing
-- skip over HPSG, go straight to semantic representations.
\citep{oepen2014semeval,oepen2015semeval}.
\citet{buys2017parse}.
\citet{chen2018parse}.

Evaluation of deep learning systems
-- ShapeWorld
-- use a grammar to produce annotations.
\citet{kuhnle2018shapeworld}.





\section{Linguistic Insights}
\label{cl:insight}

\begin{itemize}
    \item Ambiguity % Guy
    \item Long-tail phenomena (raising and control?) % Emily
    \item Scaling up (thematic roles) % Emily
    \item CLIMB methodology % Emily
\end{itemize}

\section{Summary}

\section*{Abbreviations}
\section*{Acknowledgements}

\printbibliography[heading=subbibliography,notkeyword=this] 
\end{document}
