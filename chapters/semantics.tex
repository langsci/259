\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.13644931}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[23]
}{}


\author{Jean-Pierre Koenig\orcid{0000-0002-5431-5978}\affiliation{University at Buffalo} and Frank Richter\orcid{0000-0003-1929-5489}\affiliation{Goethe Universit√§t Frankfurt}}
\title{Semantics}

\abstract{This chapter discusses the integration of theories of semantic representations into HPSG. It focuses on those aspects that are specific to HPSG and, in particular, recent approaches that make use of underspecified semantic representations, as they are quite unique to HPSG.}


\begin{document}
\maketitle
\label{chap-semantics}

\section{Introduction} 

A\is{semantics|(} semantic level of description is more integrated into the architecture of HPSG
than in many frameworks (although, in the last couple of decades, the integration of syntax and
semantics has become tighter overall; see \citealt{HeimandKratzer1998} for Mainstream Generative
Grammar\footnote{ We follow \citet[\page 3]{CJ2005a} in using the term \emph{Mainstream Generative
    Grammar} (MGG) to refer to work in Government \& Binding or Minimalism.}, for example). Every
node in a syntactic tree includes all appropriate levels of structure, phonology, syntax, semantics,
and pragmatics so that \emph{local} interaction between all these levels is in principle possible
within the HPSG architecture. The architecture of HPSG thus follows the spirit of the
\isi{rule-to-rule approach} advocated in \citet{Bach1976} and more specifically
\citet{KleinandSag1985} to have every syntactic operation matched by a semantic operation (the
latter, of course, follows the Categorial\indexcg Grammar lead, broadly speaking;
\citealt{Ajdukiewicz35a-u,Pollard84a-u,Steedman2000a-u}). But, as we shall see, only the spirit of
the rule-to-rule approach is adhered to, as there can be more than one semantic operation per class
of syntactic structures, depending on the semantic properties of the expressions that are
syntactically composed. The built-in interaction between syntax and semantics within HPSG is
evidenced by the fact that \citet{PollardandSag1987}, the first book-length introduction to HPSG,
spends a fair amount of time on semantics and ontological issues, much more than was customary in
syntax-oriented books at the time.

But despite the centrality of semantics within the HPSG architecture, not much comprehensive work on
the interface between syntax and semantics was done until the late 90s, if we exclude work on the
association of semantic arguments to syntactic valents in the early 90s (see
\crossrefchapteralt{arg-st}). The formal architecture was ripe for research on the interface between
syntax and semantics, but comparatively few scholars stepped in.  Early work on semantics in HPSG
investigated scoping issues, as HPSG surface-oriented syntax presents interesting challenges when
modeling alternative scope relations. This is what \citet{PollardandSag1987, PollardandSag1994}
focus on most. Scope of modifiers is also an area that was of importance and received attention for
the same reason both in \citet{PollardandSag1994} and \citet{Kasper1997}. \citew{GinzburgandSag2001}
is the first study not devoted to argument structure to leverage the syntactic architecture of HPSG
to model the semantics of a particular area of grammar, in this case interrogatives.

The real innovation HPSG brought to the interface between syntax and semantics is the use of
underspecification in Minimal Recursion Semantics (\citealt*{Copestakeetal1995}; \citealt{Egg1998};
\citealt*{Copestakeetal2001}; \citealt*{Copestakeetal2005}) and Lexical Resource Semantics
\citep{RichterandSailer2001}; see also \citet{Nerbonne1993a} for an early use of scope
underspecification in HPSG. The critical distinction between grammars as descriptions of admissible
structures and models of these descriptions makes it possible to have a new way of thinking about
the meaning contributions of lexical entries and constructional entries: underspecification is the
other side of descriptions.

\section{A situation semantics beginning}

\largerpage
The semantic side of HPSG was initially rooted in \isi{Situation Semantics} (\citealp[Chapter
4]{PollardandSag1987}, on Situation Semantics see \citealp{BP83a}). The choice of Situation
Semantics is probably somewhat a matter of happenstance, and overall, nothing too crucial depended
on that choice (and other choices have been explored since, as we detail below). However, this
statement should not be construed as implying the choice was inconsequential. There were several
interesting aspects of this choice for the study of the interface between syntax and semantics that
is integral to any grammatical framework. We briefly mention a few here. A first interesting aspect
of this choice is that the identification of arguments was not through an ordering but via keywords
standing for role names, something that made it easier to model argument structure in subsequent
work (see \crossrefchapteralt{arg-st}). A second aspect is the built-in ``intensionality'' of
Situation Semantics. Since atomic formulas in Situation Semantics denote circumstances rather than
truth values, and circumstances are more finely individuated than truth values, the need to resort
to possible world semantics to properly characterize differences in the meaning of basic verbs, for
example, is avoided. A third aspect of Situation Semantics that played an important role in HPSG is
parameters. Parameters are variables that can be restricted and sorted, thus allowing for an easy
semantic classification of types of NPs, something that HPSG's Binding Theory makes use of
\crossrefchapterp[Section~\ref{binding:sec-a-non-configurational-binding-theory}]{binding}.

Parameters also play an important role in early accounts of quantification; these accounts rely on restrictions on parameters that constrain how variables are anchored, akin to predicative conditions on discourse referents in Discourse Representation Theory \citep{KampandReyle1993}. Restrictions on parameters are illustrated with (\ref{donkey}), the (non-empty) semantic content of the common noun \word{donkey}, where the variable \ibox{1} is restricted to individuals that are donkeys, as expressed by the value of the attribute \attrib{rest}.\footnote{In \citet{PollardandSag1987}, which we discuss here, semantic relations are the values of a \attrib{reln}\isfeat{reln} attribute and restrictions are single semantic relations rather than set-valued. To ensure historical accuracy, we use the feature geometry that was used at the time.}

\begin{exe}
\ex\label{donkey}
\avm{
[var & \1 \\
rest & [reln & donkey \\
		inst & \1]]
}
\end{exe} 

\largerpage
Because indices are restricted variables/parameters, the model of quantification proposed in
\citet[Chapter 4]{PollardandSag1987} involves restricted quantifiers. Consider the sentence
\word{Every donkey sneezes} and its semantic representation in (\ref{donk-sn})
\citep[109]{PollardandSag1987}.


\begin{exe}
\ex\label{donk-sn}

\avm{
[quant & [det & forall \\
			ind & [var & \1 \\
					rest & [reln & donkey \\
							inst & \1] ] ]\\
scope & [reln & sneeze \\
		sneezer & \1]]
}

\end{exe}

The subject NP contributes the value of the attribute \attrib{quant}, while the verb contributes the
value of \attrib{scope}. The quantifier includes information on the type of quantifier contributed
by the determiner (a universal quantifier in this case) and the index (a parameter restricted by the
common noun).

Because HPSG is a sign-based grammar, each constituent includes a phonological and semantic
component as well as a syntactic level of representation (along with other possible levels, e.g.\
information structure; see \crossrefchapteralt{information-structure}). Compositionality has thus
always been directly incorporated by principles that regulate the value of the mother's \attrib{sem}
attribute, given the \attrib{sem} values of the daughters and their mode of syntactic combination
(as manifested by their syntactic properties). Different approaches to semantics within HPSG propose
variants of a Semantics Principle\is{principle!Semantics} that constrains this relation. The Semantics Principle of
\citet[109]{PollardandSag1987} is stated in \ili{English} in (\ref{SP87}) (we assume for simplicity
that there is a single complement daughter; \citeauthor{PollardandSag1987} define semantic
composition recursively for cases of multiple complement daughters).

\ealnoraggedright
\label{SP87}
\ex
If the semantic content \iboxb{1} of the head daughter is of sort \type{circumstance} and the semantic content \iboxb{2} of the complement daughter is of sort \type{quantifier}, the semantic content of the mother is\\ \avm{[quant & \2 \\ scope & \1]}.\\
\ex Otherwise, the semantic contents of the head daughter and the mother are identical.
\zl

\largerpage
The fact that the Semantics Principle in (\ref{SP87}) receives a case-based definition is of
note. Since HPSG is monostratal, there is only one stratum of representation (see
\citealt{Ladusaw1988b} for the difference between levels and strata). But the semantic contribution
of complement daughters varies. Some complement daughters are proper names or pronouns, while others
are generalized quantifiers, for example. Since it is assumed that the way in which the meaning of
(free) pronouns or proper names combines with the meaning of verbs differs from the way generalized
quantifiers combine with the meaning of verbs, the Semantics Principle must receive a case-based
definition. In other words, syntactic combinatorics is less varied than semantic combinatorics. The
standard way of avoiding violations of compositionality (the fact that semantic composition is a
\emph{function}) is to have a case-based definition of the semantic effect of combining a head
daughter with its complements, a point already made in \citet{Partee1984a}. As (\ref{SP87}) shows,
HPSG has followed this practice since its beginning. The reason is clear: one cannot maintain a
surface-oriented approach to syntax, where syntax is ``simpler'', to borrow a phrase from
\citet{CulicoverandJackendoff2005}, without resorting to case-based definitions of the semantic
import of syntactic combinatorics.


\section{Scope relations in HPSG}
\label{sec-scope-in-hpsg}

In Mainstream Generative Grammar, there is an assumption that syntactic constituency reflects
semantic constituency at one stratum of representation. In the case of quantifier scope in works
like \citet{May1985}, this means that quantified expressions are moved out of their surface position
and raised to a position where they can receive their proper scope through Quantifier Raising
(and/""or Quantifier Lowering; see, among others \citealt{Hornstein1995}).\footnote{For a discussion
  of the relation between the semantic scope of aspect markers and the syntactic structures they
  enter in in Mainstream Generative Grammar vs. HPSG, see \citet{KoenigandMuansuwan2005}}. Of
course, such a move requires multiple strata, as there is little evidence that quantifier scope
affects surface syntactic structure. The Semantics Principle and the representation of quantifier
meanings outlined in \citet{PollardandSag1987} and briefly presented in the previous section was not
flexible enough to model the relation between single syntactic structures and multiple scopal
relations. As \citeauthor{PollardandSag1987} explicitly recognized (p.\,112), their Semantics
Principle only models left-to-right scopal relations, i.e.\ quantifiers that are expressed by a
complement (or subject) that is to the left of another complement have wide scope with respect to
that quantifier. So-called inverse scope, including the fact that quantifiers in object position can
outscope quantifiers in subject position, cannot be modeled by the kind of Semantics Principle they
propose. Much of the discussion of semantics within HPSG in the 90s pertains to improving how scope
is modeled, both the scope of quantifiers and the scope of adjuncts. We discuss each in turn in this
section.



\subsection{Quantifier scope}

\enlargethispage{4pt}
Until the mid-2000s, HPSG's ``standard'' model of the interface between the syntax and semantics of
phrases that contain quantifiers adapted to HPSG the approach proposed in
\citet{Cooper1975,Cooper1983}, i.e.\ so-called Cooper storage: when a quantified expression combines
with another expression, the quantifier is put in a store, and various scopal relations correspond
to the various nodes at which the quantifier can be retrieved from storage. Within HPSG, quantifier
storage involves a \attrib{qstore} attribute where each quantifier starts, and at each node,
quantifiers are either retrieved (part of the \attrib{retrieved} list) or continue to be on the
mother's \attrib{qstore}. The relative scope of quantifiers itself is determined by the ordering of
quantifiers on the \attrib{quants} list. The simplified tree in Figure~\ref{qu-sc} from
\citet[324]{PollardandSag1994} illustrates the inverse scope reading of an \ili{English} sentence
containing two quantifiers.

\begin{figure}\centering
\begin{forest}
sm edges
[%
\avm{
	[quants & < \3 , \5 > \\
nucleus & \4 \\
retrieved & < \3 , \5 >]
}, s sep+=6ex
	[%
	\avm{
		[ind & \1 \\
	qstore & \{ \5 \} ]
	}
		[every student, roof]]
	[%
	\avm{
		[quants & < > \\
		nucleus & \4 \\
		qstore & \{ \3 \} ]
	}
		[%
		\avm{
			[quants & < > \\
			nucleus & \4	[\type*{know}
							knower & \1 \\
							known & \2 ] ]
		}
			[knows]
		]
		[%
		\avm{
			[ind & \2 \\
			qstore & \{ \3 \} ]
		}
			[a poem, roof]
		]
	]
]
\end{forest}
\itddone{renumber top down? \ibox{3} is missing. Should there be QSTORE and RETRIEVED values at all/more nodes? \\
JPK Done w.r.t. missing 3. Not worth the fuss to renumber top down. Yes there should be but inferrable and would clutter the reader's mind-- figure IS the P\&S figure}
\caption{\label{qu-sc}Semantic composition of an English sentence containing two quantifiers}
\end{figure} 

Both subject and object quantifiers start with their quantifiers (basically, something very similar
to the representation in (\ref{donk-sn})) in a \attrib{qstore}. Since the reading of interest is the
one where \word{a poem} outscopes \word{every student}, the quantifier introduced by \word{a poem}
cannot be retrieved at the VP level. This is because the value of \attrib{quants} is the
concatenation of the value of \attrib{retrieved} with the \attrib{quants} value of the head
daughter. Were the quantifier introduced by \word{a poem} \iboxb{3} retrieved at the VP level, the
sole quantifier retrieved at the S level, the one introduced by \word{every student}, would outscope
it. So, the only way for the quantifier introduced by \word{a poem} to outscope the quantifier
introduced by \word{every student} is for the former to be retrieved at the S node just like the
latter. Simplifying somewhat for presentational purposes, two principles govern how quantifiers are
passed on from head daughter to mothers and how quantifier scope is assigned for retrieved
quantifiers; they are stated in (\ref{quant-princ}) (adapted from
\citealt[322--323]{PollardandSag1994}).



\eal \label{quant-princ}
\ex\label{quant-inh}
In a headed phrase, the \attrib{retrieved} value is a list whose set of elements is a subset of the union of the \attrib{qstore} of the daughters; the \attrib{qstore} value is the relative complement of that set.
\ex\label{quant-sc}
In a headed phrase (of sort \type{psoa}, which stands for ``parameterized state of affairs''), the \attrib{quants} value is the concatenation of the \attrib{retrieved} value and the \attrib{quants} value of the semantic head.
\zl

\noindent
(\ref{quant-inh}) ensures that quantifiers in storage are passed up the tree, except for those that are retrieved; (\ref{quant-sc}) ensures that quantifiers that are retrieved outscope quantifiers that were retrieved lower in the tree. Retrieval at the VP level entails narrow scope of quantifiers that occur in object position; wide scope of quantifiers that occur in object position entails retrieval at the S level. But retrieval at the S level of quantifiers that occur in object position does not entail wide scope, as the order of two quantifiers in the same \attrib{retrieved} list (i.e.\ retrieved at the same node) is unconstrained. Constraints on quantifier retrieval and scope underdetermines quantifier scope. To ensure that quantifiers are retrieved sufficiently ``high'' in the tree to bind bound variable uses of pronouns, e.g.\ \word{her} in (\ref{quant-ex}), \citeauthor{PollardandSag1994} propose the constraint in (\ref{quant-bind}).

\begin{exe}
\ex\label{quant-ex}
One of her$_{i}$ students approached [each teacher]$_{i}$. \citep[327]{PollardandSag1994}
\end{exe}
\itddone{There are many 27 in PS. I wanted to replace it, since I find the brackets of examples ugly, but it has to be the page number for other reasons too.}

\begin{exe}
\ex\label{quant-bind}A quantifier within a \attrib{content} value must bind every occurrence of that quantifier's index within that \attrib{content} value.  
\end{exe}

The use of Cooper storage allows for a syntactically parsimonious treatment of quantifier scope ambiguities in that no syntactic ambiguity needs to be posited to account for what is a strictly semantic phenomenon. But as \citeauthor{PollardandSag1994} note (p.\,328), their model of quantifier scope does not account for the possible narrow scope interpretation of the quantifier \word{a unicorn} in (\ref{quant-rais}) (the interpretation according to which the speaker does not commit to the existence of unicorns). Raised arguments only occur once, in their surface position, and (\ref{quant-princ}a--b) ensure that quantifiers are never retrieved ``lower'' than their surface position.

\begin{exe}
\ex\label{quant-rais}A unicorn appears to be approaching.
\end{exe}

\noindent
\citet{PollardandYoo1998} tackle that problem, as well as take into account the fact that a sentence such as (\ref{ext-sc}) is ambiguous (i.e.\ the quantifier \word{five books} can have wide or narrow scope with respect to the meaning of \word{believe}).\itddone{put (\mex{1}) here.} 
\begin{exe}
\ex\label{ext-sc}
Five books, I believe John read. (ambiguous)
\end{exe}

As \citeauthor{PollardandYoo1998} note, since quantifier storage and retrieval is a property of signs, and fillers (see \crossrefchapteralt{udc}) only share their \attrib{local} attribute values with arguments of the head (\word{read} in (\ref{ext-sc})), the narrow scope reading cannot be accounted for. (\ref{quant-rais}) and (\ref{ext-sc}), among other similar examples, illustrate some of the complexities of combining a surface-oriented approach to syntax with a descriptively adequate model of semantic composition.



\citeauthor{PollardandYoo1998}'s solution (p.\,419--420) amounts to making quantifier storage and retrieval a property of the \attrib{local} value  and to restricting quantifier retrieval to semantically potent heads (so, the \word{to} of infinitive VPs cannot be a site for quantifier retrieval). The new feature geometry of \type{sign} that Pollard and Yoo propose is represented in (\ref{poll-yoo}). The pool of quantifiers collects the quantifiers on the \attrib{qstore} of its selected arguments (members of the \attrib{subj}, \attrib{comps}, and \attrib{spr} lists, and the value of \attrib{mod}, except for quantifying determiners and semantically vacuous heads like \word{to} or \word{be}) and the constraints in (\ref{sign-cons}) and (\ref{cons-PY}) \citep[423]{PollardandYoo1998} ensure proper percolation of quantifier store values within headed phrases as well as the semantic order of retrieved quantifiers.

\begin{exe}
\ex\label{poll-yoo}

\avm{
[\type*{sign}
phonology & \list*(phon\_string) \\
synsem    &	[local & [category & category \\
			  content  & content \\
			  qstore   & \setOf{quantifier} \\
			  pool     & \setOf{quantifier} ] ] \\
retrieved & \listOf{quantifier} ]
}
\end{exe}

%fmr12-21: turned rightarrow into Rightarrow to make consistent with (19)
\begin{exe}
\ex\label{sign-cons}
\begin{tabular}{@{}l@{~}l@{}}
\emph{sign} \impl&
\avm{
[synsem|loc &	[qstore & \1 \\
				pool & \2 ] \\
retrieved & \3 ]
} \\
& $\land$ set-of-elements (\ibox{3} , \ibox{4})
$\land$ \ibox{4} $\subseteq$ \ibox{2}
$\land$ \ibox{1} = \ibox{2} $-$ \ibox{4}
\end{tabular}
\end{exe}

\begin{exe}
\ex\label{cons-PY}
\begin{xlist}
\ex \label{cons-PYa} The pool of the mother of a headed-phrase is identical to the quantifier store of the head daughter. 
\ex \label{cons-PYb} For a semantically nonvacuous lexical head, the \attrib{quants} value is token-identical with the \attrib{retrieved} value.
\end{xlist}
\end{exe}

\noindent
What remains in the \attrib{qstore} of a sign is the quantifiers that were in the \attrib{pool} of (unretrieved) quantifiers minus quantifiers that were retrieved, according to the  constraint in (\ref{sign-cons}). Since the \attrib{pool} of  the sign's mother is the \attrib{qstore} of its head daughter as per constraint (\ref{cons-PYa}), a quantifier retrieved on a head daughter is not part of the \attrib{pool} of (unretrieved) quantifiers of the mother. 

In a follow-up paper, \citet{Przepiorkowski1998b} proposed a strictly lexicalized retrieval mechanism which removes structural ambiguities arising from different possible retrieval sites for quantifiers along a syntactic head path, is compatible with trace-based and traceless analyses of extraction (\citeauthor{PollardandYoo1998}'s analysis only covers trace-based extraction), and shifts all semantic structure under the \attrib{content} attribute.

%JPK 12-17 I find "under" OK. What do you think?  fmr12-18: agreed


\subsection{Adjunct scope}
\label{semantics-sec-adjunct-scope}

HPSG phrase structure schemata are built, for a significant part, around headed structures. In the
case of the head-complement or head-subject schemata, syntactic headedness and semantic headedness
match. The verb is the head of VPs and clauses, and the circumstance or state of affairs denoted by
verbs typically takes as arguments the indices of its complements or subjects, and more generally,
part of the \attrib{content} value of the verb takes as arguments part of the \attrib{content} value
of its dependents. But in the case of head-adjunct structures, syntactic and semantic headedness do
not match. The denotation of adjuncts often takes the denotation of heads as arguments. Thus, in
(\ref{fast}), fastness is ascribed to Bob's running. Accordingly, the Semantics Principle
distinguishes between head-adjunct structures and other structures, as shown in (\ref{sem-princ})
\citep[56]{PollardandSag1994}. (The principle we cite does not consider the quantifier retrieval we
discussed in the previous section.)

\begin{exe}
\ex\label{fast} Bob runs fast.
\end{exe} 

\begin{exe}
\ex\label{sem-princ}
In a headed phrase, the \attrib{content} value is token-identical to that of the adjunct daughter if the \attrib{dtrs} value is of sort \type{head-adj-struc}, and to that of the head daughter otherwise.
\end{exe}

Unfortunately, the hypothesis that the content of phrases ``projects'' from the adjunct in the case of head-adjunct structures leads to difficulties in the case of so-called recursive modification, e.g.\ (\ref{rec-mod}), as \citet{Kasper1997} shows. 
\begin{exe}
\ex\label{rec-mod}a potentially controversial plan
\end{exe}
The NP in (\ref{rec-mod}) denotes an existential quantifier whose restriction is a plan that is potentially controversial; intuitively speaking, what is potential is the controversiality of the plan, not it being a plan. But the Semantics Principle, the syntactic selection of modified expressions by modifiers, and lexical entries for intersective and non-intersective adjectives conspire to lead to the wrong meaning for recursive modification of the kind (\ref{rec-mod}) illustrates: since \word{controversial} selects for \word{plan}, combining their meaning leads to the meaning represented in (\ref{contr-pl-sem}), as \word{controversial} is an intersective adjective.

\begin{exe}
%\ex\label{inter-adj}
%\begin{xlist}
%\ex \label{inter-adj-a} A controversial senator
%\ex\label{inter-adj-b} \sem{x$|$controversial'(x) \& senator'(x)}
\ex\label{contr-pl-sem}

\avm{
[\type*{nom-obj}
index & \1 \\
restr &	[reln & plan \\
		inst & \1 ] \&	[reln & controversial \\
						arg & \1 ] ]
}
%\end{xlist}
\end{exe}

\noindent
But since adjuncts are the semantic head, the meaning of \word{potentially controversial plan} will be projected from the meaning of \word{potentially}, the most deeply embedded adjunct. Now, \word{potentially} is  a conjectural adverb, to adapt to adverbs the classification of adjectives proposed by \citet[125]{KeenanandFaltz1985}. Within HPSG, this means that the meaning of \word{potentially} is a function that takes the meaning of what it modifies as argument, i.e.\ the meaning represented in (\ref{contr-pl-sem}). But this leads to the meaning represented in (\ref{pot-contr-pl-sem}), which is the wrong semantics, as a potentially controversial plan is not a potential plan, as \citet[10--11]{Kasper1997} points out.

\begin{exe}
\ex\label{pot-contr-pl-sem}
\avm{
[\type*{nom-obj}
index & \1 \\
restr &	[reln & potential \\
		arg &	[reln & plan \\
				inst & \1 ] \&	[reln & controversial \\
								arg & \1 ] ] ]
}
\end{exe}

\noindent
The problem with \citeauthor{PollardandSag1994}'s Semantics Principle\is{principle!Semantics|(},
when it comes to recursive modification, is clear: semantic selection follows an adjunct path, so to
speak, so the most deeply embedded adjunct will have widest scope.  

\largerpage[2]
Kasper's solution is to distinguish the inherent meaning of an expression (its regular content) from
meanings it may have in a particular construction:\ its combinatorial semantics (its internal and
external content). With respect to prenominal adjuncts, the internal content corresponds to the
content of the adjunct's maximal projection, whereas the external content corresponds to the content
of the combination of the adjunct's meaning with what it modifies. The Semantics Principle is
revised to reflect the distinction between internal and external contents and is provided in
(\ref{sem-princ-k}) \citep[19]{Kasper1997}. 

\begin{exe}
\ex\label{sem-princ-k}
\begin{xlist}
\ex\label{sem-princ-k-a}
The semantic content of a head-adjunct phrase is token-identical to the \feat{mod$|$econt} value of the adjunct daughter, and the \feat{mod$|$icont} value of the adjunct daughter is token-identical to the adjunct daughter's \attrib{cont}.
\ex\label{sem-princ-k-b}
For all other headed phrases, the \attrib{cont} value is token-identical to the \attrib{cont} value of the head daughter.
\end{xlist}
\end{exe}

The result of applying the revised Semantics Principle to \word{potentially controversial} is
provided in Figure~\ref{pot-contr-sem} and the semantics of \word{controversial} and
\word{potentially} are provided in (\ref{contr-sem}) and (\ref{pot-sem}), respectively. (The value
of \attrib{arg} in Kasper's analysis of \word{controversial} corresponds to the syntactic and
semantic properties of the modified constituent. The feature \attrib{arg} value is thus the
equivalent of the \attrib{synsem} value in current HPSG.)
\largerpage[2]

\begin{figure}\centering
\begin{forest}
[%
\avm{
	[head & \4 [mod &	[arg &	[cont &	[ind & \1 \\
			             				restr & \2 ] ]\\
                    	econt &	[ind & \1 \\
                                restr & \2 \& \tag{6}]\\
                    	icont & \tag{6} ] ]\\
cont & \5 ]
}
	[%
	\avm{
	[head|mod &	[arg & \7 \\
				econt & \5 \\
				icont & \5 ] \\
	cont & \5	[reln & potential \\
				arg & \3 ] ]
	}
	]
	[%
	\avm{
	\7	[head & \4 \\
		cont & \3	[reln & controversial \\
					inst & \1 ] ]
	}
	]
]
\end{forest}
\caption{\label{pot-contr-sem}\citeauthor{Kasper1997}'s analysis of \word{potentially controversial}}
\end{figure}


\begin{exe}
\ex\label{contr-sem}
\avm{
[head|mod & [arg & [cont  & [ind   & \1\\
			     restr & \2]]\\
	            econt & [ind   & \1 \\
			     restr & \2 \& \5 ]\\
                 	icont & \5 ] \\
cont &	[reln & controversial \\
		inst & \1 ] ]
}
\end{exe}

\begin{exe}
\ex\label{pot-sem}
\avm{
[head & [\type{adv} \\
			mod & [arg & [cont & \3 psoa ] \\
						icont & \5 \\
						econt & \5 psoa ]
				 ]\\
cont & [reln & potential \\
			arg & \3 ] 
]
}

\end{exe}

\pagebreak

Critically, each kind of modifier specifies in its \feat{mod$|$econt} value the combinatorial
effects it has on the meaning of the modifier and modified combination; the \attrib{econt} value
contains that result and will be inherited as \attrib{cont} value by the mother node. Intersective
adjectives like \word{controversial} specify that their combinatorial effect is intersective, as
shown in (\ref{contr-sem}); conjectural adverbs like \word{potentially}, on the other hand, specify
their \attrib{cont} value as the result of applying their meaning to the \attrib{cont} value of the
modified sign. As shown in the left daughter of Figure~\ref{pot-contr-sem}, the resulting
\attrib{cont} value of \word{potentially} is identified by (\ref{sem-princ-k-a}) with its
\attrib{icont} value (which is in turn lexically specified as identical with the \attrib{econt}
value) when \word{potentially} combines as adjunct daughter with an intersective adjective such as
\word{controversial} in a head-adjunct structure. Moreover, when the depicted phrase
\word{potentially controversial} combines with a noun such as \word{plan} in another head-adjunct
phrase, \ibox{5} and \ibox{6} in Figure~\ref{pot-contr-sem} become identical, again by
(\ref{sem-princ-k-a}), thereby also integrating the meaning of \word{potentially controversial} into
the second conjunct of the \feat{econt|restr} value of the depicted phrase. Now, since the
\attrib{mod} value of the head in a head-adjunct phrase determines the \attrib{mod} value of the
phrase, it means that \word{controversial} determines in its \attrib{econt}, in combination with its
\attrib{arg} value, what it modifies (\word{plan}) and, ultimately, the \attrib{cont} value of the
entire phrase \word{potentially controversial plan}, thus ensuring that its intersectivity is
preserved even when it combines with a conjectural adverb. \citet{AsudehandCrouch2002} and
\citet{Egg2004a} provide more recent solutions to the same problem through the use of a \isi{Glue
  Semantics} approach to meaning composition within HPSG and semantic underspecification,
respectively. On Glue Semantics in LFG see also \crossrefchapterw[Section~\ref{sec:glue-semantics}
and~\ref{sec:glue-lfg}]{lfg}.
\is{principle!Semantics|)}
%fmr12-19: turned ``Glue semantics'' into Glue Semantics; correct?

\section{Sorting semantic objects}

One of the hallmarks of HPSG is that all grammatical objects are assigned a sort (see the chapters \crossrefcitealt[Section~\ref{prop:sec-elements}]{properties} and \crossrefcitealt[Section~\ref{formal:sec-essentials}]{formal-background} for details). This includes semantic objects. Sorting of semantic objects has been used profitably in models of lexical knowledge, in particular in models of argument structure phenomena. We refer the reader to \crossrefchaptert{arg-st} for details about argument structure and only provide an illustrative example here. Consider the constraint in (\ref{KD-03-sem}) from \citet[231]{KoenigandDavis2003}. It says that all verbs that denote a causal change of state, i.e.\ verbs whose \attrib{content} values are of sort \type{cause-rel},
%JPK 12-20 I disambiguated the above
 link their causer argument to an NP that is the first member of the \feat{arg-st} list. 
\ea\label{KD-03-sem}
\avm{
[content &	[\type*{cause-rel}
			causer & \1 ] \\
arg-st & <np, \ldots > ]
}
\impl{}
\avm{
[arg-st & <np$_{\1}$, \ldots> ]
}
\z
Critically, verbs like \word{frighten}, \word{kill}, and \word{calm} have as meanings a relation that is a subsort of \type{cause-rel} and are therefore subject to this constraint.  Sorting lexical semantic relations thus makes for a compact statement of linking constraints. (The chapter on argument structure provides many more instances of the usefulness of sorting semantic relations, a hallmark of HPSG semantics.)


Constructional analyses that flourished in the late 1990s also benefited from the sorting of semantic objects. The analysis  of clause types in \citet{Sag1997} and \citet{GinzburgandSag2001} makes extensive use of the sorting of semantic objects to model different kinds of clauses, as our discussion of the latter in the next section makes clear.

\section{The advantages of a surface-oriented grammar}
\label{sec:adv}

Until now, we have mostly covered how semantic composition works in an approach where each node in a tree is associated with a meaning and where there is only one stratum and therefore the ``location'' of an expression in a syntactic tree does not necessarily correspond to where its meaning is composed: direct object quantifiers, for example, are syntactic sisters of the verb, even when they have wide scope over a quantifier in subject position. Although important as a proof that semantic composition can be modeled in a surface-oriented grammar, it is fair to say that HPSG work until the late 1990s does not have too much new insight to contribute to our understanding of the interface between syntax and semantics. This is in no way a slight of that early research on the interface between syntax and semantics. Demonstrating that you can ``get things right'' without multiple strata is important, and work on the relation between lexical meaning and argument structure (see \crossrefchapteralt{arg-st}) is also important in showing that simplicity of syntactic representation does not come at the cost of adequacy. The message was good news: you do not need to make your syntax more complex in order to interface it with semantics. Of course, that was Montague's point already in the late 1960s and early 1970s (see the collected papers in \citealt{Montague1974}), but that work was more a proof of concept.  Carrying out what is basically the Montagovian agenda with a large scale grammar is more difficult, and this is what early work in HPSG, at least retrospectively, seems to have focused on.

The development of a more constructional HPSG in the mid-90s opened up new possibilities for modeling the interface between syntax and semantics. One of these possibilities is to organize families of informationally rich phrasal constructions into a multi-dimensional inheritance hierarchy so as to model the shared semantic combinatorics of quite distinct constructional patterns. This is, for example, apparent in \citet{Sag1997}, where a single modification meaning is assigned to a family of relative constructions that differ markedly syntactically. This is also what \citet{GinzburgandSag2001} show with their analysis of interrogatives. But their analysis goes further in demonstrating that there may be advantages to a surface-oriented approach to syntax in that it correctly predicts an effect of the \emph{surface} syntax onto semantics for the interpretation of interrogatives, as we now show. 

\largerpage[-1]
The approach to interrogatives that \citeauthor{GinzburgandSag2001} propose is new in that it does
not rely on the traditional Hamblin semantics for questions, namely that the meaning of questions is
the set of (exhaustive) answers; see \citet{Hamblin1973} and
\citet{GroenendijkandStokhoff1997}. Rather, the meaning of questions consists of propositional
abstracts (not sets of propositions). Parameters of the kind that have been part of HPSG approaches
to semantics since the beginning are used to model these propositional abstracts. Because the
meaning of questions consists of propositional abstracts, the meaning of \word{wh}-phrases is not
the same as that of generalized quantifiers either; rather, \word{wh}-phrases introduce a parameter
(roughly, the equivalent of a lambda-abstracted variable). (\ref{ques-sem-a}) and (\ref{ques-sem-b})
provide examples of the meaning of \word{wh}-questions and polar questions, respectively
\citep[137]{GinzburgandSag2001}, where the AVM that follows $\mapsto$ is a description of the value
of the \attrib{content} attribute of the expression that precedes $\mapsto$. Note that polar
questions are modeled as zero-parameter propositional abstracts.

\begin{exe}
\ex 
\begin{xlist}
\ex \label{ques-sem-a}
Who left? $\mapsto$ \\*
\avm{
[\type*{question}
params &	\{[\type*{param}
			index & \1 \\
			restr & \{ \normalfont person!(\1)!\} ] \} \\
prop &	[\type*{proposition}
		sit & s \\
		soa &	[quants & < > \\
				nucl &	[\type*{leave-rel}
						leaver & \1 ] ] ] ]
}
\ex\label{ques-sem-b}
Did someone leave? $\mapsto$ \\
\avm{
[\type*{question}
params & \{ \} \\
prop &	[\type*{proposition}
		sit & s  \\
		soa &	[quants & < [\type*{some-rel}
			             index & \1 \\
			             restr & \{ \normalfont person!(\1)!\} ] > \\
				nucl &	[\type*{leave-rel}
						leaver & \1 ] ] ] ]
}
\end{xlist}
\end{exe}


\largerpage[-1] 
The meaning assigned to questions illustrated above relies on an ontology of messages (the semantic
content a clause expresses) which is richer than the traditional notion of propositional content (as
distinct from illocutionary force) in work such as \citet{Searle1969}. Questions in this view are
not just a speech act (where the propositional content of that act remains a proposition), but
rather a particular kind of propositionally constructed message, namely a
proposition-cum-parameters, as shown in Figure~\ref{mess-types}. Crucially, questions are defined as
a parameterized proposition.

\begin{figure}
\begin{forest}
[\emph{message}
	[%
	\avm{
		[\type*{austinian}
		sit & situation \\
		soa & soa]
	}
		[%
		\avm{
			[\type*{outcome}
			soa & i-soa]
		}
		]
		[%
		\avm{
			[\type*{proposition}
			soa & r-soa]
		}
		]
	]
	[%
	\avm{
		[\type*{prop-constr}
        prop & proposition ]
	}
		[\emph{fact}]
		[%
		\avm{
			[\type*{question}
			params & set!(param)!]
		}
		]
	]
]
\end{forest}
\caption{\label{mess-types}A hierarchy of sorts of messages}
\end{figure}

Of concern to us here is less the specifics of this ontology of messages (or of the introduction in
the universe of discourse of place holders and other abstract objects, as is typical of Situation
Semantics) than its role in the interface between syntax and semantics, e.g.\ the fact that clause
types can refer to different kinds of messages. Declarative and interrogative clauses are defined as
in (\ref{cl-typ}), where the expression that precedes the colon indicates the sort of the phrase and
what follows the colon is an informal representation of properties of the phrase's constituents
(AVMs to the left of the arrow are properties of the mother node and what follows the arrow are
properties of the daughters), /\is{$\slash$} indicates \isi{default} identity between information on the
mother and daughter nodes in (\ref{decl-cl}), and ``\ldots'' in (\ref{inter-cl}) informally
indicates the absence of constraints on daughters on the general \type{inter-cl} sort.

\begin{exe}
\ex\label{cl-typ}
\begin{xlist}
\ex\label{decl-cl}
\avm{
\emph{decl-cl}:
[cont & [\type*{austinian}
soa & / \1 ] ]
$\rightarrow$
H [cont & / \1]
}
\ex\label{inter-cl}
\avm{
\emph{inter-cl}:
[cont & question] $\rightarrow$  \ldots
}
\end{xlist}
\end{exe}

In contrast to earlier approaches to semantics in HPSG where combining a VP with a subject amounted to nothing more than adding the relevant information in the event structure (akin to functional application), this more constructional approach adds to the ``traditional'' subject-predicate construction a type-shift unary rule that maps a state of affairs description onto a proposition. In other words, the analysis of clause types familiar from traditional grammar plays an explicit role in the grammar, as they are associated  with a particular kind of semantic content.  Interrogative clauses (clauses of sort \type{inter-cl}) are partially defined by their message, i.e.\ as denoting questions. Different kinds of interrogatives (polar interrogatives, \word{wh}-interrogatives, and \word{in situ} interrogatives) can then be defined as subsorts of \type{inter-cl}. Because this constructional analysis of clause types is embedded in a multiple inheritance network of constructions, an elegant model of similarities in syntax that do and do not correspond to similarities in meaning becomes possible. For example, \ili{English} declaratives, like typical \word{wh}-interrogatives, can be inverted (and therefore some declaratives are subject-auxiliary-inversion phrases, as in \word{Under no circumstance will I allow Tobi to go out at night}, see \citealt{Fillmore1999} for a study of the family of inversion constructions in English) and, conversely, some interrogatives are not (\word{in situ} interrogatives, in particular), but some must be inverted (polar interrogatives). Embedding a constructional semantics (i.e.\ the association of meaning to particular kinds of clauses) in a multidimensional analysis of phrases allows a model that associates meaning to some structures.
%JPK 12-17 I simply deleted the following, as I don't think it adds much (since we say "some structures) and can indeed be confusing , while recognizing that they need not to. 
It is similar to some versions of Construction Grammar (see \crossrefchapteralt{cxg}), but it does not require phrasal constructions to be associated with an unpredictable meaning (i.e.\ with more than the equivalent of functional application in Categorial Grammar-like approaches). 

\largerpage
One particularly interesting aspect of the constructional semantics of \citet{GinzburgandSag2001} is
that it can model differences in scoping possibilities of the parameters associated with
\word{wh}-phrases that occur as fillers of head-filler structures and those associated with
\word{wh}-phrases that occur \word{in situ}. Consider the sentences in (\ref{baker-exs}) and the
difference in interpretation that they can receive. (The observation is due to \citealt{Baker1970a};
see \citealt[242--246]{GinzburgandSag2001} for discussion.) Sentence (\ref{baker-a}) only has
interpretation (\ref{baker-sem-a}) and, similarly, sentence (\ref{baker-b}) has interpretation
(\ref{baker-sem-b}).


\begin{exe}
\ex\label{baker-exs}
\begin{xlist}
\ex\label{baker-a} Who wondered \emph{who} saw what?
\ex\label{baker-b} Who wondered \emph{what} was seen by who?
\end{xlist}
\ex\label{baker-sem}
\begin{xlist}
\ex\label{baker-sem-a}For which person $x$ and thing $y$ did $x$ wonder who saw $y$.
\ex\label{baker-sem-b}For which person $x$ and person $z$ did $x$ wonder which $z$ saw what.
\end{xlist}
\end{exe}

The generalization seems to be that the scope of the parameters  introduced by \word{wh}-phrases that occur in filler position (i.e.\ as (part of) the filler daughter of a head-filler phrase) is constrained by its surface position, but \word{wh}-phrases that occur \word{in situ} are not so constrained. Thus, \word{who} in (\ref{baker-a})  (\word{what} in (\ref{baker-b})) cannot outscope the embedded clause, but \word{what} in (\ref{baker-a}) (\word{who} in (\ref{baker-b})) can.
The explanation for this puzzling observation runs as follows. \word{Wh}-interrogatives are a subsort of interrogative clauses and head-filler phrases. They are thus subject to the Filler-Inclusion Constraint in (\ref{fic}) that requires the \attrib{wh} value of the filler to be a retrieved parameter (i.e.\ become part of the \attrib{params} set; \isi{$\uplus$} in the statement of the constraint stands for disjoint union, i.e.\ the intersection of the two sets is the empty set). 
%the two sets cannot contain identical members). 
%JPK 12-17 added a parenthesis explaining uplus
%fmr12-18: how about: ``the intersection of the two sets is the empty set''? too complicated for informal readers?
%JPK 12-18 much nicer I put it in
This constraint ensures that \word{wh}-phrases that are fillers of a head-filler phrase contribute their parameter in the clause they are fillers of. In contrast, the parameter of \word{wh}-phrases that remain \word{in situ} are not so constrained and are thus free either to be retrieved in the clause in which they occur or to be retrieved in a higher clause. 

\begin{exe}
\ex\label{fic}Filler Inclusion Constraint:\\
\avm{
\emph{wh-inter-cl}:
[cont & [params & \{ \1 \} \uplusAVM set] ]
$\rightarrow$
[wh & \{ \1 \}], H
}
\end{exe}

\largerpage
It should be noted that the combination of a constructional and a surface"=oriented approach to the
semantics of interrogatives requires positing several unary branching constructions whose sole
function is to ``type-shift'' the meaning of the daughter phrase to match the semantic requirements
of the phrase it occurs in. Consider the discourses in (\ref{rep-is-ex}) and (\ref{dir-is-ex}) (from
\citealt[270, (37) and 280 (63a)]{GinzburgandSag2001}), a reprise and non-reprise use, respectively,
of \word{in situ} \word{wh}-phrases.

\begin{exe}
\ex\label{rep-is-ex}
\begin{xlist}
\exi{A:}Jo saw absolutely every shaman priest from East Anglia.
\exi{B:} Jo saw absolutely every shaman priest from \textsc{where}.
\end{xlist}
\ex\label{dir-is-ex}
\begin{xlist}
\exi{A:} Well, anyway, I'm leaving.
\exi{B:} OK, so you'll be leaving \textsc{when} exactly?
\end{xlist}
%\ex\label{emb-is-ex}I wondered [Marc met who].
\end{exe}

We focus on the latter case, which involves an ``ordinary'' question interpretation, for simplicity. Since B's answer is syntactically a declarative subject-predicate clause, its meaning will be of sort \type{proposition} (as will that of any head-subject clause that is not a \word{wh}-subject clause). But the meaning associated with this construction is that of a question. So, we need a unary-branching construction that maps the propositional meaning onto the question meaning, i.e.\ that retrieves the stored parameter contributed by the \word{wh}-phrase and makes the \attrib{content} of the head-subject phrase the value of the \attrib{prop} attribute of a question. This is what is accomplished by the \type{is-inter-cl} construction defined in (\ref{is-inter-cl}). One of the subsorts of this construction is the one involved in discourse (\ref{dir-is-ex}) and defined in (\ref{dir-is-int-cl}) (the Independent Clause feature value ``$+$'' in (\ref{is-inter-cl}) is meant to prevent
%in \ili{English} sentences such as (\ref{emb-is-ex}), i.e.\ preventing 
\word{in situ} interrogatives from being embedded interrogatives). 
%JPK 12-17 edited what just precedes to make it \ili{English}!
Assigning distinct messages to
different clause types while maintaining a surface-oriented approach requires quite a few such unary
branching constructions whose function is strictly semantic.\footnote{\citet{MuellerSatztypen}
  argues that a surface-oriented grammar does not have to rely on a hierarchy of clause types to
  model the interaction of clause types and semantics: appropriate lexical specifications on verbs
  (as the heads of sentences) and phrasal principles that exploit the local internal structure of a
  sentence's immediate daughters can be used to achieve the same effects. See also \crossrefchaptert[Section~\ref{sec-mixed-approaches}]{order}.}
%, i.e.\ ensuring the right message is associated with a clause structure which otherwise is associated with another kind of message. 



\begin{exe}
\ex\label{is-inter-cl}
\avm{
\emph{is-inter-cl}: \\
[cat &	[ic & $+$ \\
		subj & < > \\
		vform & fin] ]
$\rightarrow$
H ![ ]!
}
\ex\label{dir-is-int-cl}
\avm{
\emph{dir-is-inter-cl}:  \\
[cont|prop & \1]
$\rightarrow$
H [cont & \1]
}
\end{exe}

%fmr12-21: turned feature ``pro'' into ``prop'' in AVM to make it consistent with the text



%\begin{exe}
%\ex\label{form-adj}
%\begin{xlist}
%\ex\label{form-adj-a}
%A former senator
%\ex\label{form-adj-b}
%\sem{x$|$former'(senator'(x))}
%\ex\label{form-adj-c}
%{\avmoptions{center}
%\begin{avm}
%\[\asort{$\textit{nom-obj}$}
%index & \@1 \\
%restr & \{ \[reln & former \\
%					arg & \[reln & senator %\\ inst & \@1 \]\] \}
%\]
%\end{avm}}
%\end{xlist}
%\end{exe}


\section{Semantic underspecification}
\label{semantics:sec-semantic-underspecification}

\largerpage
\enlargethispage{3pt}
One of the hallmarks of constraint-based grammatical theories is the view that grammars involve
\emph{descriptions} of structures and that these descriptions can be non-exhaustive or incomplete,
as almost all descriptions are. This is a point that was made clear a long time ago by Martin Kay
in his work on unification (see among others \citealt{Kay1979}). For a long time, the distinction
between (partial) descriptions (possible properties of linguistic structures, what grammars are
about) and (complete) described linguistic structures was used almost exclusively in the syntactic
component of grammars within HPSG. But starting in the mid-90s, the importance of distinguishing
between descriptions and described structures began to be appreciated in HPSG's model of semantics,
as discussed for example in \citet{Nerbonne1993a}, in Frank \& Reyle's HPSG implementation of
Underspecified Discourse Representation Theory \citep{FrankandReyle1992,FrankandReyle1995}, and
\citet*{Copestakeetal1995}, and recent work has also stressed the importance of the same distinction
when modeling inflectional morphology (see \citealt{CrysmannandBonami2016} and
\crossrefchapteralt[Section~\ref{sec:inflection}]{morphology}). Because underspecification,
partiality, and the like are so critical to HPSG, their inclusion in the model of the semantics of
grammar has made recent work in semantics in HPSG quite distinctive from work in semantics within
even conceptually related frameworks such as Lexical\indexlfg Functional Grammar (see
\citealt{BK82a}, among others, and \crossrefchapteralt{lfg}) or variants of Categorial\indexcg
Grammar (see \citealt{Steedman1996}, among others, and \crossrefchapteralt{cg}). Two competing
approaches to semantic underspecification have been developed within HPSG:\ Minimal Recursion
Semantics (henceforth, MRS; see \citealt{Copestakeetal1995}, \citealt{Copestakeetal2001}, and
\citealt{Copestakeetal2005} for introductions to MRS) and Lexical Resource Semantics (henceforth,
LRS; see \citealt{RichterandSailer2004,RichterandSailer2001} and
\citealt{IordachioaiaandRichter2015} for an introduction to LRS). MRS and LRS are not the only two
``recent'' approaches to assembling the meaning of phrases from lexical ``meanings'' (or
resources). \citet{AsudehandCrouch2002}, for example, show how to apply a glue approach to semantic
interpretation to HPSG. Aside from simplification of the Semantics Principle\is{principle!Semantics} (which, under a Glue
Semantics approach, does not distinguish how to compose meaning on the basis of the semantic type of
the daughters, e.g.\ whether one of the daughters is a quantifier), a glue approach leads to
``highly efficient techniques for semantic derivation already implemented for LFG, and which target
problems of ambiguity management also addressed by Minimal Recursion Semantics'' (p.\,1). For
reasons of space, we cannot detail Asudeh and Crouch's glue approach here; we concentrate on MRS and
LRS, as they have been the dominant approaches to semantic composition in HPSG in recent years. But
the existence of yet another approach to semantic interpretation attests of the flexibility of the
HPSG architecture when trying to model the interface between syntax and semantics.
\largerpage

\subsection{Minimal Recursion Semantics}
\label{sec-minimal-recursion-semantics}\label{semantics:sec-mrs}

\subsubsection{Why minimally recursive semantic representations}

MRS\indexmrsstart developed out of computational semantic engineering considerations related to machine translation for face-to-face dialogue that started in the early 90s (see \citealt{Kayetal1992} for an overview of the \verbmobil project). As \citet{Copestakeetal1995} argue, syntactic differences between languages can lead to logically equivalent distinct semantic representations when using traditional ``recursive'' semantic representations. They point out, for example, that the \ili{English} expression \word{fierce black cat} and \ili{Spanish} \word{gato negro y feroz} would be given distinct semantic representations under standard assumptions, as shown in (\ref{gato}).

\begin{exe}
\ex\label{gato}
\begin{xlist}
\ex\label{gatoa}
$\lambda x$.(fierce($x$) $\wedge$ (black($x$) $\wedge$ cat($x$)))
\ex\label{gatob}
$\lambda x$.(cat($x$) $\wedge$ (black($x$) $\wedge$ fierce($x$)))
\end{xlist}
\end{exe}

These distinct semantic representations would make translating these simple nominal expressions from one language to the other difficult. Furthermore, some sentences may be similarly ambiguous in \ili{English} and \ili{Spanish} (for example, sentences that contain generalized quantifiers), and requiring the semantic disambiguation of these sentences prior to translating them into sentences that contain similar ambiguities is inefficient. Semantic representations should only be as disambiguated as the source language grammar entails. For these reasons and others they detail, \citet{Copestakeetal1995} propose to model the semantics of grammar via semantic representations that are as flat (or non-recursive) as possible. To achieve this minimal recursivity despite the fact that disambiguated scope relations among generalized quantifiers require embedding, they add additional variables or handles that serve as labels to particular relations in the flat list of relations and that can serve as ``arguments'' of scopal operators. (\ref{ex-mrs}) and its underspecified and fully disambiguated semantic representations in (\ref{ex-mrs-sem}) illustrate this informally and (\ref{ex-mrs-alt}) more formally. Subscripts on names of relations in the informal representation stand for labels of the formulas they are part of. Thus, $1$ in \word{every$_{1}$($x$, 3, $n$)} is a label for the entire formula. In the more explicit representation in (\ref{ex-mrs-alt}), the label of a formula is written before it and separated from it by a colon (e.g.\ h1:every(x,h3,h2)\,); 
%JPK 12-17 de-italicized as requested
variables over labels are simply labels that do not correspond (yet) to labels of formulas ($h2$ and $h6$).\footnote{\citet{Copestake2007} presents a Neo-Davidsonian version of MRS called R(obust)MRS where arguments of predicates (aside from their event variable) are contributed via independent elementary\is{predication!elementary} predications. Copestake shows that RMRS can be profitably used with shallower analyses, ``including part-of-speech tagging, noun phrase chunking and stochastic parsers which operate without detailed lexicon'' (p.\,73); see \citet{PeldzusandSchlangen2012} for how RMRS allows for the incremental\is{incremental processing} construction of meaning representations in dialogue systems.}

\begin{exe}
\ex\label{ex-mrs}
Every dog chased some cat.
\ex\label{ex-mrs-sem}
\begin{xlist}
\ex\label{ex-mrs-sem-a}
every$_{1}$($x$, 3, $n$), dog$_{3}$($x$), cat$_{7}$($y$), some$_{5}$($y$, 7, $m$), chase$_{4}$($e$, $x$, $y$)
\ex\label{ex-mrs-sem-b}
every$_{1}$($x$, 3, 4), dog$_{3}$($x$), cat$_{7}$($y$), some$_{5}$($y$, 7, 1), chase$_{4}$($e$, $x$, $y$)
\ex\label{ex-mrs-sem-c}
every$_{1}$($x$, 3, 5), dog$_{3}$($x$), cat$_{7}$($y$), some$_{5}$($y$, 7, 4), chase$_{4}$($e$, $x$, $y$)
\end{xlist}
\ex\label{ex-mrs-alt}
\begin{xlist}
\ex\label{ex-mrs-alt-a}
h1:every(x,h3,h2), h3:dog(x), h7:cat(y), h5:some(y,h7,h6), h4:chase(x,y)
\ex\label{ex-mrs-alt-b}
$h2=_{qeq}h4$, $h6=_{qeq}h1$
\ex\label{ex-mrs-alt-c}
$h2=_{qeq}h5$, $h6=_{qeq}h4$
\end{xlist}
\end{exe} 

To understand the use of handles, consider the expression \word{every$_{1}$\textup{(}x, 3, n\textup{)}}. The first argument of the generalized quantifier is the handle numbered 3, which is a label for the formula \word{dog$_{3}$\textup{(}$x$\textup{)}}. The formula that serves as the first argument of \word{every} is fixed:\ it is always the meaning of the nominal phrase that the determiner selects for. But to avoid embedding that relation as the restriction of the quantifier and to preserve the desired flatness of semantic representations, the second argument of \word{every} is not \word{dog$_{3}$\textup{(}$x$\textup{)}}, but the label of that formula (indicated by the subscript $3$ on the predication \word{dog$_{3}$\textup{(}$x$\textup{)}}). Now, in contrast to the quantifier's restriction, which must include the content of the head noun it combines with, the nuclear scope or body of the quantifier is not as restricted. In other words, the semantic representation determined by an MRS grammar of \ili{English} does not fix the second argument of \word{every}, represented here as the variable over handles $n$. The same distinction applies to \word{some}: its first argument is fixed to the formula \word{cat$_{7}$\textup{(}$y$\textup{)}}, but its second argument is left underspecified, as indicated by the variable over numbered labels $m$. Resolving the scope ambiguity of the underspecified representation in (\ref{ex-mrs-sem-a}) amounts to deciding whether \word{every} takes the formula that contains \word{some} in its scope or the reverse; in the first case, $n=5$, in the second, $m=1$. Since the formula that encodes the meaning of the verb (namely, \mbox{\word{chase$_{4}$\textup{(}$e$, $x$, $y$\textup{)}})} is outscoped by the nuclear scope or body of both generalized quantifiers, either constraint will fully determine the relative scope of all formulas in (\ref{ex-mrs-sem}). Although it is possible to use a typed feature structure formalism to resolve scope ambiguities, \citet[309--311]{Copestakeetal1995} argue that it is more efficient for the relevant resolution process not to be part of the grammar and be left to a separate algorithm.



\subsubsection{The nitty-gritty}

We now present a brief outline of how MRS works in typed feature structures. First, the content of an expression is of sort \type{mrs}. Structures of that sort consist of (1) a bag of relations or elementary\is{predication!elementary} predications (the value of \attrib{rels}), (2) a \attrib{hook}, which groups together the labels or handles that correspond to elementary predications that have widest local and global scope and the expression's index (these three semantic objects are what is visible to semantic functors), and (3) a set of constraints on handles that restrict or determine the scope of scope-relevant elementary predications (the value of \attrib{hcons} -- for handle constraints). Each constraint in the value of \attrib{hcons} consists of a greater or equal relation between handles. A representation of the structure of an object of sort  \type{mrs} is provided in (\ref{mrs-str}).

\begin{exe}
\ex\label{mrs-str}
\avm{
[\type*{mrs}
hook &	[index & index \\
		ltop & handle \\
		gtop & handle] \\
rels & \listOf{relation} \\
hcons & \listOf{qeq} ]
}
\end{exe}

Sentence (\ref{mrs-ex-sent}) and its (underspecified) \type{mrs} representation in (\ref{mrs-ex-sc}) illustrate how \type{mrs} structures can be used to capture scope underspecification (see \citealt[306]{Copestakeetal2005}). 

\begin{exe}
\ex\label{mrs-ex-sent}
Every dog probably sleeps.
\ex\label{mrs-ex-sc}
\avm{
[\type*{mrs}
hook &	[\type*{hook}
		gtop & \1 \\
		ltop & \5] \\
rels &	<[\type*{every\_rel}
		lbl & \2 \\
		arg0 & \3 \\
		rstr & \4\\
		body & handle],
		[\type*{dog\_rel}
		lbl & \6 \\
		arg0 & \3]
		,
		[\type*{prbly\_rel}
		lbl & \5 \\
		arg1 & \7]
		,
		[\type*{sleep\_rel}
		lbl & \8 \\
		arg1 & \3]> \\
hcons &	<[\type*{qeq}
		harg & \1 \\
		larg & \5]
		,
		[\type*{qeq}
		harg & \4 \\
		larg & \6]
		,
		[\type*{qeq}
		harg & \7 \\
		larg & \8]>				
]
}
\end{exe}

\largerpage
\enlargethispage{4pt}
Members of \attrib{rels} correspond to the content of lexical entries while members of
\attrib{hcons} constrain the relative scope of semantic arguments of members of \attrib{rels}. Now,
although the grammar of \ili{English} leaves the meaning of (\ref{mrs-ex-sent}) underspecified, it
\emph{does} constrain some scope relations, and the \type{mrs} in (\ref{mrs-ex-sc}) therefore
constrains how some elementary predications relate to each other. First, the identity between the
value of \attrib{arg0} for both the \type{every\_rel} and \type{dog\_rel} elementary predications
indicates that \word{every} in (\ref{mrs-ex-sent}) quantifies over dogs; \ibox{3} is the variable
bound by the quantifier. And similarly, the value of \attrib{arg1} of \word{sleep\_rel} is lexically
constrained to correspond to the index of the subject, itself constrained to be identical to the
value of \attrib{arg0} for the \word{dog\_rel} predication (i.e.\ \ibox{3}). Second,
\type{prbly\_rel} is required to outscope \type{sleep\_rel} (a \type{qeq} constraint either
identifies its \attrib{harg} or \attrib{larg} or it constrains its \attrib{harg} to outscope its
\attrib{larg}).  Similarly, the restriction of \type{every\_rel} is constrained to outscope
\type{dog\_rel} as \ibox{4} $=_{qeq}$ \ibox{6}. Finally, the global top (the value of \attrib{gtop})
is constrained to outscope the local top (the value of \attrib{ltop}). (To simplify, the local top
is the handle of the elementary predication that is not a quantifier with the widest scope.) The
semantic representation that the grammar of \ili{English} motivates remains underspecified, as it
does not specify what the value of the \attrib{body} of \type{every\_rel} is, in particular whether
it is the handle of the \type{prbly\_rel} or \type{sleep\_rel} elementary predications. Resolving
this scope ambiguity amounts to adding an \attrib{hcons} that identifies the value of \attrib{body}
with either handle, i.e.\ \ibox{5} or \ibox{8}.

Examples that include multiple quantifiers work in a similar way. Take the sentence in (\ref{2-quant-ex}) and the elementary predications for \word{every}, \word{chases}, and \word{some} (we only include relevant elementary predications and attributes for simplicity). We know that the body of \type{every\_rel} and \type{some\_rel} each outscope \word{chase\_rel} (so \mbox{\ibox{1} $=_{qeq}$ \ibox{3}} and \ibox{2} $=_{qeq}$ \ibox{3}, where the left-hand side of the equality corresponds to the \attrib{harg} and the right-hand side to the \attrib{larg}).\footnote{\citet{Copestakeetal2005} do not explicitly require the nuclear scope of generalized quantifiers to outscope the predicate denoted by the verb they are syntactic dependents of, as it follows from some general assumptions about the structure of fully resolved MRS. Since Lexical Resource Semantics does so, we include additional \ibox{1 } $=_{qeq}$ \ibox{3} and \ibox{2} $=_{qeq}$ \ibox{3} constraints in the text and their effect in Figure~\ref{fig:MRS-tree} for ease of comparison. Nothing critical hinges on this issue.} But we do not know if \type{every\_rel} outscopes \type{some\_rel} or the reverse; adding either \attrib{hcons} \ibox{1} $=_{qeq}$ \ibox{2} or \ibox{2} $=_{qeq}$ \ibox{1} specifies which is the case. (This example illustrates that $=_{qeq}$ is not commutative, as it is meant to encode greater or equal scope.) Figure~\ref{fig:MRS-tree} provides a tree representation of the underspecified outscope relation induced by $=_{qeq}$ constraints; dashed lines indicate that there may be intervening semantic material between the operator and what it outscopes.

\begin{exe}
\ex\label{2-quant-ex}Every dog chases some cat.
\ex\label{2-quant-sem}
\avm{
[\type*{every\_rel}
restr & handle \\
body & \1 ]
%
\hspace{.15in}
%
[\type*{some\_rel}
restr & handle \\
body & \2 ]
%
\hspace{.15in}
%
[\type*{chase\_rel}
lbl & \3 ]
}
\end{exe}

\begin{figure}
\begin{forest}
[ top
[ {chased(x,y)}, edge=dashed
	[ every, no edge
		[ dog(x), edge=dashed ]
		[ {}, no edge 
				[ some(y), no edge 
					[ cat (y), edge=dashed ] 
					[ {}, [ {chased(x,y)}, edge=dashed	 ] ] ] ] 
		[ {}, [ {chased(x,y)}, edge=dashed ] ]			
					] 
	]
	]
\end{forest}
\caption{\label{fig:MRS-tree}A graphical representation of the underspecified scope relations induced by $=_{qeq}$ constraints for sentence (\ref{2-quant-ex})}
\end{figure}

\largerpage
\enlargethispage{5pt}
Semantic composition within MRS is relatively simple and is stated in (\ref{mrs-comp})
\citep[313--314]{Copestakeetal2005}; the third clause of this semantic composition rule amounts to a
case-based definition, as is true of all Semantics Principles\is{principle!Semantics} since \citet{PollardandSag1987}, as
different constructions determine differently the \attrib{hook} of the head daughter
(\citealt{Copestakeetal2005} only discuss intersective and scopal constructions in their
paper).\footnote{A slot in (\ref{mrs-comp}--4) is defined as ``a semantic argument position in a
  word or phrase A that is associated with syntactic constraints on the word or phrase B whose
  semantics will supply that argument when the relevant grammar rule combines A and B''
  \citep[313]{Copestakeetal2005}.}

\begin{exe}
\ex\label{mrs-comp}
\begin{enumerate}
\item The \attrib{rels} value of a phrase is the concatenation (append\isrel{append}) of the \attrib{rels}
values of the daughters.
\item The \attrib{hcons} of a phrase is the concatenation (append) of the \attrib{hcons} values of the daughters.
\item The \attrib{hook} of a phrase is the \attrib{hook} of the semantic head daughter, which is determined uniquely for each construction type.
\item One slot of the semantic head daughter of a phrase is identified with the \attrib{hook} in the other daughter. 
\end{enumerate}
\end{exe}

This quite brief description of \attrib{mrs} illustrates what is attractive about it from an
engineering point of view. Semantic composition is particularly simple:\ concatenation of lists
(lists of elementary predications and constraints), percolation of the \attrib{hook} from the
semantic head, and some general constraint on connectedness between the head daughter and the
non-head daughter. Furthermore, resolving scope means adding $=_{qeq}$ constraints to a list of
$=_{qeq}$, thus avoiding traversing the semantic tree to check on scope relations. Furthermore, a
flat representation makes translation easier, as argued in \citet{Copestakeetal1995}, and has
several other advantages from an engineering perspective as detailed in \citet{Copestake2009}. The
ease flat representations provide comes at a cost, though, namely that semantic representations are
cluttered with uninterpretable symbols (handles) and, more generally, do not correspond to
sub-pieces of a well-formed formula. For example, we would expect the value of a quantifier
restriction and nuclear scope to be, say, formulas denoting sets (as per
\citealt{BarwiseandCooper1981}), not pointers to or labels of predications. This is not to say that
a compositional, ``standard'' interpretation of MRS structures is not possible (see, for example,
\citealt{Copestakeetal2001}); it is rather that the model-theoretic interpretation of MRS requires
adding to the model hooks and holes, abstract objects of dubious semantic import. While it is true,
as \citeauthor{Copestakeetal2005} point out, that abstract objects have been included in the models
of other semantic approaches, \isi{Discourse Representation Theory} (DRT) in particular \citep{Zeevat1989}, abstract objects in
compositional specifications of DRT and other such dynamic semantic approaches are composed of
semantically interpretable objects. In the case of DRT, the set of variables (discourse referents)
that form the other component of semantic representations (aside from predicative conditions) are
anchored to individuals in the ``traditional'' model-theoretic sense. Holes and hooks, on the other
hand, are not necessarily so anchored, as labels (handles) do not have any interpretation in the
universe of discourse.


An example of the model-theoretic opacity of handles is provided by the compositional semantics of intersective attributive adjectives. The \attrib{rels} value of \emph{white horse}, for example, is as shown in (\ref{int-mod}) (after identification of the handles of the labels due to the meaning composition performed by the \type{intersective\_phrase} rule that (intersective) adjectival modification is a subsort of).

\begin{exe}
\ex\label{int-mod}
\avm{
[rels & <[\type*{white\_rel}
		lbl & \1 \\
		arg0 & \2]
		,
		[\type*{horse\_rel}
		lbl & \1 \\
		arg0 & \2]> ]
}
\end{exe}

The fact that the value of \attrib{arg0} is the same for both elementary predications \iboxb{2} is model-theoretically motivated: both properties are predicated of the same individual. 
%JPK 12-17: No, it should be "of", this is a technical, philosophical use
The fact that the value of \attrib{lbl} is identical \iboxb{1} is also motivated if labels are used to help determine the scope of quantifiers; in a quantifier like \word{every white horse}, the content of \word{white} and \word{horse} conjunctively serve as the restriction of \type{every\_rel} represented in  (\ref{every-hand}).

\begin{exe}
\ex\label{every-hand}
\avm{
[\type*{every\_rel}
restr & handle \\
body & handle ]
}
\end{exe}

 But the identity of the two elementary predications' labels is not \emph{directly} model-theoretically motivated. It is a consequence of the semantic representation language that is used to model the meaning of sentences, not a consequence of the sentences' truth conditions.
\indexmrsend


\subsection{Lexical Resource Semantics}
\label{semantics:sec-lrs}

Whereas\indexlrsstart MRS emphasizes underspecification in semantic representations and expresses the syntax of underspecified representations in HPSG as typed feature structures, LRS focuses primarily on fine-grained linguistic analyses with explicit higher-order logics for meaning representation and utilizes underspecification prominently in the architecture of the syntax-semantics interface. Instead of encoding underspecified representations as denotations of grammar principles, it uses the feature logic itself as a tool for underspecifying fully specific logical representations in the symbolic languages of the literature on formal semantics. This means that a grammar with LRS semantics denotes sets of syntactic structures that comprise unambiguous meaning representations in a standard logical language, but it does so by means of underspecification in the grammar principles. By formulating very general (``underspecified'') grammar principles which define the relationship between syntactic structure and semantic representation, LRS follows the lead of HPSG syntax.
%JPK 12-17: I will let you handle that one, Frank, to make sure it's what you meant, but I think Elizabeth is right, it's a mouthful. Maybe 2 sentences would be better.
%fmr12-18: split the sentence in two, reformulated the second half: clearer?
%JPK 12-18: Much better. I slightly reformulated the beginning of the previous sentence. Hope it's OK
Grammar principles may admit a large number of structures, which in this case can be multiple semantic representations compatible with one and the same syntactic structure. An LRS analysis may then represent the readings of a sentence with two generalized quantifiers like (\ref{ex-mrs}), \word{every dog chased some cat} (repeated below as (\ref{ex-mrs2})) -- i.e.\ the two readings shown in (\ref{lrs-introex}) -- as distinct possible values of a semantics feature. 


\ea
\label{ex-mrs2}
Every dog chased some cat.
\z

\eal
\label{lrs-introex}
\ex\label{lrs-introex1a}
  
  $\forall \left(\lambda x.\text{dog}_w(x),\lambda x.\exists \left(\lambda y.\text{cat}_w(y),\lambda y.\text{chase}_w(x,y)\right)\right)$ 
\ex\label{lrs-introex1b}
  $\exists \left(\lambda y.\text{cat}_w(y),\lambda y.\forall \left(\lambda x.\text{dog}_w(x),\lambda x.\text{chase}_w(x,y)\right)\right)$ 
\zl

%${\left<\left<et\right>\left<\left<et\right>t\right>\right>}$

The syntactic format of semantic representations is flexible and can be adapted to the purposes of the linguistic analysis at hand. While (\ref{lrs-introex}) chooses predicates with an argument for possible worlds, lambda abstraction over the unary predicates which translate the nominal arguments, and categorematic quantifiers of type ${\left<\left<et\right>\left<\left<et\right>t\right>\right>}$, in many contexts less elaborate representations will suffice, and the two readings would be rendered in a notational variant of first order languages. Other phenomena might necessitate more semantic structure. The LRS framework makes a selection of choices available to linguists to decide what is most adequate to spell out a semantic analysis.


\subsubsection{Basic architecture}
\label{semantics:sec-basic-architecture}

%Essential ideas of LRS:
Lexical items contribute semantic resources to utterances; every semantic representation of an utterance must use up all and only the semantic resources provided by the lexical items in the utterance in all their legitimate combinations.\footnote{Lexical items may be phrasal.} What is legitimate is determined by semantic principles which restrict at each phrase how the semantic resources of its daughters may be combined. Anything these restrictions do not rule out is permitted. Scope ambiguities between co-arguments of a verb can be seen as arising from the lack of a principled restriction to the effect that one outscopes the other. In the absence of restrictions, LRS expects ambiguity. As a special property setting LRS apart from other semantic underspecification frameworks, LRS semantics exploits HPSG's notion of structure sharing in its semantic representations by permitting that semantic contributions of different lexemes may in fact be identical. For example, if two words in a clause contribute \isi{negation} in their meaning, the two negations may in fact turn out to be the same negation, in which case we observe a \isi{negative concord} reading. The implementation of this idea is based on the fundamental structure-sharing mechanism of HPSG, which is available throughout all levels of grammatical description.

The combinatorial semantics of phrases is encoded with structures of sort \type{lrs}:

%1.\ combinatoric semantics
%attributes: \attrib{lrs}, \attrib{excont}, \attrib{incont},
%\attrib{parts}:

\begin{exe}
  \ex\label{lrs-str}
\avm{
  [sem &	[\type*{lrs}
            excont & me\\
            incont & me\\
            parts & list!\textup{(}me\textup{)}!] ]
}
\end{exe}

Signs have an attribute \attrib{sem}antics with value
\type{lrs}. External content (\attrib{excont}) and internal content
(\attrib{incont}) designate two prominent aspects of the semantics of
signs. Both of these attributes have
values of sort \type{meaningful\_expression}, for short
\type{me}. The attribute \attrib{excont} contains a term
that represents the meaning of the maximal syntactic projection of the sign
and is built from semantic material contributed within the projection. The
\attrib{incont} is that part of a lexical sign's representation which is
outscoped by any scope-taking operator that it combines with within
its syntactic projection. The \attrib{parts} list records all semantic
resources contributed by a given sign. The LRS Projection Principle\is{principle!LRS Projection|(} in (\ref{lrs-projection})
governs the percolation of these attribute values along the syntactic
head path of phrases, whereas the \attrib{excont} and \attrib{incont}
Principles in (\mex{1}b--c) determine the relationship of the respective attribute
values to other semantic attribute values within local syntactic
trees. The most important relationships are those of term identity and
of subtermhood of one term relative to another or to some designated part
of another term. Subterm restrictions are in essence similar
to the \type{qeq} constraints of MRS.

\begin{exe}
  \ex\label{lrs-essent-principles}
  \begin{xlist}
    \ex\label{lrs-projection} LRS Projection Principle\\
    In each phrase,
    \begin{enumerate}
    \item the \attrib{excont} values of syntactic head and mother are identical,
    \item the \attrib{incont} values of syntactic head and mother are identical,
    \item the list in the \attrib{parts} value contains all and only the elements
    of the \attrib{parts} values of the daughters.
    \end{enumerate}
    \ex\label{lrs-incont} \attrib{incont} Principle\\
    In each \type{lrs}, the \attrib{incont} value is an element of the
    \attrib{parts} list and a component of the \attrib{excont} value.
    \ex\label{lrs-excont} \attrib{excont} Principle\\
    First, in every phrase, the \attrib{excont} value of the non-head daughter
    is an element of the non-head daughter's \attrib{parts} list. Second, in
    every utterance, every subexpression of the \attrib{excont} value of the
    utterance is an element of its \attrib{parts} list, and every element of
    the utterance's \attrib{parts} list is a subexpression of the \attrib{excont}
    value.
  \end{xlist}
\end{exe}

The Projection Principle guarantees the percolation of \attrib{excont} and
\attrib{incont} values along the head path of syntactic phrases, and it records
the semantic resources available at each phrase based on the semantic
contributions of their daughters (\ref{lrs-projection}). The \attrib{incont} Principle and the \attrib{excont} Principle manage the properties of the respective attribute values. The term with minimal scope of each lexeme must be contributed by the lexeme itself and must be semantically realized within the representation of the maximal syntactic head projection (\ref{lrs-incont}). The maximal semantic meaning contribution of a maximal syntactic projection must originate from within that maximal projection, and an utterance (as a distinguished maximal projection) consists of all and only those pieces of semantic representation which are contributed by some lexeme in the utterance (\ref{lrs-excont}). The meaning of an utterance is given by the semantic representation which is its \attrib{excont} value. An ambiguous utterance receives structural analyses that are potentially only distinguished by different \attrib{excont} values of their root node.
\is{principle!LRS Projection|)}

The constraints in (\ref{lrs-essent-principles}) take care of the integrity of the semantic combinatorics. The task of the clauses of the Semantics Principle\is{principle!Semantics} is to regulate the semantic restrictions on specific syntactic constructions (as in all previously discussed versions of semantics in HPSG). A quantificational determiner, represented as a generalized quantifier, which syntactically combines as non-head daughter with a nominal projection, integrates the \attrib{incont} of the nominal projection as a subterm into its restrictor and requires that its own \attrib{incont} (containing the quantificational expression) be identical with the \attrib{excont} of the nominal projection. This clause makes the quantifier take wide scope in the noun phrase and forces the semantics of the nominal head into the restrictor. In (\ref{lrs-introex}) we observe the effect of this clause by the placement of the predicate {\normalfont \sffamily dog} in the restrictor of the universal and the predicate {\normalfont \sffamily cat} in the restrictor of the existential quantifier.

Another clause of the Semantics Principle governs the combination of quantificational NP arguments with verbal projections. If the non-head of a verbal projection is a quantificational NP, the \attrib{incont} of the verbal head must be a subexpression of the scope of the quantifier. Since this clause does not require immediate scope, other quantificational NPs which combine in the same verbal projection may take scope in between, as we can again see with the two possible scopings of the two quantifiers in (\ref{lrs-introex}), in particular in (\ref{lrs-introex1b}), where the subject quantifier intervenes between the verb and the object quantifier.


The local semantics of signs is split from the combinatorial \type{lrs} structures in parallel to the separation of local syntactic structure from the syntactic tree structure. The local semantics remains under the traditional \attrib{content} attribute, where it is available for lexical selection by the valence attributes. The \attrib{local} value of the noun \word{dog} illustrates the relevant structure:

\begin{exe}
  \ex\label{local-sem}
\avm{
 [\type*{local}
 cat|spr & <\upshape Det$_{\1}$> \\
 content &	[index|dr & \1 \upshape \sffamily x \\
            main & \upshape \sffamily dog ] ]
}
\end{exe}

The attribute \feat{discourse-referent} (\attrib{dr}) contains the variable that will be the argument of the unary predicate {\normalfont \sffamily dog}, which is the \attrib{main} semantic contribution of the lexeme. The variable, {\normalfont \sffamily x}, does not come from the noun but is available to the noun by selection of the determiner by the valence attribute \attrib{spr}. The subscripted tag \ibox{1} on the \attrib{spr} list indicates the identity of \attrib{dr} values of the determiner and the nominal head \word{dog}. A principle of local semantics says that \attrib{main} values and \attrib{dr} values are inherited along the syntactic head path.


\begin{figure}
%\small
\begin{forest}
[%
\avm{
	[\phon  < every, dog >\\
  \ldots\ cont & [index|dr & \tag{2a} {\normalfont \sffamily x}\\
                      main & \tag{3a} {\normalfont \sffamily dog}]\\
  sem & [exc & \2~!{\normalfont \sffamily $\forall$x ($\phi$,$\psi$)}!\\
          inc & \3~!{\normalfont \sffamily dog(x)}!\\
          pts & <\2, \tag{2a}, \tag{2b}, \tag3, \tag{3a} >] ]
}
\\ \& \ibox{$3$} $\triangleleft$ $\phi$
	[%
	\avm{
		[\phon  < every >\\
		\ldots\ cont &	[index|dr & \tag{2a} {\normalfont \sffamily x}\\
	                         main & \tag{2b} $\forall$]\\
		sem  &	[exc & \2 \\
			 inc & \2~!{\normalfont \sffamily $\forall$x ($\phi$,$\psi$)}! \\
	          	 pts & <$\forall$, {\normalfont \sffamily x}, !{\normalfont \sffamily $\forall$x ($\phi$,$\psi$)}!>] ]
	}
	]
	[%
	\avm{
		[\phon < dog >\\
		ss|l & [cat|spr & < \textrm{Det}$_{\tag{2a}}$ >\\
	                cont & [index|dr & \tag{2a}\\
	             		main & \tag{3a} {\normalfont \sffamily dog}]]\\
		sem & [exc & \2 \\
	               inc & \3~!{\normalfont \sffamily dog(x)}!\\
	               pts & <!{\normalfont \sffamily dog}, {\normalfont \sffamily dog(\tag{2a})}!>] ]
	}
	]
]
\end{forest}
\caption{\label{every-dog-sem}Combining the meaning of \word{every} and \word{dog}}
\end{figure}

\largerpage
The semantics of phrases follows from the interaction of the (lexical) selection of local semantic
structures and the semantic combinatorics that results from the principles in
(\ref{lrs-essent-principles}) and the clauses of the Semantics Principle.  For ease of readability,
Figure~\ref{every-dog-sem} omits the lambda abstractions from the generalized quantifier, chooses a
notation from first order logic, and does not make all structure sharings between pieces of the
logical representation explicit. The head noun \word{dog} contributes (on \attrib{parts},
\attrib{pts}), the predicate {\normalfont \sffamily dog} and the application of the predicate to a
lexically unknown argument, \ibox{2a}, identical with the \attrib{dr} value of \word{dog}. As shown
in (\ref{local-sem}), the \attrib{dr} value of the noun is shared with the \attrib{dr} value of the
selected determiner, which is the item contributing the variable {\normalfont \sffamily x} to the
representation. In addition, \word{every} contributes the quantifier and the application of the
quantifier to its arguments. The clause of the Semantics Principle which restricts the combination
of quantificational determiners with nominal projections identifies the \attrib{inc} of \word{every}
with the \attrib{exc} of \word{dog}, and requires that the \attrib{inc} of dog \iboxb{3} be a
subterm of the restrictor of the quantifier, $\phi$ (notated as `\ibox{3} $\triangleleft$ $\phi$',
conjoined to the AVM describing the phrase). The identification of the \attrib{exc} and \attrib{inc}
of \word{every} follows from (\ref{lrs-essent-principles}b--c). According to this analysis, the
semantic representation of the phrase \word{every dog} is a universal quantification with
{\normalfont \sffamily dog(x)} in the restrictor and unknown scope ($\psi$). The scope will be
determined when the noun phrase combines with a verb phrase. For example, such a verb phrase could
be \word{barks}, as in Figure~\ref{every-dog-barks-sem}. If its semantics is represented as a unary
predicate {\normalfont \sffamily bark}, the predicate and its application to a single argument are
contributed by the verb phrase, and local syntactic selection of the subject \word{every dog} by the
verb \word{barks} identifies this argument as variable {\normalfont \sffamily x}, parallel to the
selection of the quantifier's variable by \word{dog} above. The relevant clause of the Semantics
Principle requires that {\normalfont \sffamily bark(x)} be a subterm of $\psi$, and the
\attrib{exc}, \ibox{2}, of the complete sentence receives the value \mbox{\normalfont \sffamily
  $\forall$x (dog(x), bark(x))} as the only available reading in accordance with the \attrib{excont}
Principle.


\begin{figure}
%\small
\begin{forest}
[%
\avm{
	[\phon < every, dog, barks >\\
	\ldots\ cont & [main & \tag{1a} {\normalfont \sffamily bark} ] \\
	sem &	[exc & \2~!{\normalfont \sffamily $\forall$x ($\phi$,$\psi$)}! \\
			inc & \1~!{\normalfont \sffamily bark(x)}!\\
			pts & <\2, \tag{2a}, \tag{2b}, \3, \tag{3a}, \1, \tag{1a}>]]
}
\\ \& \ibox{$1$} $\triangleleft$ $\psi$
	[%
	\avm{
		[\phon < every, dog > \\
		\ldots\ cont & [index|dr & \tag{2a} {\normalfont \sffamily x}\\
				main & \tag{3a} {\normalfont \sffamily dog}]\\
		sem & [exc & \2~!{\normalfont \sffamily $\forall$x ($\phi$,$\psi$)}!\\
          inc & \3~!{\normalfont \sffamily dog(x)}! \\
          pts & <\2, \tag{2a}, \tag{2b}, \3, \tag{3a} > ] ]
	}
	\\ \& \ibox{$3$} $\triangleleft$ $\phi$
	]
	[%
	\avm{
		[\phon  < barks > \\
		ss|l &	[cat  & [subj & < NP$_{\tag{2a}}$ >] \\
			 cont &	[main & \tag{1a} {\normalfont \sffamily bark}]]\\
		sem &	[exc & \2 \\
				inc & \1~!{\normalfont \sffamily bark(x)}! \\
				pts & <\tag{1a} {\normalfont \sffamily bark}, \tag{1}~!{\normalfont \sffamily bark(\tag{2a})}!> ] ]
	}
	]
]
\end{forest}
\caption{\label{every-dog-barks-sem}Combining the meaning of \word{every dog} and \emph{barks}}
\end{figure}

\largerpage
In Figure~\ref{every-dog-barks-sem},
the identity of the restrictor $\phi$ of the universal quantifier with
\ibox{3} {\normalfont \sffamily dog(x)} and of its scope $\psi$ with \ibox{1} {\normalfont \sffamily 
  bark(x)} are determined at the utterance level by the lack of other
material that could be added to the two arguments of the
quantifier. For example, an extraposed relative clause which belongs
to \word{every dog} could consistently contribute its meaning
representation to the restrictor, and only the absence of such
additional semantic material leads to the inferred identity of \ibox{3} with
$\phi$.

Underspecification of the structure of meaning representations in the
clauses of the Semantics Principle and in lexical entries interacts
with the possibility of structure sharing. If two pieces of meaning
representation have the same shape and obey compatible structural
conditions (as determined by relevant subterm constraints), they can be
identical. Even stronger, in certain grammatical constellations,
principles of grammar may even require their
identity. Lexical underspecification of meaning contributions moreover
permits the shared construction of functors such as the construction
of a polyadic quantifier from several lexical items in a
sentence. These two applications of LRS lead to new possibilities
of semantic composition compared to standard compositional semantics
in Mainstream Generative Grammar, because functors can be composed in (logical) syntax
which cannot be semantically decomposed or cannot be decomposed within
the structural limits of a surface-oriented syntax, i.e.\ a syntactic
structure which only reflects syntactic but not semantic composition.

%The two types of construction are illustrated below.




Consider the semantic representation of the \ili{Polish} sentence \word{nikt nie przyszed\l} `nobody came' in Figure~\ref{polish-nc-sem}. 

%fmr12-21: tag 2c was missing on the parts list of the mother (damn! why did I miss that before?)

\begin{figure}\centering
%\footnotesize
\small
\begin{forest}
[%
\avm{
	[\phon < nikt, nie przyszed\l > \\
	\ldots\ cont & [main & \tag{4a} {\normalfont \sffamily come}] \\
	sem &	[exc & \1~!{\normalfont \sffamily $\neg\exists$x ($\phi$,$\psi$)}! \\
	inc & \4~!{\normalfont \sffamily come(x)}! \\
	pts & <\2, \tag{2a}, \tag{2b}, \tag{2c}, \3, \tag{3a}, \tag4, \tag{4a} >] ]
}
\\ \& \ibox{$4$} $\triangleleft$ $\psi$
	[%
	\avm{
		[\phon  < nikt > \\
		\ldots\ cont &	[index|dr & \tag{2a} {\normalfont \sffamily x}\\
	                      main & \tag{3a} {\normalfont \sffamily person} ] \\
		sem & [exc & \2~!{\normalfont \sffamily $\exists$x ($\phi$,$\psi$)}! \\
	          inc & \3~!{\normalfont \sffamily person(x)}! \\
	          pts & <\2, \tag{2a}, \tag{2b}, \tag{2c}$\neg\beta$, \3, \tag{3a} >] ] 
	}
	\\ \& \ibox{$3$} $\triangleleft$ $\phi$ \& \ibox{$2$} $\triangleleft$ $\beta$
	]
	[%
	\avm{
		[\phon  < nie przyszed\l > \\
		ss|l &	[cat  & [subj & < NP$_{\tag{2a}}$ > ]\\
             	         cont & [main & \tag{4a} {\normalfont \sffamily come}] ]\\
		sem &	[exc & \1 \\
          		inc & \4~!{\normalfont \sffamily come(x)}!\\
          		pts & <{\normalfont \sffamily come}, !{\normalfont \sffamily come(\tag{2a}), \tag{4b}$\neg\alpha$}!>] ]
	}
	\\ \& \ibox{$4$\textup{b}} $\triangleleft$ \ibox{$1$} \& \ibox{$4$} $\triangleleft$ $\alpha$
	]
]
\end{forest}
\caption{\label{polish-nc-sem}Combining the meaning of Polish \word{nikt} and \word{nie przyszed\l}}
%\itdopt{Renumber top down? Starting with \ibox{1} from left to right?\\
%JPK No, too many chances of errors and not worth it for a purely aesthetic and kinda subjective issue :)}
\end{figure}

Negated finite verbs in \ili{Polish} contribute a \isi{negation} that must be realized
within the verb's \attrib{excont} (\ibox{4\textup{b}} $\triangleleft$ \ibox{1}) and
outscopes the \attrib{incont} of the verb (\ibox{4} $\triangleleft$ $\alpha$).
Similarly, the existential quantifier of the n-word \word{nikt} `nobody' is
outscoped by negation (\ibox{2} $\triangleleft$ $\beta$). However, in addition
to the familiar restriction when the quantificational subject combines
with the finite verb, \ili{Polish} as a strict negative concord language requires
that a negated finite verb be in the scope of at most one negation in
its \attrib{excont}, entailing identity of the two negations,
\ibox{2\textup{c}} $=$ \ibox{4\textup{b}}, and the single negation reading \word{nobody came} as
the only admissible reading of the sentence shown in Figure~\ref{polish-nc-sem}.
To capture obligatory negation marking on finite words in \ili{Polish}, a second
principle of negative concord rules that if a finite verb is in the scope
of negation in its \attrib{excont}, it must itself be a contributor of negation \citep[316]{RichterandSailer2001}. The resulting \attrib{excont} value
in Figure~\ref{polish-nc-sem} is \mbox{{\normalfont \sffamily $\neg \exists$ x (person(x), come(x))}}.

The idea of identifying contributions from different constituents in
an utterance is even more pronounced in cases of unreducible polyadic
quantification. The reading of (\ref{different-s-lrs}) in which each
unicorn from a collection of unicorns has a set of favorite meadows that
is not the same as the set of favorite meadows of any other unicorn is known
to be expressible by a polyadic quantifier taking two sets and a binary
relation as arguments (\ref{every-diff-lrs}), but it cannot be expressed by
two independent monadic quantifiers \citep{Keenan1992b}.


\begin{exe}
\ex\label{different-lrs}
\begin{xlist}
\ex\label{different-s-lrs} Every unicorn prefers different meadows.
\ex \label{different-meadows-lrs}
different meadows: \hspace{0.18cm} $(\gamma', \Delta)(\sigma_1, \lambda y.\text{meadow}(y), \lambda \nu_1\lambda y.\rho')$
\ex \label{every-unicorn-lrs}
every unicorn: \hspace{0.95cm}$(\forall, \gamma)(\lambda x.\text{unicorn}(x), \sigma_2, \lambda x\lambda \nu_2.\rho)$
\ex \label{every-diff-lrs}
$(\forall, \Delta)(\lambda x.\text{unicorn}(x), \lambda y.\text{meadow}(y), \lambda x\lambda y.\text{prefer}(x,y))$
\end{xlist}
\end{exe}

\noindent
(\ref{different-lrs}) sketches the LRS solution to this puzzle in
\citet{Richter2016}. The
adjective \word{different} contributes an incomplete polyadic
quantifier of appropriate type which integrates the representation of
the nominal head of its NP into the second restrictor but leaves open
a slot in the representation of its functor for another quantifier it
must still combine with (\ref{different-meadows-lrs}). The determiner
\word{every} underspecifies the realization of its quantifier in such
a way that one of the possible representations yields
(\ref{every-unicorn-lrs}) for \word{every unicorn}, which is exactly
of the right shape to be identified with the representation of
\word{different meadows}, leading to the expression in
(\ref{every-diff-lrs}) for (\ref{different-s-lrs}).
\citet{Lahm2016b} presents an alternative account of such readings
with \word{different} using Skolem functions which also hinges on
LRS-specific techniques. \citet{IordachioaiaandRichter2015} study \ili{Romanian}
negative concord constructions and represent their readings using polyadic negative
quantifiers; \citet{Lahm2018} develops a lexicalized theory
of plural semantics.


\subsubsection{Representation languages and notational conventions}

\enlargethispage{5pt}
Any LRS grammar relies on an encoding of the syntax of an appropriate semantic representation
language in the feature logic. In principle, any finitary logical language can be encoded in
Relational Speciate Reentrant Language, which covers every language that has been proposed for
meaning representations in linguistics. Work in LRS has so far been couched mostly in variants of
Two-sorted Type Theory (Ty2, \citealt{Gallin1975}) as one of the standard languages of formal
semantics, or in Montague's Intensional Logic. The type system of these logical languages is useful
for underspecified descriptions in semantic principles, since relevant groups of expressions can be
generalized over by their type without reference to their internal structure. For example, a clause
of the Semantics Principle\is{principle!Semantics} can use the type of generalized quantifiers to
distinguish quantificational complement daughters of verbal projections and state the necessary
restrictions on how they are integrated with the semantics of the verbal head daughter, while other
types of complement daughters are treated differently and may not even be restricted at all by a
clause in the Semantics Principle in how they integrate with the verbal semantics. The latter is
often the case with proper names and definite descriptions, which can be directly integrated with
the semantics of the verb by lexical argument selection.

Encodings of semantic representations in feature logic are usually assumed as given by the
background LRS theory. Examples of encodings can be found in \citet{Sailer2000a} and
\citet{Richter2004a-u}. \citet{Sailer2000a} offers a correspondence proof of the encoded structures
with a standard syntax of languages of Ty2. As descriptions of logical terms in literal feature
logic are very cumbersome to read and write and offer no practical advantage or theoretical insight,
all publications use notational shortcuts and employ logical expressions with metavariables for
their descriptions instead. As nothing depends on feature logical notation, the gain in readability
outweighs any concerns about notational precision.%
\indexlrsend

\section{Conclusion}

Semantics in HPSG underwent significant changes and variations over the past three decades, and the analyses couched in the different semantic theories were concerned with a wide variety of semantic phenomena. Two common denominators of the approaches are the relative independence of syntactic and
%JPK 12-17 there were indeed two common denominators
%fmr12-18 :-)
%fmr12-21: tow denominators are, not is (changed)
 semantic structure in the sense that the syntactic tree structure is never meant to mirror directly the shape of the syntax of semantic expressions, and the use of HPSG-specific techniques to characterize semantic expressions and their composition along the syntactic tree structure. Of particular relevance here is the use of a rich sort hierarchy in the specification of semantic structures and the use of underspecification in determining their shape, as these two aspects of the HPSG framework play a prominent and distinguishing role in all semantic theories. The flexibility of these tools makes HPSG suitable for the integration of very diverse theories of meaning of natural languages while respecting representational modularity, i.e.\ the 
 %assumption that 
% distinct kinds of information associated with strings (i.e.\ inflectional information, constituency, semantic information) are not the reflections of a single kind of information, say tree configurations, as it typically is assumed to be in Mainstream Generative Grammar.
%JPK 12-21 New formulatiom
assumption that 
 distinct kinds of information associated with strings (e.g.\ inflectional information, constituency, semantic information) are not reflected in a single kind of syntactic information, say tree configurations, as it typically is assumed to be in Mainstream Generative Grammar.
\is{semantics|)}

%\section*{Abbreviations}

\section*{\acknowledgmentsUS}

We thank Jonathan Ginzburg and Stefan M√ºller for many careful critical remarks which helped
us improve this chapter considerably. We thank
Elizabeth Pankratz for editorial comments and proofreading.


{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this] 
}
\end{document}


%      <!-- Local IspellDict: en_US-w_accents -->
