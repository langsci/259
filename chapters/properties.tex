\documentclass[output=paper,biblatex,babelshorthands,newtxmath,draftmode,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.13644935}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \usepackage{../nomemoize}
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[1]
}{}

\title{Basic properties and elements} 
\author{Anne Abeillé\orcid{0000-0002-9187-2298}\affiliation{Université de Paris} and Robert D. Borsley\orcid{0000-0002-4856-4732}\affiliation{University of Essex and Bangor University}}


\abstract{Head-Driven Phrase Structure Grammar (HPSG) is a declarative and monostratal version of Generative Grammar, in which linguistic expressions have a single relatively simple constituent structure. It seeks to develop detailed formal analyses using a system of types, features, and constraints. Constraints on types of \type{lexical-sign} are central to the lexicon of a language and constraints on types of \type{phrase} are at the heart of the syntax, and both lexical and phrasal types include semantic and phonological information. Different versions of the framework have been developed, including versions in which constituent order is a reflection not of constituent structure but of a separate system of order domains, and the Sign-Based Construction Grammar version, which makes a fundamental distinction between signs of various kinds and the constructions which license them.} 


% This chapter was converted from a word submission. Thanks to Elisabeth Eberle.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\label{chapter-basic-properties}\label{chap-properties}

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:prop1}
\label{prop:sec-intro}

Head-Driven Phrase Structure Grammar (HPSG) dates back to early 1985 when Carl Pollard presented his
\emph{Lectures on HPSG}. It was often seen in the early days as a revised version of the earlier
Generalised Phrase Structure Grammar (GPSG) framework \citep*{GKPS85a}, but it was also influenced
by Categorial Grammar \citep{Ajdukiewicz35a-u,Steedman2000a-u}, and, as \citet[1]{ps} emphasised, by
other frameworks like Lexical-Functional Grammar (LFG; \citealt{Bresnan82a-ed}), as well. Naturally
it has changed in various ways over the decades. This is discussed in much more detail in the next
chapter (Flickinger, Pollard \& Wasow \citeyear{chapters/evolution}),
% this does not work. it links the whole footer to the list of references \citep*{chapters/evolution}, 
but it makes sense here to distinguish three versions of
HPSG. Firstly, there is what might be called early HPSG, the framework presented in \citet{ps} and
\citet{ps2}.% 
%
\footnote{As discussed in \crossrefchapterw{formal-background}, the approaches that are developed in
  these two books have rather different formal foundations. However, they propose broadly similar
  syntactic analyses, and for this reason it seems reasonable to group them together as early HPSG.} 
%
This has most of the properties of more recent versions but only exploits the analytic potential of
type hierarchies to a limited degree \citep*{Flickinger87,FPW85a}. Next there is what is sometimes
called Constructional HPSG, the framework adopted in \citew{Sag97a}, \citew{GSag2000a-u}, and much
other work. Unlike earlier work this uses a rich hierarchy of phrase-types. This is why it is called
constructional.% 
%
\footnote{As discussed below, HPSG has always assumed a rich hierarchy of lexical types. One might
  argue, therefore, that it has always been constructional.} 
%
Finally, in the 2000s, Sag developed a version of HPSG called \emph{Sign-Based Construction Grammar}\indexsbcg
(SBCG; \citealt{Sag2012a}). The fact that this approach has a new name suggests that it is very
different from earlier work, but probably most researchers in HPSG would see it as a version of
HPSG, and it was identified as such in \citet[486]{Sag2010b}. Its central feature is the special
status it assigns to constructions. In earlier work, they are just types of sign, but for SBCG,
signs and constructions are quite different objects. In spite of this difference, most analyses in
Constructional HPSG could probably be translated into SBCG and vice versa. In this chapter we will
concentrate on the ideas of Constructional HPSG, which is probably the version of the framework that
has been most widely assumed. We will comment briefly on SBCG in the penultimate section. 

The chapter is organised as follows. In Section~\ref{sec:prop2}, we set out the properties that
characterise the approach and the assumptions it makes about the nature of linguistic analyses and
the conduct of linguistic research. Then, in Section~\ref{sec:prop3}, we consider the main elements
of HPSG analyses: types, features, and constraints. In Section~\ref{sec:prop4}, we look more closely
at the HPSG approach to the lexicon, and in Section~\ref{sec:prop5}, we outline the basics of the
HPSG approach to syntax. In Section~\ref{sec:prop6}, we look at some further syntactic structures,
and in Section~\ref{sec:prop7}, we consider some further topics, including SBCG. Finally, in
Section~\ref{sec:prop8}, we summarise the chapter. 


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Properties}\label{sec:prop2}
\label{prop:sec-properties}

\largerpage[1.5]
Perhaps the first thing to say about HPSG is that it is a form of Generative Grammar in the sense of
\citet[\page 4]{Chomsky65a}. This means that it seeks to develop precise and explicit analyses of
grammatical phenomena. But unlike many versions of Generative Grammar, it is a declarative or
constraint-based approach to grammar, belonging to what \citet{PS2001a} call ``Model Theoretic
Syntax''. As such, it assumes that a linguistic analysis involves a set of constraints to which
linguistic objects must conform, and that a linguistic object is well-formed if and only if it
conforms to all relevant constraints.% 
%
\footnote{In most HPSG work, all constraints are equal. Hence, there is no possibility -- as there
  is in Optimality Theory \citep{PS2004a-u} -- of violating one if it is the only way to satisfy another more
  important one \citep{Malouf2003a}. However, see \citew{MK2000a} and \citew{OFTM2004a-u} for an
  HPSG parser with probabilities or weighted constraints.} 
%
This includes linguistic objects of all kinds: words, phrases, phonological segments, and so
on. There are no procedures constructing representations such as the phrase structure and
transformational rules of classical Transformational Grammar or the \isi{Merge} and \isi{Agree} operations of
\isi{Minimalism}. Of course, speakers and hearers do construct representations and must have procedures
that enable them to do so, but this is a matter of performance, and there is no need to think that
the knowledge that is used in performance has a procedural character. Rather, the fact that it is
used in both production and comprehension (and other activities, e.g.\ translation) suggests that it
should be neutral between the two and hence declarative. For further discussion of the issues, see
e.g.\ \citet{PS2001a}, \citet{Postal2003a}, \citet{SW2011a,SW2015a}, and
\crossrefchaptert{processing}. 

\largerpage[1.5]
HPSG is also a monostratal approach, which assumes that linguistic expressions have a single
constituent structure. This makes it quite different from Transformational Grammar, in which an
expression can have a number of constituent structures. It means, among other things, that there is
no possibility of saying that an expression occupies one position at one level of structure and
another position at another level. Hence, HPSG has nothing like the \isi{movement} processes of
\isi{Transformational Grammar}. The relations that are attributed to movement in transformational work are
captured by constraints that require certain features to have the same value. For example, as
discussed in Section~\ref{prop:sec-lexicon}, a~raising sentence is one with a verb which has the
same value for the feature \textsc{subj(ect)} as its complement and hence combines with whatever
kind of subject its complement requires. 

HPSG is sometimes described as a concrete approach to syntax. This description refers not only to
the fact that it assumes a single constituent structure, but also to the fact that this structure is
relatively simple, especially compared with the structures that are postulated within
Minimalism\indexmp. Unlike Minimalism, HPSG does not assume that all branching\is{branching!binary}
is binary. This inevitably leads to simpler, flatter structures. Also unlike Minimalism, it makes
limited use of phonologically empty elements. For example, it is not assumed, as in Minimalism, that
because some clauses contain a complementiser they all do, an empty one if not an overt
one. Similarly, it is not assumed that because some languages like \ili{English} have determiners, they
all do, overt or covert. It is also not generally assumed that null subject sentences, such as
(\ref{ex:prop1b}) from \ili{Polish}, have a phonologically empty subject in their constituent
structure. Thus, the constituent structure of the two following sentences is quite different, even
if their semantics are similar:

\eal\label{ex:prop1}
\ex\label{ex:prop1a}
I read a book.
\ex\label{ex:prop1b}
\gll Czytałem książkę.\\
     read\textsc{.pst.1sg} book\textsc{.acc}\\\hfill(\ili{Polish})
\glt `I read a book.'
\zl

\noindent
It is also assumed in much HPSG work that there are no phonologically empty elements in the
constituent structure of an unbounded dependency construction such as the following:

\ea\label{ex:prop2}
What did you say?
\z

\noindent
On this view, the verb \emph{say} in (\ref{ex:prop2}) does not have an empty complement. There is,
however, some debate here (\citealp{SF94a,Mueller2004e};
\crossrefchapteralt[Section~\ref{sec:UDC:MoreOnGaps}]{udc}).

\largerpage[1.5]
A further important feature of HPSG is a rejection of the Chomskyan idea that grammatical phenomena
can be divided into a core, which merits serious investigation, and a periphery, which can be safely
ignored.\footnote{% 
  This is not to deny that some constructions are more canonical and more frequent in use than
  others and that this may be important in various ways.} 
%
This means that HPSG is not only concerned with such ``core'' phenomena as \emph{wh}-interrogatives,
relative clauses, and passives, but also with more ``peripheral'' phenomena such as the following: 


\eal\label{ex:prop3}
\ex\label{ex:prop3a}
It’s amazing the people you see here.
\ex\label{ex:prop3b}
The more I read, the more I understand.
\ex\label{ex:prop3c}
Chris lied his way into the meeting.
\zl

\noindent
These exemplify the nominal extraposition construction \citep{ML96a}, the comparative correlative
construction \citep{Abeille2006a,AB2008a-u,Borsley2011a-u}, and the \emph{X’s Way} construction
\citep[Section~7.4]{Sag2012a}. As we will see, HPSG is an approach which is able to accommodate
broad linguistic generalisations, highly idiosyncratic facts, and everything in between.\footnote{% 
  Idioms have also been an important focus of research in HPSG. See e.g.\
  \citew[Section~5.4]{Sag2007a}, \citew{RS2009a}, \citew{KM2019a}, and
  \crossrefchaptert{idioms}.} 
%

Another notable feature of the framework since the earliest work is a concern with semantics as well
as syntax. More generally, HPSG does not try to reduce either semantics or morphology to syntax (see
\crossrefchapteralt{morphology} on morphology in HPSG and \crossrefchapteralt{semantics} on
semantics). We will comment further on this in the following sections. 

\largerpage[1.5]
We turn now to some assumptions which are more about the conduct of linguistic research than the
nature of linguistic analyses. Firstly, HPSG emphasises the importance of firm empirical foundations
and detailed formal analyses of the kind advocated by Chomsky in \emph{Syntactic Structures}
\citep[\page 5]{Chomsky57a}. Whereas transformational work typically offers sketches of analyses
which might be fleshed out one day, HPSG commonly provides detailed analyses which can be set out in
an appendix. A notable example is \citet{GSag2000a-u}, which sets out its analysis of \ili{English}
interrogatives in a fifty-page appendix. Arguably, one can only be fully confident that a complex
analysis works if it is incorporated into a computer implementation. Hence, computer implementations
of HPSG analyses are also quite common (see e.g.\
\citealp{Babel,MuellerCoreGram,Copestake2002a,BDFPS2010a-u,Bender2016}, and
\crossrefchapteralp{cl}). 

Another property of the framework is a rejection of abstract analyses with tenuous links to the
observable data. As we noted above, phonologically empty elements are only assumed if there is
compelling evidence for them.% 
%
\footnote{There may be compelling evidence for some empty elements in some languages. For example,
  \citet[Section~8]{Borsley2009a-u} argues that \ili{Welsh} has phonologically empty\is{empty
    element} pronouns. For general discussion of empty elements, see
  \citet[Chapter~19.2]{MuellerGT-Eng1}.} 
%
Similarly, overt elements are only assumed to have properties for which there is clear evidence. For example, words are only assumed to have case or agreement features if there is some concrete morphological evidence for them, as in \ili{Polish}, illustrated in (\ref{ex:prop1b}). This feature of HPSG stems largely from considerations about acquisition (\citealt[Chapter~19]{MuellerGT-Eng1}; %\crossrefchaptert{acquisition}; 
\crossrefchapteralt[Section~\ref{sec-acquisition-minimalism}]{minimalism}). Every postulated element
or property for which there is no clear evidence in the data increases the complexity of the
acquisition task and hence necessitates more complex innate machinery. This suggests that such
elements and properties should be avoided as much as possible. It has important implications both
for the analysis of individual languages and for how differences between languages are viewed. 
	
A related property of the framework is a rejection of the idea that it is reasonable to assume that
a language has some element or property if some other languages do. Many languages have case and
many languages have agreement, but for HPSG, it does not follow that they all do. As
\citet[25]{MuellerCoreGram} puts it, ``Grammars should be motivated on a language-specific basis.''
Does this mean that other languages are irrelevant when one investigates a specific language?
Clearly not. As Müller also states, ``In situations where more than one analysis would be compatible
with a given dataset for language X, the evidence from language Y with similar constructs is most
welcome and can be used as evidence in favour of one of the two analyses for language X''
\citep[43]{MuellerCoreGram}.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Elements}\label{sec:prop3}
\label{prop:sec-elements}

\largerpage[2]
For HPSG, a linguistic analysis is a system of types\is{type} (or sorts\is{sort}),
features\is{feature}, and constraints\is{constraint}. Types provide a complex classification of
linguistic objects, features identify their basic properties, and constraints impose further
restrictions. In this section, we will explain these three elements. We note at the outset that HPSG
distinguishes between the linguistic objects (lexemes, words phrases, etc.) and descriptions of such
objects. Linguistic objects must have all the properties of their description and cannot be
underspecified in any way.\footnote{%
  As pointed out by \citet[Chapter~2]{ps}, HPSG grammars provide descriptions for models of linguistic
  objects rather than for linguistic objects per se. See also
    \crossrefchapterw{formal-background} for a detailed discussion of the formal background of HPSG.
} 
Descriptions, in contrast, can be underspecified and, in fact, always are.

There are many different kinds of types, but particularly important is the type \type{sign} and its
various subtypes. For \citet[19]{GSag2000a-u}, this type has the subtypes \type{lexical-sign} and
\type{phrase}, and \type{lexical-sign} has the subtypes \type{lexeme} and \type{word}. (Types are
written in lower case italics.) Thus, we have the \isi{type hierarchy} in Figure~\ref{fig:prop1}.


\begin{figure}
\begin{forest}
type hierarchy
[sign
	[lexical-sign
		[lexeme]
		[word]
	]
	[phrase]
]
\end{forest}
\caption{A hierarchy of types of signs}\label{fig:prop1}\label{fig-type-hierarchy-sign}
\end{figure}


\type{lexeme}, \type{word}, and \type{phrase} have a complex system of subtypes. The type
\type{lexical-sign}, its subtypes, and the constraints on them are central to the lexicon of a
language, while the type \type{phrase}, its subtypes, and the constraints on them are at the heart
of the syntax. In both cases, complex hierarchies mean that the framework is able to deal with
broad, general facts, very idiosyncratic facts, and facts somewhere in between. We will say more
about this below. 

Signs are obviously complex objects with (at least) phonological, syntactic, and semantic
properties. Hence, the type \type{sign} must have features that encode these properties. For much
work in HPSG, phonological properties are encoded as the value of a feature \textsc{phon(ology)},
whose value is a list of objects of type \type{phon}, while syntactic and semantic properties are
grouped together as the value of a feature \textsc{synsem}, whose value is an object of type
\type{synsem}. (Features or attributes are written in small caps.) A type has certain features
associated with it, and each feature has a value of some kind. A bundle of features can be
represented by an attribute"=value matrix (AVM) with the type name at the top on the left hand side
and the features below followed by their values. Thus, signs can be described as follows: 

\largerpage[2]
\ea\label{ex:prop4}
\avm{
	[\type*{sign}
	phon & \listOf{phon}\\
	synsem & synsem ]
}
\z

\noindent
The descriptions of specific signs will obviously have specific values for the two features. For
example, we might have the following simplified AVM for the phrase \emph{the cat}: 

\ea\label{ex:prop5}
\avm{
	[\type*{phrase}
	\phon  < the, cat >\\
	synsem & \upshape NP ]
}
\z

\noindent
Here, following a widespread practice, we use standard orthography instead of real \type{phon}
objects,%
\footnote{See \citet{BK94b}, \citet{Hoehle99a-u}, and \citet{Walther99a-u} for detailed approaches to phonology
and structured \phon values, and \crossrefchaptert{information-structure} and \crossrefchaptert[\page \pageref{coord:page-rnr-I-phi-start}--\pageref{coord:page-rnr-I-phi-end}]{coordination} for reference to structured \phon values.
% checked inf-struc: PHON is in Section 3 and 4
}
%
and we use the traditional label NP as an abbreviation for the relevant \type{synsem} object. We will say more about \type{synsem} objects shortly. First, however, we must say something about phrases.

A central feature of phrases is that they have internal constituents. More precisely, they have
daughters, i.e.\ immediate constituents, one of which may be the head. This information is encoded
by further features, for \citet[29]{GSag2000a-u} the features \textsc{daughters} (\textsc{dtrs}) and
\textsc{head-daughter} (\textsc{hd-dtr}). The value of the latter is a \type{sign}, and the value of
the former is a list of \type{signs}, which includes the value of the latter.\footnote{%
  Some HPSG work, e.g.\ \citet{Sag97a}, has a \textsc{head-daughter} feature and a
  \textsc{non-head-daughters} feature, and the value of the former is not part of the value of the
  latter.  

  The sign that is the value of \textsc{head-dtr} can be a word or a phrase. Within Minimalism, the
  term \term{head} is only applied to words. On this usage, the value of \textsc{head-dtr} is either
  the head or a phrase containing the head. But there are good reasons for not adopting this usage,
  for example the fact that the head can be an unheaded phrase: for example, a coordination (see
  \crossrefchapteralp[Section~\ref{coord:sec-headedness}]{coordination}). So we will say that the
  value of \textsc{hd-dtr} is the head. See \citet[30]{Jackendoff77a} for an early discussion of the
  term.}
%
Thus, phrases take the form in (\ref{ex:prop6a}), and headed phrases the form in (\ref{ex:prop6b}):

\begin{multicols}{2}
\eal\label{ex:prop6}
\ex\label{ex:prop6a}
	\avm{
		[\type*{phrase}
		phon & \listOf{phon}\\
		synsem & synsem\\
		dtrs & \listOf{sign} ]
	}

\columnbreak
\ex\label{ex:prop6b}
	\avm{
		[\type*{headed-phrase}
		phon & \listOf{phon}\\
		synsem & synsem\\
		dtrs & \listOf{sign}\\
		hd-dtr & sign ]
	}
\zl
\end{multicols}
%
To take a concrete example, the phrase \emph{the cat} might have the fuller AVM given in (\ref{ex:prop7}).

\ea\label{ex:prop7}
\avm{
	[\type*{phrase}
	\phon  < the, cat >\\
	synsem & \textnormal{NP}\\
	dtrs &	<[ \phon  < the >\\
	           synsem & \textnormal{Det}],
%			
		   \1[\phon  < cat >\\
		      synsem & \textnormal{N} ] >\\
	hd-dtr & \1 ]
}
\z

\noindent
Here, the two instances of the tag \ibox{1} indicate that the \type{sign} which is the
second member of the \textsc{dtrs} list is also the value of \textsc{hd-dtr}. Thus, the word
\emph{cat} is the head of the phrase \emph{the cat}. An object occupying more than one position in a
representation, either as a feature value or as part of a feature value (a member of a list or set),
for example \ibox{1} in (\ref{ex:prop7}), is known as re-entrancy or structure sharing. As we will
see below, it is a pervasive feature of HPSG. 

Most HPSG work on morphology has assumed a realizational approach, in which there are no morphemes
(see \crossrefchapteralp{morphology}). Hence, words do not have internal structures in the way that
phrases do. However, it is widely assumed that lexemes and words that are derived through a lexical
rule have the lexeme from which they are derived as a daughter (see \citealt{BC99a,Meurers2001a} and
Section~\ref{prop:sec-lex-rules} below). Hence, the \textsc{dtrs} feature is relevant to words as
well as phrases. 

AVMs like (\ref{ex:prop7}) can be quite hard to look at. Hence, it is common to use traditional tree
diagrams instead. Thus, we might have the tree-like representation in Figure~\ref{fig:prop2} instead
of (\ref{ex:prop7}). But one should bear in mind that AVMs correspond to (rooted) graphs and provide
more detailed descriptions than traditional phrase structure trees, with richer node and edge
labels, and with shared feature values between nodes. Thus, at each node, all kinds of information
are available: not just syntax but also \isi{phonology}, \isi{semantics}, and \isi{information structure}.\footnote{%
 This differs from Lexical Functional Grammar\indexlfg, for instance, which distributes the
 information between different kinds of structures (see \crossrefchapteralp{lfg}).}
%

\begin{figure}
\begin{forest}
sm edges
[NP
	[Det
		[the]
	]
	[N, edge label={node[midway,right]{~\textsc{hd-dtr}}}
		[cat]
	]
]
\end{forest}
	
\caption{A simple tree for \emph{the cat}}\label{fig:prop2}
\end{figure}

If the head is either obvious or unimportant, the \textsc{hd-dtr} annotation might be omitted. This
is a convenient informal notation, but it is important to remember that it is just that and has no
status within the theory. 

We return now to \type{synsem} objects. Standardly, these have two features: \textsc{local}, whose
value is a \type{local} object, and \textsc{nonlocal}, which we will deal with in
Section~\ref{prop:sec-syntax}. A \type{local} object has the features \textsc{cat(egory)} and
\textsc{cont(ent)}, whose values are objects of type \type{category} and \type{content},
respectively, and the feature \textsc{context}.% 
%
\footnote{Words also have a \textsc{morph} (or \textsc{infl}) attribute that we ignore here (see
  \crossrefchapteralp{morphology}).
}
%
In much work, a \type{category} object has the features, \textsc{head}, \textsc{subj}, and
\textsc{comp(lement)s}. \textsc{head} takes as its value a \type{part-of-speech} object, while
\textsc{subj} and \textsc{comps} have a list of \type{synsem} objects as their value. The former
indicates what sort of subject a sign requires, and the latter indicates what complements it
takes. In both cases, the value is the empty list if nothing is required.  It is generally assumed
that the \textsc{subj} list never has more than one member. \textsc{subj} and \textsc{comps} are
often called \type{valence}\is{valence} features. Thus, the following AVM provides a fuller representation of signs:

\ea\label{ex:prop8}
\avm{
	[\type*{sign}
	phon & \listOf{phon}\\
	synsem &	[\type*{synsem}
				local &	[\type*{local}
						category &	[\type*{category}
									head & part-of-speech\\
									subj & \listOf{synsem}\\
									comps & \listOf{synsem}]\\
						content & \ldots\\
						context & \ldots]\\
				nonlocal \ldots]
	]
}
\z

\noindent
The type \type{part-of-speech} has subtypes such as \type{noun}, \type{verb}, and \type{adjective}. In other words, we have a type hierarchy of the form given in Figure~\ref{fig:prop3}.

\begin{figure}
\begin{forest}
type hierarchy
[part-of-speech
	[noun]
	[verb]
	[adjective]
	[\ldots]
]
\end{forest}	
\caption{A hierarchy for part of speech}\label{fig:prop3}
\end{figure}

The type hierarchy in Figure~\ref{fig-type-hierarchy-sign} can be viewed as an ontology of possible objects in the language. A particular word or phrase must instantiate one of the maximal (most specific) types and have the properties specified for it and all its supertypes.%
%
\footnote{AVMs associated with types used to be combined by unification \citep[Chapter~2]{ps}. See
  \crossrefchaptert[\page \pageref{formal:page-unification-start}--\pageref{formal:page-unification-end}]{formal-background} for
  discussion of the term ``unification''.}
%
We might have a \type{synsem} object of the following form for the phrase \emph{the cat}:

\ea\label{ex:prop9}
\avm{
	[\type*{synsem}
	local &	[\type*{local}
			category &	[\type*{category}
						head & noun\\
						subj & <>\\
						comps & <>]\\
			content & \ldots\\
			context & \ldots]\\
	nonlocal \ldots]
}
\z

\largerpage
\noindent
This ignores a number of matters including the value of \textsc{content, context}, and
\textsc{nonlocal}. It also ignores the fact that the type \type{noun} will have certain features,
for example \textsc{case}, but it highlights some important aspects of HPSG analyses. Notice that
(\ref{ex:prop9}) is compatible with the \textsc{synsem} feature in (\ref{ex:prop8}): it contains
more specific information, such as [\textsc{head}~\type{noun}], but no conflicting information:
$\langle \rangle$ is the empty list and is compatible with \type{list}(\type{synsem}).

Rather different from most of the features mentioned above are fairly traditional features like
\textsc{person, number, gender}, and \textsc{case}. In most HPSG work, these have as their value an
atomic type\istype{atomic}: a type with no features. A simple treatment of person might have the
types \type{first}, \type{second}, and \type{third}, and a simple treatment of number the types
\type{sg} (\type{singular}) and \type{pl} (\type{plural}).%
%
\footnote{In practice, a more complex system of values may well be appropriate \citep[Section~3]{Flickinger2000a}.}
%
There are also Boolean features with $+$ and $-$ as their values. An example is \textsc{aux}, used
to distinguish auxiliary verbs ([\textsc{aux}~$+$]) from non-auxiliary verbs ([\textsc{aux}~$-$]).% 
%
\footnote{In some recent work, e.g.\ \citet[157--162]{Sag2012a} and \citet{Sag2020a}, the feature is
  used to distinguish positions that only allow an auxiliary from positions that allow any
  verb. Within this approach, auxiliaries (except support \emph{do}) are unspecified for
  \textsc{aux}, since they may appear in both [\textsc{aux}~$+$] and [\textsc{aux}~$–$]
  constructions. Non-auxiliary verbs are [\textsc{aux}~$–$]; see
  \crossrefchaptert[Section~\ref{sec-auxiliaries-as-raising-verbs}]{control-raising}.} 
%

As the preceding discussion makes clear, features in HPSG can have a number of kinds of value. They
may have an atomic type (\textsc{person}, \textsc{number}, \textsc{gender}, \textsc{case},
\textsc{aux}), a feature structure (\synsem, \local, \textsc{category}, etc.), or a list of some
kind (\subj, \comps).% 
%
\footnote{A list can be represented as a feature description with the features \textsc{first} and
  \textsc{rest}, where the value of \textsc{first} is the first element of the list. See
  \crossrefchapterw[\page \pageref{page-list-encoding}]{formal-background} for more on the encoding
  of lists.} 
%
As we will see in Section~\ref{sec:prop5}, HPSG also assumes features with a set as their value.

\largerpage
{\sloppypar
The \textsc{content} feature, whose value is a \type{content} object, highlights the importance of
semantics within HPSG. But what exactly is a \type{content} object? Different views of semantics
have been taken within the HPSG literature. Much HPSG work has assumed some version of Situation
Semantics\index{Situation Semantics} \citep{BP83a}. But some work has employed so-called Minimal
Recursion Semantics\indexmrs \citep*{CFPS2005a}, while others use Lexical Resource
Semantics\indexlrs \citep{RS2004a-u}. \citet[501]{Sag2010b} adopts a
conventional, Montague-style possible-worlds semantics \citep{Montague74a-ed} in his analysis of
\ili{English} filler-gap constructions, and SBCG\indexsbcg (Section~\ref{sec:prop7.2}) has generally employed a
version of Frame Semantics. See \crossrefchaptert{semantics} for a discussion of the issues.
}

Finally, the \textsc{context} feature is used for information structure, deixis, and, more generally, pragmatics (see \crossrefchapteralp{information-structure}).

We will say more about types and features in the following sections. We turn now to constraints\is{constraint!implicational}. These are the machinery which imposes conditions on linguistic objects by saying that if an object has some property or properties, it must have some other property or properties. Constraints take the following form:%
%
\footnote{The double-shafted arrow \isi{\impl} is used in implicational constraints, and a single shafted arrow \isi{$\mapsto$} in lexical rules.}
%

\ea\label{ex:prop10}
X \impl Y
\z

\noindent
Commonly, X is a type and Y a feature description, and this is the case in all the constraints that
we discuss below. However, X may also be a feature description with or without an associated
type. This is necessary, for example, in the constraints that constitute Binding Theory (see
\crossrefchapteralt{binding}). Here is a very simple constraint: 

\ea
\label{ex:prop11}
\label{ex-phrase-coonstraint}
\type{phrase} \impl [\textsc{comps} $\langle \rangle$]
\z

\noindent
This says that a phrase has the empty list ($\langle \rangle$) as the value of the \textsc{comps}
feature, which means that it does not require any complements.% 
%
\footnote{The constraint in (\ref{ex:prop11}) is plausible for \ili{English}, but it is too strong
  for some languages, especially for languages with complex predicates or partial VPs (see
  \crossrefchapteralp{complex-predicates}), and also for SOV languages if they are analysed in terms
  of binary branching (see \crossrefchapteralp{order}).} 
%
As we will see below, most constraints are more complex than (\ref{ex:prop11}) and impose a number
of restrictions on certain objects. For this reason, one might speak of a set of
constraints. However, we will continue to use the term ``constraint'' for objects of the form in
(\ref{ex:prop10}), no matter how many restrictions are imposed. Particularly important are
constraints dealing with the internal structure of various types of phrases. We will consider some
constraints of this kind in Section~\ref{sec:prop5}. 

In most HPSG work, some shortcuts are used to abbreviate a feature path; for example, in
(\ref{ex:prop11}), \textsc{comps} stands for \textsc{synsem|loc|cat|comps}. We use this practice in
the rest of the chapter, and it is used throughout the Handbook. 


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{The lexicon}\label{sec:prop4}
\label{prop:sec-lexicon}

As noted above, the type \type{lexical-sign}, its subtypes, and the constraints on them are central
to the lexicon of a language and the words it licenses.% 
%
\footnote{\label{fn-constraints-synsem-phon}%
Other types of constraint are relevant to the form of lexemes and words, e.g.\ constraints on
\type{synsem} objects and on \textsc{phon} values. These are also relevant to the form of phrases.} 
%
Lexical rules are also important. Some of the earliest work in HPSG focused on the organisation of
the lexicon and the question of how lexical generalisations can be captured, and detailed proposals
have been developed.% 
%
\footnote{The lexicon is more important in HPSG than in some other constructional approaches, e.g.\
  that of \citet{Goldberg95a,Goldberg2006a}. See \citet{MWArgSt} and
  \crossrefchaptert[Section~\ref{sec-valence-vs-phrasal-patterns}]{cxg} for discussion.} 
%

%%%%%%%%%%%%%%%%%%%%
\subsection{Lexemes and words}\label{sec:prop4.1}
\label{properties:lexemes-and-words}

In some frameworks, the lexicon contains not lexemes but morphemes, i.e.\ roots and affixes of
various kinds. But most work in HPSG has assumed a realizational approach to morphology. Within this
approach, there are no morphemes, just lexemes and the words that realise them, and affixes are just
bits of \isi{phonology} realising certain morphosyntactic features \citep{Stump2001a,Anderson92a-u}. One
consequence of this is that HPSG has no syntactic elements like the T(ense) and Num(ber) functional
heads of Minimalism\indexmp, which are mainly realised by affixes. See
\crossrefchapterw[Section~\ref{sec:inflection}]{morphology},
\crossrefchapterw[Section~\ref{sec:lex}]{lexicon}, and
\crossrefchapterw[Section~\ref{sec-morphology-minimalism}]{minimalism} for discussion. 

Probably the most important properties of any lexeme are its part of speech and its combinatorial
properties. As we saw in the last section, the \textsc{head} feature encodes part of speech
information, while the \textsc{subj} and \textsc{comps} features encode combinatorial
information. As we also noted in the last section, \textsc{head} takes as its value a
\type{part-of-speech} object, and the type \type{part-of-speech} has subtypes such as \type{noun},
\type{verb}, and \type{adjective}. At least some of the subtypes have certain features. For example,
in many languages, the type \type{noun} has the feature \textsc{case} with values like
\type{nom}(\type{inative}), \type{acc}(\type{usative}), and \type{gen}(\type{itive}). Thus, nominative pronouns like
\emph{I} might have a \type{part-of-speech} of the form in (\ref{ex:prop12}) as its \textsc{head}
value. 

\ea\label{ex:prop12}\istype{noun}\is{noun}
\avm{
	[\type*{noun}
	case & nom]
}
\z

\noindent
Similarly, in many languages, the type verb has the feature \textsc{vform} with values like
\type{fin}(\type{ite}) and \type{inf}(\type{initive}). Thus, the \headv
%\type{part-of-speech} This is a type not a value.
of the word form \emph{be} might be (\ref{ex:prop13}).

\ea\label{ex:prop13}\istype{verb}\is{verb}
\avm{
	[\type*{verb}
	vform & inf]
}
\z

\noindent
In much the same way, the type \type{adjective} might have a feature distinguishing between positive, comparative, and superlative forms, in \ili{English} and many other languages.

We must now say more about combinatorial properties. In much HPSG work, it is assumed that \textsc{subj} and \textsc{comps} encode what might be regarded as superficial combinatorial information and that more basic combinatorial information is encoded by a feature \textsc{arg(ument)-st(ructure)}.%
%
\footnote{\textsc{arg-st} is also crucial for Binding Theory, which takes the form of a number of constraints on \textsc{arg-st} lists. See \crossrefchapterw{binding}.}
%
Normally the value of \textsc{arg-st} of a word is the concatenation of the values of \textsc{subj} and \textsc{comps}, using \isi{$\oplus$} for list concatenation. In other words, we normally have the following situation (notice the use of re-entrancy or structure sharing):

\ea\label{ex:prop14}
\avm{
	[subj & \1\\
	comps & \2\\
	arg-st & \1 \+ \2]
}
\z

\noindent
As noted earlier, it is generally assumed that the \textsc{subj} list never has more than one member. The appropriate features for the word \emph{read} in (\ref{ex:prop1a}), for example, would include the following, where the tags identify not lists but list members:

\eas
\label{ex:prop15}%
Lexical item for \emph{read}:\\
\avm{
[subj   & < \1 >\\
 comps  & < \2 >\\
 arg-st & < \1\,NP, \2\,NP >]
}
\zs

\noindent
Under some circumstances, however, we have something different. For example, it has been proposed, e.g.\ in \citet[65]{MS99a}, that null subject sentences have an element representing the understood subject in the \textsc{arg-st} list of the main verb but nothing in the \textsc{subj} list. Thus, the verb \emph{czytałem} `read' in (\ref{ex:prop1b}), repeated here as (\ref{ex:prop16}), has the features in (\ref{ex:prop17}).

\ea\label{ex:prop16}
\gll Czytałem książkę.\\
read.\textsc{pst.1sg} book.\textsc{acc}\\\hfill(\ili{Polish})
\glt `I read a book.'
\z
\ea\label{ex:prop17}
Lexical item for \emph{czytałem} `read' with the subject dropped:\\
\avm{
[subj   & <>\\
 comps  & < \1 >\\
 arg-st & < NP, \1\,NP >]
}
\z

\noindent
A similar analysis is widely assumed for unbounded dependency gaps. On this analysis, the verb \emph{say} in (\ref{ex:prop2}), repeated here as (\ref{ex:prop18}), has the features in (\ref{ex:prop19}).

\ea\label{ex:prop18}
What did you say?

\ex\label{ex:prop19}
Lexical item for \emph{say} with the object extracted:\\
\avm{
[subj   & < \1\,NP >\\
 comps  & <>\\
 arg-st & < \1\,NP, NP >]
}
\z

\noindent
It is also assumed that the arguments that are realised as pronominal affixes (traditionally known
as clitics\is{clitic} in \ili{Romance} languages) are absent from \textsc{comps} lists
\parencites[Section~3]{MS97a-u}{monachesi05},
% monachesi as a whole relevant
and other differences between \textsc{subj}, \textsc{comps}, and \textsc{arg-st} have been
proposed for other languages (see \citealt{MS99a},
%whole paper 
\crossrefchapteralt[Section~\ref{argst-valence-sec}]{arg-st} for
discussion). In much work, the relation between \textsc{arg-st}, \textsc{subj}, and \textsc{comps}
is regulated by a constraint called the Argument Realisation Principle (ARP)\is{principle!Argument
  Realization (ARP)}.\label{page-argument-realization-principle} The following is a simplified
version of the constraint proposed in \citeauthor{GSag2000a-u} (\citeyear[171]{GSag2000a-u}; see also \citealt[12]{BMS2001a}): 

\ea\label{ex:prop20}\label{properties:ex-ARP}
\type{word} \impl
\avm{
[subj   & \1\\
 comps  & \2 \- \listOf{non-canonical}\\
 arg-st & \1 \+ \2]
}
%\itdopt{This allows non-cannonicals in \ibox{1}. Not sure this is what is intended.} 
\z

\noindent
This ensures that non-canonical arguments, including gaps and arguments realised as affixes, do not appear in \textsc{comps} lists.%
%
\footnote{\label{fn-shuffle}As we saw above, the sign $\oplus$ means concatenation of lists. \citet[\page 170]{GSag2000a-u} state the
  following about $\ominus$\is{$\ominus$}: ``Here `$\ominus$' designates a relation of contained list difference. If
$\lambda_2$ is an ordering of a set $\sigma_2$ and $\lambda_1$ is a subordering of $\lambda_2$, then
$\lambda_2 \ominus \lambda_1$ designates the list that results from removing all members of
$\lambda_1$ from $\lambda_2$; if $\lambda_1$ is not a sublist of $\lambda_2$, then the contained
list difference is not defined. For present purposes, $\ominus$ is interdefinable with the sequence
union operator ($\bigcirc$\is{$\bigcirc$}\isrel{shuffle}) of
\citet{Reape94a} and Kathol (1995): $(A \ominus B = C) \Leftrightarrow (C \bigcirc B = A)$.'' The
operator $\bigcirc$ is called \emph{shuffle} and is also explained in \crossrefchapterw[\page \pageref{rel-shuffle}]{order}.}
%
Notice, however, that it says nothing special about subjects.%
%
\footnote{\citet[177--183]{GSag2000a-u} explicitly allow gaps in \textsc{subj} lists, but this is
  controversial, as discussed in \crossrefchapterw[\page
  \pageref{udc:page-subject-gaps-start}--\pageref{udc:page-subject-gaps-end}]{udc}.}
%
There are complex issues here, and the principle will probably take a different form in different languages. So we will not try decide exactly what form it should take.

A variety of HPSG work assumes the \textsc{subj} and \textsc{comps} features, but some work assumes
a \textsc{spr (specifier)} feature instead of, or in addition to, the \textsc{subj} feature. Where it
replaces \textsc{subj}, the idea is that subjects are one of a number of types of specifiers, others
being determiners within NPs and degree words like \emph{so} and \emph{too} within APs
\citep[\page 358]{ps2}. Where it is an additional feature, the idea is that there are a number of
types of specifier, but subjects are not specifiers. Predicative nominals (e.g.\ \emph{my cousin} in
\emph{Paul is my cousin}) may need both
\parencites[Section~9.4.1]{ps2}[409]{GSag2000a-u}{AG2003b-u}. There are other positions in the HPSG 
community. Much early work has a single feature called \textsc{subcat}
instead of \textsc{subj} and \textsc{comps} \citep{ps}. Essentially the same position has been
adopted within Sign-Based Construction Grammar, which has a single feature called \textsc{valence}
instead of \textsc{subj}, \textsc{spr}, and \textsc{comps}.\footnote{%
  SBCG\indexsbcg also has a feature \textsc{x-arg}, which picks out subjects and other external arguments. But
  unlike the other features mentioned here, this always has the same value in a head and its
  mother. Its role is to make information about external arguments available outside the phrases in
  which they appear.  See \citet[84, 149--151]{Sag2007a,Sag2012a}.} 
%
Obviously, there are some important issues here.

It is a central feature of lexical items that part of speech and combinatorial properties are
separate matters. Members of the same part of speech can have different combinatorial properties, and
members of different parts of speech can have the same combinatorial properties. Much HPSG work
captures this fact by proposing that the type \type{lexeme}\is{type hierarchy!lexical}\istype{lexeme} be cross-classified along two
dimensions, one dealing with part of speech information and one dealing with argument selection
information \citep[\page 20]{Flickinger87}. Figure~\ref{fig:prop4} is a simple illustration based on \citet[20]{GSag2000a-u}.

\begin{figure}
\begin{forest}
type hierarchy
[lexeme
	[part-of-speech,partition
		[v-lx,name=v]
		[\ldots]
		[\ldots]
		[\ldots]
	]
	[arg-selection,partition
		[intr-lx
			[s-rsg-lx
				[srv-lx,name=srv]
				[,phantom]
			]
			[\ldots]
			[\ldots]
		]
		[\ldots]
	]
]
{
	\draw (v.south) to (srv.north);
}
\end{forest}
\caption{Cross-classification of lexemes}\label{fig:prop4}\label{properties:page-cross-classification-of-lexemes}
\end{figure}

Upper case letters are used for the two dimensions of classification, and \type{v-lx}, \type{intr-lx}, \type{s-rsg-lx}, and \type{srv-lx} abbreviate \type{verb-lexeme, intransitive-lexeme, subject-raising-lexeme}, and \type{subject-raising-verb-lexeme}, respectively. All these types will be subject to specific constraints. For example, \type{v-lx} will be subject to something like the following constraint, based on that in \citet[22]{GSag2000a-u}:

\ea\label{ex:prop21}
\type{v-lx} \impl
\avm{
[head & verb\\
 arg-st & < XP, \ldots >]
}
\z

\largerpage
\noindent
This says that a verb lexeme has a \type{verb} part of speech and requires a phrase of some kind as its first (syntactic) argument (corresponding to its subject). Similarly, we will have something like the following constraint for \type{s-rsg-lx}:

\ea\label{ex:prop22}
\type{s-rsg-lx} \impl
\avm{
	[arg-st & <\1, [subj & <\1>], \ldots>]
}
\z
%
%\noindent
This says that a subject-raising-lexeme has (at least) two (syntactic) arguments, a subject and a complement, and that the subject is whatever the complement requires as a subject, indicated by \ibox{1}. Most of the properties of any lexeme will be inherited from its supertypes. Thus, very little information needs to be listed for each specific lexeme, and the richness of the lexical description comes from the classification in a system like this.

For example, for a subject-raising verb like \emph{seem}, its \textsc{cat} and \textsc{content}
features are the following, using a simplified version of Minimal Recursion Semantics\indexmrs (MRS; \citealp{CFPS2005a}): \feat{rel(ation)s} is the
attribute for the list of elementary predications associated with a word, a lexeme, or a phrase, and
\attrib{soa} is for \term{state-of-affairs} (see \crossrefchapteralp{semantics}). 
\emph{Seem} takes an infinitival VP complement.%
%
\footnote{The entry can be modified to allow predicative complements, as well as a second \emph{to} complement (\emph{John seems tired/in a good mood to me}).}
%
Notice that the first syntactic argument (the subject) is not mentioned in the
  \textsc{content}, i.e.\ it is not assigned a semantic role by \emph{seem} (see
\crossrefchapteralp[Section~\ref{control:sec-distinction-control-raising}]{control-raising}).

%\eas
\ea
Constraints on type \type{seem-lx} in addition to those inherited from \type{srv-lx}:\label{ex:prop23}\\
%\type{seem-lx} \impl \deleted{\type{s-rsg-lx} \&}\\
\type{seem-lx} \impl\\
\avm{
[cat  & [arg-st & < ![]!, VP[head  & [vform & inf]\\
	                     index & \1]>]\\
 cont & [ %index & s\\
          rels  & < [\type*{seem-rel}
	             soa & \1] > ]]
}
%\zs
\z
%
\largerpage[1.2]
%\noindent
Once these more specific features are combined with features from the type \type{srv-lx}, we get a
more complete AVM like the following for the word \emph{seem}:
%\inlinetodostefan{Stefan: These were formulated as type implication, which is not correct, since the type implication is in (\ref{ex:prop23}). I removed the ``\type{seem-lx} \impl'' part, put the type in the AVM and added a heading. OK?}
\ea\label{ex:prop24}
%\type{seem-lx} \impl\\
Constraints for the lexeme \emph{seem}:\\
\avm{
[\type*{seem-lx}
  cat &	[head  & verb\\
         subj  & <\1>\\
	 comps & <\2>\\
         arg-st & <\1, \2 VP[head & [vform & inf]\\
			             subj & <\1>\\
				     index & \3]> ]\\
%			
  cont & [ %index & s\\
           rels  & < [\type*{seem-rel}
                      soa & \3 ] > ]]
}
\z

\noindent
Notice that the \textsc{subj} value is underspecified. Thus, \emph{seem} combines with an
infinitival complement and with any subject (nominal or verbal, expletive or referential), provided
this subject is appropriate for the infinitival complement (see
\crossrefchapteralp[Section~\ref{control:sec-HPSG-anaylsis-of-raising}]{control-raising}): 

\eal\label{ex:prop25}
\ex[]{
Kim is/seems to be sleeping.
}
\ex[*]{
Kim is/seems to be snowing.
}
\ex[]{
That he is clever is/seems to be obvious.
}
\ex[*]{
That he is clever is/seems to be obese.
}
\ex[]{
There is/seems to be a problem.
}
\ex[*]{
There is/seems to be in Paris.
}
\zl


%%%%%%%%%%%%%%%%%%%%
\subsection{Lexical rules}\label{sec:prop4.2}
\label{prop:sec-lex-rules}

The hierarchy of lexical types provides one way of capturing lexical generalisations. Lexical rules provide another.%
%
\footnote{Lexical rules can be seen as a generative device, or alternatively, as a set of
  well-formedness conditions on the lexicon: if the lexicon contains items with description $x$, it
  must also contain items with description $y$ \citep{Meurers2001a}. See also \crossrefchapterw[Section~\ref{lexicon-sec-lexical-rules}]{lexicon}.}
%
They are used in morphology to relate lexemes to words (inflection) and lexemes to lexemes
(derivation) (see \crossrefchapteralp[Section~\ref{sec:derivational-morphology}, \ref{sec:inflection}]{morphology}). For syntax, they are relevant especially to valence alternations such as that illustrated in the following (see \crossrefchapteralp[Section~\ref{arg-st:sec-passives}]{arg-st}):

\eal\label{ex:prop26}
\ex That Kim was late annoyed Lee.
\ex That Sandy was there is unimportant. 
\ex That Lee won impressed everyone.
\zl
	
\eal\label{ex:prop27}
\ex It annoyed Lee that Kim was late.
\ex It is unimportant that Sandy was there. 
\ex It impressed everyone that Lee won.
\zl

\noindent
These show that verbs and adjectives which allow a clausal subject generally also allow an expletive \emph{it} subject and a clause as an extra complement \citep[150]{ps2}. The lexemes required for the latter use can be derived from the lexemes required for the former use by a lexical rule of the following form:%
%
\footnote{Another representation of lexical rules is an AVM with features \textsc{input} and \textsc{output}, or with the left hand side as a daughter. As for (\ref{ex:prop27}), assuming that both clauses and VPs have a verbal head, it easily extends to infinitival subjects, to accommodate pairs of examples like the following:
	\eal
        \ex To annoy Lee is easy.
	\ex It is easy to annoy Lee.
	\zl
Clauses introduced by \emph{that} are sometimes considered as CPs in HPSG (see Section~\ref{sec:prop7}), with verbs and complementisers as two subtypes of \type{verbal}.
}
%

\ea\label{ex:prop28}\is{$\mapsto$}
% avm todo, I used \sliste to lower the S, which comes out too high otherwise
\avm[stretch=.5]{[arg-st & \sliste{S} \+ \1]} $\mapsto$
	\avm[stretch=.5]{[arg-st & <NP![\type{it}]!>  \+ \1 \+ \sliste{S}]}
\z

\noindent
The active-passive relation can be captured by a similar lexical rule
\citep[Section~5.1.1]{Flickinger87}. Since these rules do not change the \textsc{content} feature,
these alternations will preserve the meaning of the verb or adjective lexeme (see
\crossrefchapteralp{lexicon}). Thus, the sentences in (\ref{ex:prop27}) will have a different
syntactic structure from their counterparts in (\ref{ex:prop26}), but may have the same semantic
representation (they will probably have different information structures, thus different
\textsc{context} features; see \crossrefchapterw{information-structure} on information structure).


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Syntax}\label{sec:prop5}
\label{prop:sec-syntax}

As noted above, the type \type{phrase}, its subtypes, and the constraints on them are at the heart of the syntax of a language.%
%
\footnote{As noted in Footnote~\ref{fn-constraints-synsem-phon}, constraints on \type{synsem} objects and \textsc{phon} values are relevant to phrases as they are to lexemes and words.}
%
A simple hierarchy of phrase types was assumed in early HPSG, but what we have called Constructional HPSG employs complex hierarchies of phrase types comparable to the complex hierarchies of lexical types employed in the lexicon.

%%%%%%%%%%%%%%%%%%%%
\subsection{A hierarchy of phrase types}\label{sec:prop5.1}
\label{properties:sec-hierarchy-of-phrase-types}

Like much other work in syntax, HPSG takes from X-bar theory \citep{Jackendoff77a} the idea that the
local trees that make up syntactic structures fall into a limited number of types. Like
\citet{Jackendoff77a}, and unlike Minimalism\indexmp, HPSG assumes that not all phrases are headed,
even if many are, and does not limit the term \term{head} to lexical elements. Thus, among phrases
there is a basic distinction between non-headed phrases and headed phrases. There are various kinds
of headed phrase. We will consider three here. First there are head-complement phrases: combinations
of a head and its complements. These can be headed by various parts of speech -- verbs,
prepositions, adjectives, nouns, and others -- and may have one complement or more than one. Next,
there are head"=subject phrases. Typically, the head of such a phrase is a VP. However, the
bracketed material in the following may well be head-subject phrases with a non-verbal head.  

\ea\label{ex:prop29}
With [Kim ill/in London/a candidate], anything is possible.
\z

\noindent
Finally, there are head-filler phrases: clauses in which an initial constituent is associated with a
gap in the following constituent. \emph{Wh}-interrogatives and \emph{wh}-relatives, such as the
bracketed material in the following, are typical examples. 

\eal\label{ex:prop30}
\ex I’m wondering [who I talked to].
\ex This is the official [who I talked to].
\zl

\noindent
All this suggests the simple type hierarchy in Figure~\ref{fig:prop5}. Each of these types is
associated with a constraint capturing its distinctive properties. 
\begin{figure}
\begin{forest}
type hierarchy
[phrase
	[non-headed-ph]
	[headed-ph
		[hd-comp-ph]
		[hd-subj-ph,calign with current]
		[hd-filler-ph]
	]
]
\end{forest}
\caption{A hierarchy of types of phrases}\label{fig:prop5}\label{prop:fig-type-hierarchy-phrases}
\end{figure}


Consider first the type \type{headed-ph}. Here we need a constraint capturing what all headed
phrases have in common. This is essentially that they have a head, with which they share certain
features. But what features? One view is that the main features that are shared are those that are
the value of \textsc{head}. This is embodied in the following constraint, which is known as the Head
Feature Principle\is{principle!Head Feature}:\footnote{
  \head here is an abbreviation for \textsc{synsem|loc|cat|head}. In later implicational constraints, we abbreviate
  \textsc{synsem|loc|cat|comps} as \comps and \textsc{synsem|loc|cat|subj} as \subj.
}

\ea\label{ex:prop31}\label{page-hfp}
\type{headed-ph} \impl
\avm{
[\punk{head}{ \1}\\
 head-dtr & [head & \1]]
}
\z

\noindent
Each of the three subtypes of \type{headed-ph} is subject to a constraint embodying its distinctive
properties. Here is a constraint on the type \type{hd-comp-ph}\istype{head-complement-phrase} (with \textsc{synsem} abbreviated as
\textsc{ss}):\is{schema!Head-Complements} 

\ea\label{ex:prop32}
\type{hd-comp-ph} \impl
\avm{
	[hd-dtr & \1	[\type*{word}
			 comps & <\2, \ldots, \tag{n}>]\\
	dtrs & <\1, [ss & \2], \ldots, [ss & \tag{n}]>]
}
\z

\noindent
This ensures that a head-complement phrase has a word as a head daughter and non-head daughters with the \type{synsem} properties that appear in the head’s \textsc{comps} list.%
%
\footnote{The head could be identified as a [\textsc{lex}~+], [\textsc{light}~+], or [\textsc{weight}~\type{light}] phrase, to accommodate coordination of heads as in \emph{John} [\emph{knows and likes}] \emph{this record} \citep[Section~5.1]{Abeille2006a}.}
%
Notice that nothing is said about the \textsc{synsem} value of the phrase. It will be [\textsc{comps}~\eliste], as required by the constraint in (\ref{ex-phrase-coonstraint}), and it will have the same value for \textsc{head} as the head daughter as a consequence of the Head Feature Principle. It must also have the same value for \textsc{subj} as the head daughter. One might add this to the constraint in (\ref{ex:prop32}), but that would miss a generalisation. Head-complement phrases are not the only phrases which have the same value for \textsc{subj} as their head. This is also a feature of head-filler phrases, as we will see below. It seems, in fact, that it is normal for a phrase to have the same value for any valence feature as its head. This is often attributed to the Valence Principle\is{principle!Valence}, which can be stated informally as follows (cf.\ \citealp[\page 86]{SagW99a-u}):

\eanoraggedright
\label{ex:prop33}
\label{prop:valence-principle}
Unless some constraint says otherwise, the mother's values for the valence features are identical to those of the head daughter.
% SWB does not have capitalization of Valence and Head, but refer to the VAL features.
\z

\noindent
There is no assumption in HPSG that all branching is binary.\footnote{%
However, binary branching has been assumed in HPSG grammars for a number of languages. See \crossrefchaptert[Section~\ref{sec-binary-flat}]{order}.}
%
Hence, where a head takes two complements, both may be its sisters. An example of the sort of structures that the analysis licenses is illustrated in Figure~\ref{fig:prop6}.

\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-comp-ph}
	head & \1 verb\\
	subj & \2 <NP>\\
	comps & <>]
},l sep=1cm
	[\avm{
		[\type*{word}
		head & \1\\
		subj & \2\\
		comps & <\3, \4>]
	}, edge label={node[midway,left]{\textsc{hd-dtr}~~~~}}
		[give]
	]
	[\ibox{3} NP
		[some money,roof]
	]
	[\ibox{4} PP
		[to charity,roof]
	]
]
\end{forest}
\caption{A tree for a head-complement phrase}\label{fig:prop6}
\end{figure}

Instead of the Head Feature Principle and the Valence Principle, \citet[33]{GSag2000a-u} propose the
Generalised Head Feature Principle\is{principle!Head Feature!Generalized}, which takes the following form:

\ea\label{ex:prop34}\label{properties:ex-generalized-head-feature-principle}
\type{headed-ph} \impl
\avm{
	[synsem & / \1\\
	hd-dtr & [synsem & / \1]]
}
\z

\noindent
The slashes (/) here indicate that this is a default constraint \citep{LC99a}. Thus, it says that a
headed phrase and its head daughter have the same \textsc{synsem} value unless some other constraint
requires something different. In versions of HPSG which assume this constraint, it is responsible
for the fact that a head-complement phrase has the same value for \textsc{subj} as the head
daughter, among many other things. 

We turn now to the type \type{hd-subj-ph}\istype{head-subject-phrase}. Here we need a constraint which mentions the
\textsc{synsem} value of the phrase -- more precisely, its \subj value -- and not just the
daughters, as follows: 

\ea\label{ex:prop35}\label{ex-head-subj-phrase}
\is{schema!Subject Head}
\type{hd-subj-ph} \impl
\avm{
	[subj & <>\\
	hd-dtr & \1	[ subj & <\2>\\
                          comps & <>]\\
	dtrs & <[synsem \2], \1>]
}
\z

\noindent
This ensures that a head-subject phrase is [\textsc{subj}~\eliste] and has a head daughter which is [\textsc{comps}~\eliste] and a non-head daughter with the \type{synsem} properties that appear in the head’s \textsc{subj} list.%
%
\footnote{Instead of requiring the head to be [\textsc{comps}~\eliste], one might require it to be a phrase (which would be required by (\ref{ex:prop11}) to be [\textsc{comps}~\eliste]). However, this would require e.g.\ \emph{laughed} in \emph{Kim laughed} to be analysed as a phrase consisting of a single word. With (\ref{ex-head-subj-phrase}), it can be analysed as just a word.}
%
It licenses structures like that in Figure~\ref{fig:prop7}.

\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-subj-ph}
	head & \1 verb\\
	subj & <>\\
	comps & <>]
},l sep=1cm
	[\ibox{2} NP
		[They]
	]
	[\avm{
		[\type*{hd-comp-ph}
		head & \1\\
		subj & <\2>\\
		comps & <>]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}},l sep=1cm
		[give some money to charity,roof]
	]
]
\end{forest}
\caption{A tree for a head-subject phrase}\label{fig:prop7}
\end{figure}

Finally, we consider the type \type{hd-filler-ph}\istype{head-filler-phrase}. This involves the feature \textsc{slash}, one of
the features contained in the value of the feature \textsc{nonlocal} introduced earlier in
(\ref{ex:prop9}). Its value is a set of \type{local} objects, and it encodes information about
unbounded dependency gaps (see \crossrefchapteralp{udc}). Here is the relevant constraint:% 
%
\footnote{We use \isi{$\cup$} for set union. Notice that the mother category does not have to have an
  empty \textsc{slash} list, thus allowing for multiple extractions (\emph{Paul, who could we talk
    to about?} where \emph{Paul} is understood as object of \emph{about} and \emph{who} as object of \emph{to}).}
%

\ea\label{ex:prop36}\label{prop:head-filler-phrase}\is{schema!Filler-Head}
\type{hd-filler-ph} \impl
\avm{
	[slash & \1\\
	hd-dtr & \2	[comps & <>\\
				slash & \{\3\} \cupAVM \1]\\
	dtrs & <[synsem|local & \3], \2>]
}
\z

\noindent
This says that a head-filler phrase has a head daughter with a \textsc{slash} set which is the
\textsc{slash} set of the head-filler phrase plus one other \type{local} object,
and a non-head daughter, whose \textsc{local} value is the additional \type{local} object
of the head daughter. \ibox{1} is normally the empty set.%
%
\footnote{As with (\ref{ex-head-subj-phrase}), one might substitute \type{phrase} here for [\textsc{comps}~\eliste]. But this would mean that \emph{to} in \emph{I would do it but I don’t know how to} must be analysed as a phrase containing a single word. With (\ref{prop:head-filler-phrase}), it can be just a word.}
%
Figure~\ref{fig:prop8} illustrates a typical head-filler phrase.

\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-filler-ph}
	head & \1 verb\\
	subj & <>\\
	comps & <>\\
	slash & \{\}]
},l sep=1cm
	[\avm{NP[local & \2]}
		[who]
	]
	[\avm{
		[\type*{hd-subj-ph}
		head & \1\\
		subj & <>\\
		comps & <>\\
		slash & \{\2\}]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}},l sep=1cm
		[I talked to,roof]
	]
]
\end{forest}
\caption{A tree for a head-filler phrase}\label{fig:prop8}
\end{figure}

Notice that the head daughter in a head-filler phrase is not required to have an empty \textsc{subj} list (it is not marked as [\textsc{subj}~\eliste]) and hence does not have to be a head-subject phrase. It can also be a head-complement phrase (a VP), as in the following:

\ea\label{ex:prop37}
I’m wondering [who [to talk to]].
\z

\noindent
Either the Valence Principle\is{principle!Valence} or the Generalised Head Feature Principle\is{principle!Head Feature!Generalized} will ensure that a head-filler phrase has the same value for \textsc{subj} as its head daughter.

The constraints that we have just discussed are rather like phrase structure rules. This led
\citet[33]{GSag2000a-u} to use an informal notation which reflects this. This involves the phrase
type on the first line followed by a colon, and information about the phrase itself and its
daughters on the second line separated by an arrow and with the head daughter identified by
``\textbf{H}''. Thus, instead of the hypothetical (\ref{ex:prop38a}), one would have (\ref{ex:prop38b}). 

\eal\label{ex:prop38}
\ex\label{ex:prop38a}
\type{example-type} \impl
\avm{
	[synsem & \upshape X\\
	dtrs & < \1\,Y, Z >\\
	hd-dtr & \1]
}
\ex\label{ex:prop38b}
\type{example-type}:
	
[\synsem X] $\to$ \textbf{H}[Y], Z
\zl

\noindent
Notice that while the double arrow in (\ref{ex:prop38a}) has the normal ``if-then'' interpretation, the single arrow in (\ref{ex:prop38b}) means ``consists of''. In some circumstances, this informal notation may be more convenient than the more formal notation used in (\ref{ex:prop38a}).

\largerpage
In the preceding discussion, we have ignored the semantics of the phrase. 
% Leaving aside quantification and other complex matters, the \textsc{content} of a headed phrase can be handled via
% two semantic principles, assuming \textsc{index} and \textsc{rel(a\-tion)s} as in MRS as in
% (\ref{ex:prop23}) above: 
Leaving aside quantification and other complex matters, and assuming \textsc{index} and
\textsc{rel(ation)s} as in MRS (as shown in (\ref{ex:prop23}) above), the \textsc{content} of a
headed phrase can be handled via two semantic principles: a coindexing principle (the \textsc{index}
of a headed phrase is the \textsc{index} of its \textsc{head-dtr}) and a ``compositionality'' principle (the \textsc{rels} of
a phrase is the concatenation of the \textsc{rels} of its \textsc{dtrs}; \citealp[Section~4.3.2,
Section~5]{CFPS2005a}; \crossrefchapteralp[Section~\ref{sec-minimal-recursion-semantics}]{semantics}).


The type hierarchy in Figure~\ref{prop:fig-type-hierarchy-phrases} is simplified in a number of respects. It includes no non-headed phrases.%
%
\footnote{The most important type of non-headed phrase is coordinate structure. See \crossrefchapterw{coordination} for discussion.}
%
It also ignores various other subtypes of \type{headed-phrase}, some of which are discussed in the next section. Most importantly, it is widely assumed that the type \type{phrase}, like the type \type{lexeme}, can be cross-classified along two dimensions, one dealing with head-dependent relations and the other dealing with the properties of various types of clauses. A simplified illustration is given in Figure~\ref{fig:prop9}.

\begin{figure}\istype{phrase}
\begin{forest}
type hierarchy
[phrase
	[headedness,partition
		[\ldots]
		[headed-phrase
			[\ldots]
			[\ldots]
			[head-filler-phrase
				[\ldots]
				[\ldots]
				[wh-interr-cl,edge to=!r211]
			]
		]
	]
	[clausality,partition
		[clause
			[interr-cl]
			[\ldots]
		]
		[non-clause]
	]
]
\end{forest}
\caption{Cross-classification of phrases}\label{fig:prop9}
\end{figure}

\largerpage
Here \type{wh-interr-cl} is identified as a subtype of \type{head-filler-phrase} and a subtype of
\type{interr}(\type{ogative})\type{-cl}. As such, it has both the properties required by the
constraint in (\ref{prop:head-filler-phrase}) and certain properties characteristic of interrogative
clauses, most obviously interrogative semantics. 

%%%%%%%%%%%%%%%%%%%%
\subsection{Constituency and constituent order}\label{sec:prop5.2}

We must now say something about constituent order. In much HPSG work, this is a matter of phonology:
more precisely, a matter of the relation between the \textsc{phon} value of a phrase and the
\textsc{phon} values of its daughters.% 
%
\footnote{As discussed in Section~\ref{sec:prop7.1}, in some HPSG work, linear order is a property
  of so-called order domains, which essentially mediate constituent structure and phonology (see
  \crossrefchapteralp[Section~\ref{sec-domains}]{order}). } 
%
Consider, for example, a phrase with two daughters, each with its own \textsc{phon} value. The
\textsc{phon} value of the phrase will be the concatenation of the \textsc{phon} values of the
daughters. Clearly, they can be concatenated in two ways as in (\ref{ex:prop39}),
or their order may be left unspecified for ``free'' word order:\footnote{
Unspecified means any combination of \ibox{1} and \ibox{2} using the shuffle operation: \ibox{1}
$\bigcirc$ \ibox{2}. (see footnote~\ref{fn-shuffle})
}
% \itdobl{Stefan: ``or their order may be left unspecified for ``free'' word order''. I think it is
%   not viable to say nothing about the \phonv of the mother. If you don't say anything \phon could be
%   anything. For example the combination of ``the'' with ``new president'' could be ``Donald Trump''
%   or ``Mickey Mouse''. So one would have to state at least a disjunction between \ibox{1} + \ibox{2}
%   and \ibox{2} + \ibox{1}. But this is not very sexy.\\
% Bob: of course PHON of a phrase is a function of the PHON of its DTRS, the reader is not dumb\\
% Stefan: Yes, but this is not underspecification. It is a disjunction. As I said in my comment.
% } 

\eal
\label{ex:prop39}\label{ex-phon-concatenation}
\ex
\avm{
	[phon & \1 \+ \2\\
	dtrs & <[phon \1], [phon \2]>]
}
\ex
\avm{
	[phon & \2 \+ \1\\
	dtrs & <[phon \1], [phon \2]>]
}
\zl

\noindent
Within this approach, the following \ili{English} and \ili{Welsh} examples might have exactly the same analysis (a head-adjunct phrase) except for their \textsc{phon} values:

\eal\label{ex:prop40}
\ex\label{ex:prop40a}
black sheep
\ex\label{ex:prop40b}
\gll defaid            du\\
     sheep.\textsc{pl} black\\\hfill(\ili{Welsh})
\glt `black sheep'
\zl

\largerpage
\noindent
Similarly, a prepositional\is{adposition!preposition} phrase in \ili{English} and a postpositional\is{adposition!postposition} phrase in \ili{Japanese} might have the same analysis (a head-complement phrase) apart from their \textsc{phon} values. Ordering rules are constraints on phrasal types. They are commonly written with < (``precedes''). Thus, languages with head-complement order might have the rule in (\ref{ex:prop41a}), and languages with complement-head order the rule in (\ref{ex:prop41b}).

\eal\label{ex:prop41}
\ex\label{ex:prop41a}
	\avm{
		[comps & <\ldots, \1, \ldots>] !<! [synsem & \1]
	}
\ex\label{ex:prop41b}
	\avm{
		[synsem & \1] !<! [comps & <\ldots, \1, \ldots>]
	}
\zl
%
But it should be remembered that ordering rules are well-formedness constraints on structures built with certain concatenations of \textsc{phon} values as in (\ref{ex-phon-concatenation}).\footnote{%
   An alternative notation, provided different daughters are distinguished with different names, could be:
   \eal
   \ex \textsc{hd-dtr} < \textsc{comps-dtrs}
   \ex \textsc{comps-dtrs} < \textsc{hd-dtr}
   \zl
% ~\newline\inlinetodoobl{Stefan: This should be \textsc{comps-dtr} since linearization rules affect pairs of
%   linguistic objects. One may refer to non-head daughters, but you do not have them as feature. The
%   usual formulation is \textsc{head} < \textsc{complement} \citep[\page 172]{ps}. Suggestion: I
%   change this to ``\textsc{head} < \textsc{complement}''.\\
% Bob\&Anne: NO Remember we don't assume binary branching (see figure 6) so it is important that it is COMPS-DTRS
% it is partially ordering = only the HEAD-DTR is ordered but the COMPS-DTRS may be free among
% themselves (for local scrambling).\\
% Stefan: Yes, this is independent of it. You order two objects with respect to each other. This was
% always the case since we had GPSG. I never saw a formulation like (\mex{0}b) before. What this would
% entail is a quantification over all elements of the \textsc{comps-dtrs} list.
% }
}

Not all pairs of expressions which might be seen as differing just in word order have the same
analysis apart from their \textsc{phon} values.\label{page-properties:aux-inversion} Consider, for example, the following:

\eal\label{ex:prop42}
\ex\label{ex:prop42a}
Kim is late.
\ex\label{ex:prop42b}
Is Kim late?
\zl

\noindent
Here, we have a declarative and a related interrogative. They differ semantically and in word order,
but for most work in HPSG, they also differ in their syntactic structures. (\ref{ex:prop42a}) is a
head-subject phrase much like that in Figure~\ref{fig:prop7}. Clauses like (\ref{ex:prop42b}), on
the other hand, are standardly seen as ternary branching phrases in which both the subject and the
complement are a sister of the auxiliary \citep[40]{ps2}. This requires an additional phrase type,
which might be called \type{head-subject-complement-phrase}.\footnote{In \citet[36]{GSag2000a-u}, it
  is called \type{sai-phrase}. In some HPSG work, e.g.\ \citet[409--414]{SWB2003a}, examples like
  (\ref{ex:prop42b}) are analysed as involving an auxiliary verb with two complements and no
  subject. This approach has no need for an additional phrase type, but it requires an alternative
  valence description for auxiliary verbs.}
%


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Further syntactic structures}\label{sec:prop6}
\label{prop:sec-further-syntactic-structures}

Head-complement phrases, head-subject phrases, and head-filler phrases are perhaps the most important types of syntactic structures, but there are others that are of considerable importance. Here we will say something about three of them: head-adjunct phrases, head-specifier phrases, and head-marker phrases.

\largerpage
%%%%%%%%%%%%%%%%%%%%
\subsection{Adjuncts}\label{sec:prop6.1}
\label{properties:sec-adjuncts}

Adverbs, adverbial PPs within VPs, attributive adjectives, and relative clauses within NPs are commonly viewed as adjuncts. Thus, the following illustrate head-adjunct phrases (with the head following the adjunct in (\ref{ex:prop43a}) and (\ref{ex:prop43c}) and preceding in (\ref{ex:prop43b}) and (\ref{ex:prop43d})):

\eal\label{ex:prop43}
\ex\label{ex:prop43a}
Kim [slowly [read the book]]
\ex\label{ex:prop43b}
Kim [[met Lee] in the pub]
\ex\label{ex:prop43c}
a [new [book about syntax]]
\ex\label{ex:prop43d}\label{ex-book-which-impresses}
a [[book about syntax] which impresses everyone]
\zl

\noindent
In much HPSG work, adjuncts select the heads they combine with through a feature \textsc{mod(ifies)} whose value is a \type{synsem} object, while other signs are \mbox{[\textsc{mod} \type{none}]}. The type \type{hd-adj-ph} is subject to a constraint of the following form:
\ea
\label{ex-hd-adj-ph}
\type{hd-adj-ph} \impl \avm{ [hd-dtr & \1\\
                              dtrs   & < \1 [ synsem \2 ], [head|mod \2] > ] }	
\z

\noindent
This ensures that a head-adjunct phrase has two daughters, one of which is a head, and that the
\synsem value of the head daughter is identical to the \modv of the non-head daughter. Following
\citet[41]{GSag2000a-u} this assumes that \textsc{mod} is part of the value of \head. With this
constraint, we have structures like that in Figure~\ref{fig:prop10} for (\ref{ex:prop43a}).


%\inlinetodoopt{\type{head-adj-ph} is at sign level. \ibox{3} is at \synsem level. \ldots}
\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-adj-ph}
	head & \1 verb\\
	subj & \2 <NP>\\
	comps & <>]
},l sep=1cm
	[Adv\avm{[mod & \3]}
		[slowly]
	]
	[\avm{
		\3[\type*{hd-comp-ph}
		head & \1\\
		subj & \2\\
		comps & <>]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}}
		[read the book,roof]
	]
]
\end{forest}
\caption{A tree for a head-adjunct phrase}\label{fig:prop10}
\end{figure}

In the case of adverbs, adverbial PPs, and attributive adjectives, it is a simple matter to assign
an appropriate value to \textsc{mod}, and this value can be underspecified to account for the
polymorphism of certain adverbs which can modify all (major) categories
\citep[\page 28--29]{AG2003b-u}. In the case of relative clauses, it is more complex because the value of
\textsc{mod} must be coindexed with the \emph{wh}-element, if there is one, or the gap, if there
isn’t. In (\ref{ex-book-which-impresses}), this is reflected in the fact that the verb in the
relative clause is the singular \emph{impresses} and not the plural \emph{impress}. See
\crossrefchapterw{udc} and \crossrefchaptert{relative-clauses} for some discussion. 

Notice also that in head-adjunct phrases, the adjunct is not a syntactic head, but may well be
%the semantic predicate or 
the semantic head.
%\itdopt{
%Stefan: What is a ``semantic predicate''? Do you mean ``semantic functor''? And isn't ``semantic
%functor'' and ``semantic head'' sort of the same?}  
This is an example of the difference between syntactic head and semantic head, and between syntactic argument and semantic argument in HPSG.

Although an adjunct analysis of adverbial PPs seems quite natural, it has been argued in some HPSG work that they are in fact optional complements of verbs (see e.g.\ \citealp[4]{AG97a-u,BMS2001a}; \citealp[168, Footnote~2]{GSag2000a-u}). On this view, \emph{in the pub} in (\ref{ex:prop43b}) is much like the same phrase in (\ref{ex:prop44}), where it is clearly a (predicative) complement:

\ea\label{ex:prop44}
Kim is in the pub. 
\z

\noindent
Various arguments have been advanced for this position, but it is controversial and it is rejected by \citet{Levine2003a}, \citet[Chapter~3]{LH2006a}, and \citet{Chaves2009a}. There is an unresolved issue here.%
%
\footnote{It has been argued that some adverbs and PPs are adjuncts and others are complements,
  depending on word order, case, and so on. (see, for example, \citealp{Prze99},
  \citealp{HA2014a-u}, and \crossrefchapteralp[Section~\ref{negation:sec-sentential-negation}]{negation}).
}
%

%%%%%%%%%%%%%%%%%%%%
\subsection{Specifiers and markers}\label{sec:prop6.2}

As noted earlier, some HPSG work assumes a feature \textsc{spr (specifier)} which is realised by
various categories. In some work, subjects are analysed as specifiers \citep*[100--103]{SWB2003a},
but in other approaches, they are realisations of a \textsc{subj(ect)} feature, as discussed in the
last section. For some HPSG work, e.g.\ \citew[Section~9.4]{ps2} and \citew[Section~4.3]{SWB2003a},
determiners within NPs are an important example of specifiers. The type \type{hd-spr-ph} is subject
to a constraint of the following form: 
\ea
\label{ex-head-spr-ph}
\type{hd-spr-ph} \impl \avm{[ spr & \eliste\\
                                hd-dtr & \1 [spr & < \2 > \\
                                             comps & < > ]\\
                                dtrs   & < [synsem \2], \1 > ]}              
\z

\noindent
This ensures that a head-specifier phrase is [\spr \eliste] and has a head daughter which is [\comps
\eliste] and a non-head daughter with the \type{synsem} properties that appear in the head’s \sprl. On this view, \emph{the pub} has the schematic structure in Figure~\ref{fig:prop11}.

\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-spr-ph}
	head & \1 noun\\
	spr & <>\\
	comps & <>]
},l sep=1cm
	[\ibox{2} Det
		[the]
	]
	[\avm{
		[\type*{word}
		head & \1\\
		spr & <\2>\\
		comps & <>]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}}
		[pub]
	]
]
\end{forest}
\caption{A tree for a head-specifier phrase}\label{fig:prop11}
\end{figure}

Some recent work, e.g.\ \citew[\page 84]{Sag2012a}, has adopted a rather different view of at least
some determiners, namely that they are what are known as markers, a notion first introduced in
\citet[Section~1.6]{ps2}. These are non-heads which select the head that they combine with through a
\textsc{select} feature (\citealp{VanEynde98a}; \crossrefchapteralp[Section~\ref{np:sec-functor-treatment}]{np}) but determine the
\textsc{marking} value of their mother.  Within this approach, \emph{the pub} has the schematic
structure in Figure~\ref{fig:prop12}.\footnote{%
  Work which assumes the \textsc{select} feature also uses it instead of \textsc{mod} for adjuncts
  and considers both markers and adjuncts to be ``functors'' (\citealp{VanEynde98a};
  \crossrefchapteralp[Section~\ref{np:sec-functor-basics}]{np}).}
%

\begin{figure}
\begin{forest}
	sm edges
[\avm{
	[\type*{hd-mark-ph}
	head & \1 noun\\
	comps & <>\\
	marking & \2]
},l sep=1cm
	[\avm{
		[select & \3\\
		marking & \2 the]
	}
		[the]
	]
	[\avm{
		\3 [\type*{word}
		head & \1\\
		comps & <>\\
		marking & none]
	}, edge label={node[midway,right]{~~\textsc{hd-dtr}}}
		[pub]
	]
]
\end{forest}
\caption{A tree for a head-functor phrase}\label{fig:prop12}
\end{figure}

A marker analysis was originally proposed for complementisers. However, they have also been analysed as heads within HPSG, e.g.\ in \citet[456--458]{Sag97a} and \citet[Section~2.8]{GSag2000a-u}. There is no consensus here.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Further topics}\label{sec:prop7}
\label{prop:sec-further-topics}

There are many other aspects of HPSG that could be discussed in this chapter, but we will focus on just two: what are known as order domains, and the distinguishing properties of the SBCG version of HPSG.

%%%%%%%%%%%%%%%%%%%%
\subsection{Order domains}\label{sec:prop7.1}

We noted above that much HPSG work views word order as a matter of phonology, specifically a matter of the relation between the \textsc{phon} value of a phrase and the \textsc{phon} values of its daughters (see \crossrefchapteralp{order}). Some work in HPSG argues that this is too simple in that it ties the observed order too closely to constituent structure. Consider the following examples:

\eal\label{ex:prop45}
\ex\label{ex:prop45a}
A man who looked like Churchill came into the room.
\ex\label{ex:prop45b}
A man came into the room who looked like Churchill.
\zl

\noindent
One might assume that these show different observed orders because they have different structures
\citep{Kiss2005a}, but one might also want to claim that they have the same constituent structure
\citep{KP95a}. This is possible if the observed order is not a simple reflection of constituent
structure. Much work in HPSG has proposed that the observed order is a reflection not of the
constituent structure of an expression but of a separate system of order domains (see
\citealp{Reape94a,Babel,Kathol2000a}). Within this approach, ordering rules may order non-sister
elements, as long as they belong to the same order domain: the constituent structure of an
expression can be encoded as the value of a \textsc{dtrs (daughters)} feature and the order domain
as the value of a \textsc{dom(ain)} feature. Adopting this position, one might propose that
(\ref{ex:prop45b}) has the schematic analysis in (\ref{ex:prop46}). 

\ea\label{ex:prop46}
\avm{
	[\punk{synsem}{\normalfont S}\\
	dtrs & <[\type{a man who looked like Churchill}],[\type{came into the room}]>\\
	dom & <[\type{a man}],[\type{came into the room}],[\type{who looked like Churchill}]> ]
}
\z

\noindent
Here the clause has two daughters but three domain elements. The simpler example in (\ref{ex:prop45a}) will have two daughters and two domain elements.

It is worth noting that this approach allows a different analysis for interrogatives like (\ref{ex:prop42b}). It would be possible to propose an analysis in which they have two daughters and three domain elements as follows:

\ea\label{ex:prop47}
\avm{
	[\punk{synsem}{\normalfont S}\\
	dtrs & <[\type{Kim}],[\type{is late}]>\\
	dom & <[\type{is}],[\type{Kim}],[\type{late}]> ]
}
\z

\noindent
As far as we are aware, no one has proposed such an analysis for \ili{English} interrogatives, but
essentially this analysis is proposed for \ili{German} interrogatives in \citet[81]{Kathol2000a}.% 
%
\footnote{\citet{Kathol2000a} assumes that order domains are divided into topological fields and
  shows how this idea allows an interesting approach to various aspects of clausal word order. See
  \citet{Borsley:06} for an application of this idea to \isi{negation}.} 
%

Order domains seem most plausible as an approach to the sorts of discontinuity that are found in
so-called nonconfigurational languages such as \ili{Warlpiri} \citep{DS99a}. However, they may well
have a role to play in more familiar languages \citep{BGM99a,Chaves2014a-u}. But exactly how much of
a role they should play in syntax is an unresolved matter. 

One might wonder whether a version of HPSG that includes order domains is still a monostratal
framework. It remains a framework in which linguistic expressions have a single constituent
structure. However, it does have a second important level of representation, which makes available a
variety of analyses which would otherwise not be possible. Whether the framework is still
monostratal depends on how exactly the term is used. We will not take a stand on this. 

%%%%%%%%%%%%%%%%%%%%
\subsection{Sign-Based Construction Grammar}\label{sec:prop7.2}
\label{prop:sec-sbcg}

\is{Construction Grammar (CxG)!Sign-Based|(}
The SBCG version of HPSG will be discussed in some detail in the next chapter
\citep*[\page~\refORregion{page-sbcg-start}{page-sbcg-end}]{chapters/evolution}, in the chapter on
unbounded dependencies \citep[Section~\ref{udc:sec-SBCG}]{chapters/udc}, and in the chapter on HPSG and
Construction Grammar \citep[Section~\ref{cxg:sec-sbcg}]{chapters/cxg}. Here we will just highlight
the central difference between this approach and earlier work. The term ``construction'' is widely
used in connection with the earlier Constructional HPSG, but within that work, constructions are
just types of sign. In contrast, for SBCG, signs and constructions are quite different objects. 

For SBCG, constructions are objects which associate a \textsc{mtr} (\textsc{mother}) sign with a
list of \textsc{daughter} signs, one of which is a \textsc{head-daughter} in a headed
construction. Thus, constructions take the form in (\ref{ex-constructions}) and headed-constructions
the form in (\ref{ex-headed-constructions}): 

%\begin{multicols}{2}
\eal\label{ex:prop48}
\ex\label{ex:prop48a}\label{ex-constructions}
\avm{
	[\type*{cx}
	mtr & sign\\
	dtrs & \listOf{sign} ]
}
	
%\columnbreak
	
\ex\label{ex:prop48b}\label{ex-headed-constructions}
\avm{
	[\type*{headed-cx}
	mtr & sign\\
	dtrs & \listOf{sign}\\
	hd-dtr & sign ]
}
\zl
%\end{multicols}

\noindent
Constructions are utilised by the Sign Principle\is{principle!Sign}, which can be formulated as follows:%
%
\footnote{Lexical rules are analysed in SBCG as lexical constructions. Thus, (b) covers derived words as well as phrases.}
%

\eanoraggedright
\label{ex:prop49}
Signs are well formed if either (a) they match some lexical entry, or (b) they match the mother of some construction.
\z

\noindent
Constructions and the Sign Principle are properties of SBCG which are lacking in earlier
work. Essentially, then, they are complications. But they allow simplifications. In particular, they
mean that signs do not need to have the features \textsc{dtrs} and \textsc{hd-dtr}. This in turn
allows the framework to dispense with the feature \textsc{synsem} and the type \type{synsem}. These
elements are necessary in earlier HPSG because taking the value of \textsc{comps} to be a list of
signs would incorrectly predict that heads may select complements not just with specific syntactic
and semantic properties, but also with specific kinds of internal structure. For example, it would
allow a verb to select as its complement a phrase whose head has a specific type of complement. To
exclude this possibility, earlier versions of HPSG seem to need \textsc{synsem} and \type{synsem}
\citep[\page 23]{ps2}. In SBCG, it is excluded by the assumption that signs do not have the features
\textsc{dtrs} and \textsc{hd-dtr}, and so \textsc{synsem} and \type{synsem} are unnecessary. Thus,
SBCG is both more complex and simpler than earlier versions of the framework. This means that
considerations of simplicity do not obviously favour or disfavour the approach. 
\is{Construction Grammar (CxG)!Sign-Based|)}

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Concluding remarks}\label{sec:prop8}

In the preceding pages, we have spelled out the basic properties of HPSG and the assumptions it makes about the nature of linguistic analyses and the conduct of linguistic research. We have looked at the types, features, and constraints that are the building blocks of HPSG analyses. We have also outlined the HPSG approach to the lexicon and the basics of its approach to syntax, and we have considered some of the main types of syntactic structure. Finally, we have discussed order domains and SBCG. More can be learned about all of these matters in the chapters that follow.


\section*{\acknowledgmentsEN}

We are grateful to Stefan Müller, Jean-Pierre Koenig, and Frank Richter for many helpful comments on
earlier versions of this chapter. We alone are responsible for what appears here.



{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}


%      <!-- Local IspellDict: en_GB-ise-w_accents -->
